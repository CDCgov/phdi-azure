{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# updateECRdatastore\n",
    "\n",
    "This notebook updates the ECR datastore delta table with new ECR records (`PARSED_ECR_PATH`); a new ECR datastore delta table is created if one does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "STORAGE_ACCOUNT = \"$STORAGE_ACCOUNT\"\n",
    "BASE_DATASTORE_DIRECTORY = \"ecr-datastore\"\n",
    "DELTA_TABLES_FILESYSTEM = f\"abfss://delta-tables@{STORAGE_ACCOUNT}.dfs.core.windows.net/\"\n",
    "ECR_DATASTORE_PATH = DELTA_TABLES_FILESYSTEM + \"ecr-datastore\"\n",
    "ECR_DATASTORE_DAILY_EXTRACT_PATH = DELTA_TABLES_FILESYSTEM + \"ecr-datastore\"\n",
    "PARSED_ECR_PATH = DELTA_TABLES_FILESYSTEM + \"raw_data\"\n",
    "DAILY_EXTRACT_FORMATS = [\"parquet\",\"csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils\n",
    "\n",
    "# Set up for writing to blob storage\n",
    "delta_bucket_name = \"delta-tables\"\n",
    "linked_service_name = \"$BLOB_STORAGE_LINKED_SERVICE\" \n",
    "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\n",
    "wasb_path = 'wasbs://%s@%s.blob.core.windows.net/' % (delta_bucket_name, STORAGE_ACCOUNT)\n",
    "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (delta_bucket_name, STORAGE_ACCOUNT), blob_sas_token)\n",
    "# Try mounting the remote storage directory at the mount point\n",
    "try:\n",
    "    mssparkutils.fs.mount(\n",
    "        wasb_path,\n",
    "        \"/\",\n",
    "        {\"LinkedService\": linked_service_name}\n",
    "    )\n",
    "except:\n",
    "    print(\"Already mounted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    FloatType,\n",
    "    BooleanType,\n",
    "    DateType,\n",
    "    TimestampType,\n",
    "    ArrayType\n",
    ")\n",
    "from pyspark.sql.functions import explode_outer\n",
    "\n",
    "from delta.tables import *\n",
    "import json\n",
    "from typing import Tuple\n",
    "\n",
    "# Prepare Schemas\n",
    "ecr_schema_path = f\"abfss://delta-tables@{STORAGE_ACCOUNT}.dfs.core.windows.net/ecr_datastore_config.json\"\n",
    "ecr_schema = spark.read.json(ecr_schema_path,multiLine=True)\n",
    "\n",
    "def prepare_schemas(ecr_schema):\n",
    "    table_schemas = {\n",
    "    \"core\":{\n",
    "        \"patient_id\": [\"string\", False],\n",
    "        \"person_id\": [\"string\", False],\n",
    "        \"person_id_date_added\": [\"timestamp\", True],\n",
    "        \"iris_id\": [\"string\", True],\n",
    "        \"iris_id_date_added\": [\"timestamp\", True],\n",
    "        \"incident_id\": [\"string\", True],\n",
    "        \"incident_id_date_added\": [\"timestamp\", True]}\n",
    "    }\n",
    "\n",
    "    row = ecr_schema.collect()[0].asDict()\n",
    "    column_names = list(row.keys())\n",
    "\n",
    "    for column_name, column_data in row.items():\n",
    "        if column_data['data_type'] != \"array\":\n",
    "            table_schemas[\"core\"][column_name] = [column_data['data_type'], column_data['nullable']]\n",
    "        else:\n",
    "            table_schemas[column_name] = {}\n",
    "            table_schemas[column_name]['eicr_id'] = ['string', False]\n",
    "            table_schemas[column_name][column_name] = {}\n",
    "            \n",
    "            for secondary_column_name, secondary_column_data in row[column_name]['secondary_schema'].asDict().items():\n",
    "                table_schemas[column_name][column_name][secondary_column_name] = [secondary_column_data['data_type'], secondary_column_data['nullable']]\n",
    "\n",
    "    return table_schemas\n",
    "\n",
    "def get_schemas(table_schemas: dict) -> Tuple[StructType, dict]:\n",
    "    \"\"\"\n",
    "    Get a Spark StructType object from a JSON schema string.\n",
    "\n",
    "    :param schema: A dictionary defining the schema of the ECR datastore including \n",
    "        the data type of each field and whether null values are allowed. Should be of the form:\n",
    "        '{\"fieldname\": [<data type>, <nullable?(True/False)>]}'.\n",
    "    :return: A tuple containing a Spark StructType object representing the schema \n",
    "    and a dictionary defining field mappings for merge operations. \n",
    "    \"\"\"\n",
    "\n",
    "    schema_type_map = {\n",
    "        \"string\": StringType(),\n",
    "        \"integer\": IntegerType(),\n",
    "        \"float\": FloatType(),\n",
    "        \"boolean\": BooleanType(),\n",
    "        \"date\": DateType(),\n",
    "        \"timestamp\": TimestampType(),\n",
    "        \"datetime\": DateType(),\n",
    "        \"number\": IntegerType()\n",
    "    }\n",
    "    spark_schemas = {}\n",
    "\n",
    "    for table_name, schema in table_schemas.items():\n",
    "        spark_schema = StructType()\n",
    "        flattened_df_schema = StructType()\n",
    "        merge_schema = {}\n",
    "        array_fields = []\n",
    "\n",
    "        for field in schema:\n",
    "            if isinstance(schema[field], dict):\n",
    "                for array_field, data in schema[field].items():\n",
    "                    array_fields.append(StructField(array_field, schema_type_map[data[0]],data[1]))\n",
    "                    merge_schema[array_field] = \"new.\" + array_field\n",
    "                    flattened_df_schema.add(StructField(array_field, schema_type_map[data[0]],data[1]))\n",
    "                spark_schema.add(StructField(field, ArrayType(\n",
    "                    StructType(array_fields)\n",
    "                )))\n",
    "                        \n",
    "            else:\n",
    "                spark_schema.add(StructField(field, schema_type_map[schema[field][0]], schema[field][1]))\n",
    "                flattened_df_schema.add(StructField(field, schema_type_map[schema[field][0]], schema[field][1]))\n",
    "                merge_schema[field] = \"new.\" + field\n",
    "\n",
    "        spark_schemas[table_name] = {\n",
    "            \"spark_schema\": spark_schema,\n",
    "            \"merge_schema\": merge_schema,\n",
    "            \"flattened_df_schema\": flattened_df_schema\n",
    "        }\n",
    "\n",
    "    return spark_schemas\n",
    "\n",
    "# Prepare Schemas \n",
    "table_schemas = prepare_schemas(ecr_schema)\n",
    "\n",
    "# Format table schemas for spark\n",
    "spark_schemas = get_schemas(table_schemas)\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"Update eCR Datastore\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_active_problems_ecr_records(active_problems: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Formats eCR records with active problems into dataframes that have \n",
    "     three columns, `eicr_id`, `problem`, and `problem_date`, and 1\n",
    "     active problem per row.\n",
    "    \n",
    "    :param active_problems: Spark DataFrame.\n",
    "    :return: Spark DataFrame with 1 active problem per row.\n",
    "\n",
    "    \"\"\"\n",
    "    active_problems = active_problems.select(active_problems.eicr_id,explode_outer(active_problems.active_problems).alias(\"problems\")).rdd.map(\n",
    "        lambda x: (x.eicr_id, x.problems['problem'],x.problems['problem_date'])).toDF(flattened_active_problems_schema)\n",
    "    return active_problems\n",
    "\n",
    "\n",
    "def transform_labs_ecr_records(labs: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Formats eCR records with labs into a DataFrame that has \n",
    "     1 lab test and associated results per row.\n",
    "    \n",
    "    :param labs: Spark DataFrame.\n",
    "    :return: Spark DataFrame with 1 lab per row.\n",
    "\n",
    "    \"\"\"\n",
    "    labs = labs.select(labs.eicr_id,explode_outer(labs.labs).alias(\"labs\")).rdd.map(\n",
    "    lambda x: (\n",
    "        x.eicr_id, \n",
    "        x.labs['performing_lab'],\n",
    "        x.labs['specimen_collection_date'],\n",
    "        x.labs['specimen_type'],\n",
    "        x.labs['test_result_code'],\n",
    "        x.labs['test_result_code_display'],\n",
    "        x.labs['test_result_code_system'],\n",
    "        x.labs['test_result_interp'],\n",
    "        x.labs['test_result_interp_code'],\n",
    "        x.labs['test_result_interp_system'],\n",
    "        x.labs['test_result_qualitative'],\n",
    "        x.labs['test_result_quantitative'],\n",
    "        x.labs['test_result_ref_range_high'],\n",
    "        x.labs['test_result_ref_range_high_units'],\n",
    "        x.labs['test_result_ref_range_low'],\n",
    "        x.labs['test_result_ref_range_low_units'],\n",
    "        x.labs['test_result_units'],\n",
    "        x.labs['test_type'],\n",
    "        x.labs['test_type_code'],\n",
    "        x.labs['test_type_system'],\n",
    "         )).toDF(flattened_labs_schema)\n",
    "    return labs\n",
    "\n",
    "\n",
    "def update_ecr_datastore(schema,merge_schema,table_name, ECR_DATASTORE_PATH, DELTA_TABLES_FILESYSTEM):\n",
    "    # Read JSON files into a DataFrame with the specified schema\n",
    "    new_ecr_records = spark.read.schema(schema).json(PARSED_ECR_PATH,multiLine=True)\n",
    "\n",
    "    if table_name == \"labs\":\n",
    "        new_ecr_records = transform_labs_ecr_records(new_ecr_records)\n",
    "    elif table_name == \"active_problems\":\n",
    "        new_ecr_records = transform_active_problems_ecr_records(new_ecr_records)\n",
    "\n",
    "    # Check if Delta table exists\n",
    "    ECR_DATASTORE_PATH = ECR_DATASTORE_PATH + f\"-{table_name}\"\n",
    "\n",
    "    # Check if Delta table exists\n",
    "    if DeltaTable.isDeltaTable(spark, ECR_DATASTORE_PATH):\n",
    "        # If the table exists add new records.\n",
    "        ecr_datastore = DeltaTable.forPath(spark, ECR_DATASTORE_PATH)\n",
    "\n",
    "        ecr_datastore.alias(\"old\").merge(\n",
    "            new_ecr_records.alias(\"new\"), \"old.eicr_id = new.eicr_id\"\n",
    "        ).whenNotMatchedInsert(values=merge_schema).execute()\n",
    "    else:\n",
    "        # If Delta table doesn't exist, create it.\n",
    "        new_ecr_records.write.format(\"delta\").mode(\"append\").save(ECR_DATASTORE_PATH)\n",
    "\n",
    "    # Make a copy of the Delta table in CSV format for easy access.\n",
    "    ecr_datastore = DeltaTable.forPath(spark, ECR_DATASTORE_PATH).toDF()\n",
    "\n",
    "    # Set up storage client\n",
    "    container_url = f\"https://{STORAGE_ACCOUNT}.blob.core.windows.net/\"\n",
    "\n",
    "    for format in DAILY_EXTRACT_FORMATS:\n",
    "\n",
    "        # Write standard pyspark directories for each file format\n",
    "        # Force pyspark to coalesce the results into a single file\n",
    "        format_path = ECR_DATASTORE_PATH + \".\" + format\n",
    "        modified_datastore_directory = ECR_DATASTORE_PATH + \".\" + format + \"/\"\n",
    "        ecr_datastore.coalesce(1).write.format(format).option(\"header\",True).mode('overwrite').save(format_path)\n",
    "\n",
    "        # Locate the file which actually has the data amidst the pyspark kruft\n",
    "        partial_file = \"\"\n",
    "        for f in mssparkutils.fs.ls(format_path):\n",
    "            file_in_namespace = f.path.split(\"/\")[-1]\n",
    "            if file_in_namespace.startswith(\"part-\") and file_in_namespace.endswith(\".\" + format):\n",
    "                partial_file = f.path\n",
    "\n",
    "        # Create a copy of just the data at the root level, formatted appropriately\n",
    "        mssparkutils.fs.cp(partial_file, DELTA_TABLES_FILESYSTEM + f\"/updated_ecr_datastore/updated_ecr_datastore-{table_name}.\" + format)\n",
    "\n",
    "        # Now delete the pyspark junk folder by deleting all virtual filepaths\n",
    "        mssparkutils.fs.rm(format_path, recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "update_ecr_datastore(core_spark_schema, core_merge_schema, table_name = \"core\", ECR_DATASTORE_PATH = ECR_DATASTORE_PATH, DELTA_TABLES_FILESYSTEM = DELTA_TABLES_FILESYSTEM)\n",
    "\n",
    "# Labs\n",
    "update_ecr_datastore(labs_spark_schema, labs_merge_schema, table_name = \"labs\", ECR_DATASTORE_PATH = ECR_DATASTORE_PATH, DELTA_TABLES_FILESYSTEM = DELTA_TABLES_FILESYSTEM)\n",
    "\n",
    "#  Active Problems\n",
    "update_ecr_datastore(active_problems_spark_schema, active_problems_merge_schema, table_name = \"active_problems\", ECR_DATASTORE_PATH = ECR_DATASTORE_PATH, DELTA_TABLES_FILESYSTEM = DELTA_TABLES_FILESYSTEM)\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "save_output": true,
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
