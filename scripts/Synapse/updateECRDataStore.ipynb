{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# updateECRdatastore\n",
    "\n",
    "This notebook updates the ECR datastore delta table with new ECR records (`PARSED_ECR_PATH`); a new ECR datastore delta table is created if one does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "STORAGE_ACCOUNT = \"$STORAGE_ACCOUNT\"\n",
    "BASE_DATASTORE_DIRECTORY = \"ecr-datastore\"\n",
    "DELTA_TABLES_FILESYSTEM = f\"abfss://delta-tables@{STORAGE_ACCOUNT}.dfs.core.windows.net/\"\n",
    "ECR_DATASTORE_PATH = DELTA_TABLES_FILESYSTEM + \"ecr-datastore\"\n",
    "ECR_DATASTORE_DAILY_EXTRACT_PATH = DELTA_TABLES_FILESYSTEM + \"ecr-datastore\"\n",
    "PARSED_ECR_PATH = DELTA_TABLES_FILESYSTEM + \"raw_data\"\n",
    "DAILY_EXTRACT_FORMATS = [\"parquet\",\"csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils\n",
    "\n",
    "# Set up for writing to blob storage\n",
    "delta_bucket_name = \"delta-tables\"\n",
    "linked_service_name = \"$BLOB_STORAGE_LINKED_SERVICE\" \n",
    "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\n",
    "wasb_path = 'wasbs://%s@%s.blob.core.windows.net/' % (delta_bucket_name, STORAGE_ACCOUNT)\n",
    "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (delta_bucket_name, STORAGE_ACCOUNT), blob_sas_token)\n",
    "# Try mounting the remote storage directory at the mount point\n",
    "try:\n",
    "    mssparkutils.fs.mount(\n",
    "        wasb_path,\n",
    "        \"/\",\n",
    "        {\"LinkedService\": linked_service_name}\n",
    "    )\n",
    "except:\n",
    "    print(\"Already mounted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    FloatType,\n",
    "    BooleanType,\n",
    "    DateType,\n",
    "    TimestampType,\n",
    "    ArrayType\n",
    ")\n",
    "from delta.tables import *\n",
    "import json\n",
    "from typing import Tuple\n",
    "\n",
    "# Prepare Schemas\n",
    "core_table_schema = {}\n",
    "labs_table_schema = {}\n",
    "active_problems_table_schema = {}\n",
    "\n",
    "ecr_schema_path = f\"abfss://delta-tables@{STORAGE_ACCOUNT}.dfs.core.windows.net/ecr_datastore_config.json\"\n",
    "ecr_schema = spark.read.json(ecr_schema_path,multiLine=True)\n",
    "\n",
    "for row in ecr_schema.collect():\n",
    "    row = row.asDict()\n",
    "    column_names = list(row.keys())\n",
    "    for column in range(len(column_names)):\n",
    "        if row[column_names[column]]['data_type'] != 'array':\n",
    "            core_table_schema[column_names[column]] = [row[column_names[column]]['data_type'], row[column_names[column]]['nullable']]\n",
    "        else:\n",
    "            labs_table_schema['eicr_id'] = [row['eicr_id']['data_type'], row['eicr_id']['nullable']]\n",
    "            active_problems_table_schema['eicr_id'] = [row['eicr_id']['data_type'], row['eicr_id']['nullable']]\n",
    "\n",
    "            secondary_schema_columns = row[column_names[column]].asDict()['secondary_schema'].asDict()\n",
    "            for col in secondary_schema_columns.keys():\n",
    "                if column_names[column] == 'labs':\n",
    "                    labs_table_schema[col] = [secondary_schema_columns[col].asDict()['data_type'], secondary_schema_columns[col].asDict()['nullable']]\n",
    "                elif column_names[column] == 'active_problems':\n",
    "                    active_problems_table_schema[col] = [secondary_schema_columns[col].asDict()['data_type'], secondary_schema_columns[col].asDict()['nullable']]\n",
    "\n",
    "def get_schemas(schema: dict) -> Tuple[StructType, dict]:\n",
    "    \"\"\"\n",
    "    Get a Spark StructType object from a JSON schema string.\n",
    "\n",
    "    :param schema: A dictionary defining the schema of the ECR datastore including \n",
    "        the data type of each field and whether null values are allowed. Should be of the form:\n",
    "        '{\"fieldname\": [<data type>, <nullable?(True/False)>]}'.\n",
    "    :return: A tuple containing a Spark StructType object representing the schema \n",
    "    and a dictionary defining field mappings for merge operations. \n",
    "    \"\"\"\n",
    "\n",
    "    schema_type_map = {\n",
    "        \"string\": StringType(),\n",
    "        \"integer\": IntegerType(),\n",
    "        \"float\": FloatType(),\n",
    "        \"boolean\": BooleanType(),\n",
    "        \"date\": DateType(),\n",
    "        \"timestamp\": TimestampType(),\n",
    "    }\n",
    "    spark_schema = StructType()\n",
    "    merge_schema = {}\n",
    "    for field in schema:\n",
    "        spark_schema.add(StructField(field, schema_type_map[schema[field][0]], schema[field][1]))\n",
    "        merge_schema[field] = \"new.\" + field\n",
    "    return spark_schema, merge_schema\n",
    "\n",
    "\n",
    "core_spark_schema, core_merge_schema = get_schemas(core_table_schema)\n",
    "labs_spark_schema, labs_merge_schema = get_schemas(labs_table_schema)\n",
    "active_problems_spark_schema, active_problems_merge_schema = get_schemas(active_problems_table_schema)\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"Update eCR Datastore\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_active_problems_schema = StructType(\n",
    "    [\n",
    "        StructField('eicr_id', StringType(), False), \n",
    "        StructField(\"active_problems\", ArrayType(\n",
    "            StructType(\n",
    "                [\n",
    "                    StructField(\"problem\", StringType(), True),\n",
    "                    StructField(\"problem_date\", DateType(), True)\n",
    "                ]\n",
    "            )\n",
    "        ), True)\n",
    "    ]\n",
    "    )\n",
    "\n",
    "test_labs_schema = StructType(\n",
    "    [\n",
    "        StructField('eicr_id', StringType(), False), \n",
    "        StructField(\"labs\", ArrayType(\n",
    "            StructType(\n",
    "                [\n",
    "                    StructField(\"test_type\", StringType(), True),\n",
    "                    StructField(\"test_type_code\", StringType(), True),\n",
    "                    StructField(\"test_type_system\", StringType(), True),\n",
    "                    StructField(\"test_result_qualitative\", StringType(), True),\n",
    "                    StructField(\"test_result_quantitative\", StringType(), True),\n",
    "                    StructField(\"test_result_units\", StringType(), True),\n",
    "                    StructField(\"test_result_code\", StringType(), True),\n",
    "                    StructField(\"test_result_code_display\", StringType(), True),\n",
    "                    StructField(\"test_result_interp\", StringType(), True),\n",
    "                    StructField(\"test_result_interp_code\", StringType(), True),\n",
    "                    StructField(\"test_result_interp_system\", StringType(), True),\n",
    "                    StructField(\"test_result_ref_range_low\", StringType(), True),\n",
    "                    StructField(\"test_result_ref_range_low_units\", StringType(), True),\n",
    "                    StructField(\"test_result_ref_range_high\", StringType(), True),\n",
    "                    StructField(\"test_result_ref_range_high_units\", StringType(), True),\n",
    "                    StructField(\"specimen_type\", StringType(), True),\n",
    "                    StructField(\"performing_lab\", StringType(), True),\n",
    "                    StructField(\"specimen_collection_date\", TimestampType(), True)\n",
    "                ]\n",
    "            )\n",
    "        ), True)\n",
    "    ]\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_PARSED_ECR_PATH = DELTA_TABLES_FILESYSTEM + \"raw_data_test\"\n",
    "from pyspark.sql.functions import explode_outer\n",
    "\n",
    "active_problems = spark.read.schema(test_active_problems_schema).json(test_PARSED_ECR_PATH,multiLine=True)\n",
    "active_problems = active_problems.select(active_problems.eicr_id,explode_outer(active_problems.active_problems).alias(\"problems\")).rdd.map(lambda x: (x.eicr_id, x.problems['problem'],x.problems['problem_date'])).toDF(['eicr_id','problem','problem_date'])\n",
    "active_problems.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_PARSED_ECR_PATH = DELTA_TABLES_FILESYSTEM + \"raw_data_test\"\n",
    "from pyspark.sql.functions import explode_outer\n",
    "\n",
    "labs = spark.read.schema(test_labs_schema).json(test_PARSED_ECR_PATH,multiLine=True)\n",
    "labs = labs.select(labs.eicr_id,explode_outer(labs.labs).alias(\"labs\")).rdd.map(\n",
    "    lambda x: (\n",
    "        x.eicr_id, \n",
    "        x.labs['test_type'],\n",
    "        x.labs['test_type_code'],\n",
    "        x.labs['test_type_system'],\n",
    "        x.labs['test_result_qualitative'],\n",
    "        x.labs['test_result_quantitative'],\n",
    "        x.labs['test_result_units'],\n",
    "        x.labs['test_result_code'],\n",
    "        x.labs['test_result_code_display'],\n",
    "        x.labs['test_result_interp'],\n",
    "        x.labs['test_result_interp_code'],\n",
    "        x.labs['test_result_interp_system'],\n",
    "        x.labs['test_result_ref_range_low'],\n",
    "        x.labs['test_result_ref_range_low_units'],\n",
    "        x.labs['test_result_ref_range_high'],\n",
    "        x.labs['test_result_ref_range_high_units'],\n",
    "        x.labs['specimen_type'],\n",
    "        x.labs['performing_lab'],\n",
    "        x.labs['specimen_collection_date'],\n",
    "         )).toDF([\n",
    "            'eicr_id',\n",
    "            'test_type',\n",
    "            'test_type_code',\n",
    "            'test_type_system',\n",
    "            'test_result_qualitative',\n",
    "            'test_result_quantitative',\n",
    "            'test_result_units',\n",
    "            'test_result_code',\n",
    "            'test_result_code_display',\n",
    "            'test_result_interp',\n",
    "            'test_result_interp_code',\n",
    "            'test_result_interp_system',\n",
    "            'test_result_ref_range_low',\n",
    "            'test_result_ref_range_low_units',\n",
    "            'test_result_ref_range_high',\n",
    "            'test_result_ref_range_high_units',\n",
    "            'specimen_type',\n",
    "            'performing_lab',\n",
    "            'specimen_collection_date'])\n",
    "labs.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON files into a DataFrame with the specified schema\n",
    "new_core_ecr_records = spark.read.schema(core_spark_schema).json(PARSED_ECR_PATH,multiLine=True)\n",
    "new_labs_ecr_records = spark.read.schema(labs_spark_schema).json(PARSED_ECR_PATH,multiLine=True)\n",
    "new_active_problems_ecr_records = spark.read.schema(active_problems_spark_schema).json(PARSED_ECR_PATH,multiLine=True)\n",
    "\n",
    "\n",
    "# Check if Delta table exists\n",
    "if DeltaTable.isDeltaTable(spark, ECR_DATASTORE_PATH):\n",
    "    # If the table exists add new records.\n",
    "    ecr_datastore = DeltaTable.forPath(spark, ECR_DATASTORE_PATH)\n",
    "\n",
    "    ecr_datastore.alias(\"old\").merge(\n",
    "        new_ecr_records.alias(\"new\"), \"old.eicr_id = new.eicr_id\"\n",
    "    ).whenNotMatchedInsert(values=merge_schema).execute()\n",
    "else:\n",
    "    # If Delta table doesn't exist, create it.\n",
    "    new_ecr_records.write.format(\"delta\").mode(\"append\").save(ECR_DATASTORE_PATH)\n",
    "\n",
    "# Make a copy of the Delta table in CSV format for easy access.\n",
    "ecr_datastore = DeltaTable.forPath(spark, ECR_DATASTORE_PATH).toDF()\n",
    "\n",
    "# Set up storage client\n",
    "container_url = f\"https://{STORAGE_ACCOUNT}.blob.core.windows.net/\"\n",
    "\n",
    "for format in DAILY_EXTRACT_FORMATS:\n",
    "\n",
    "    # Write standard pyspark directories for each file format\n",
    "    # Force pyspark to coalesce the results into a single file\n",
    "    format_path = ECR_DATASTORE_DAILY_EXTRACT_PATH + \".\" + format\n",
    "    modified_datastore_directory = BASE_DATASTORE_DIRECTORY + \".\" + format + \"/\"\n",
    "    ecr_datastore.coalesce(1).write.format(format).option(\"header\",True).mode('overwrite').save(format_path)\n",
    "\n",
    "    # Locate the file which actually has the data amidst the pyspark kruft\n",
    "    partial_file = \"\"\n",
    "    for f in mssparkutils.fs.ls(format_path):\n",
    "        file_in_namespace = f.path.split(\"/\")[-1]\n",
    "        if file_in_namespace.startswith(\"part-\") and file_in_namespace.endswith(\".\" + format):\n",
    "            partial_file = f.path\n",
    "\n",
    "    # Create a copy of just the data at the root level, formatted appropriately\n",
    "    mssparkutils.fs.cp(partial_file, DELTA_TABLES_FILESYSTEM + \"updated_ecr_datastore.\" + format)\n",
    "\n",
    "    # Now delete the pyspark junk folder by deleting all virtual filepaths\n",
    "    mssparkutils.fs.rm(format_path, recurse=True)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "save_output": true,
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
