{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# updateECRdatastore\n",
    "\n",
    "This notebook updates the ECR datastore delta table with new ECR records (`PARSED_ECR_PATH`); a new ECR datastore delta table is created if one does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install azure-identity azure-storage-blob azure-keyvault-secrets azure-storage-file-datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "STORAGE_ACCOUNT = \"$STORAGE_ACCOUNT\"\n",
    "BASE_DATASTORE_DIRECTORY = \"ecr-datastore\"\n",
    "ECR_DATASTORE_PATH = f\"abfss://delta-tables@{STORAGE_ACCOUNT}.dfs.core.windows.net/ecr-datastore\"\n",
    "ECR_DATASTORE_DAILY_EXTRACT_PATH = f\"abfss://delta-tables@{STORAGE_ACCOUNT}.dfs.core.windows.net/ecr-datastore\"\n",
    "PARSED_ECR_PATH = f\"abfss://delta-tables@{STORAGE_ACCOUNT}.dfs.core.windows.net/raw_data\"\n",
    "DAILY_EXTRACT_FORMATS = [\"csv\", \"parquet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import ClientSecretCredential\n",
    "\n",
    "# Set up key vault client\n",
    "vault_name = \"$KEY_VAULT\"\n",
    "vault_url = f\"https://{vault_name}.vault.azure.net/\"\n",
    "vault_linked_service = \"$KEY_VAULT_LINKED_SERVICE\"\n",
    "\n",
    "# Get client ID and secret for GitHub app registration\n",
    "# client_id = TokenLibrary.getSecret(vault_name,\"synapse-client-id\",vault_linked_service)\n",
    "# client_secret = TokenLibrary.getSecret(vault_name,\"synapse-client-secret\",vault_linked_service)\n",
    "client_id = \"\"\n",
    "client_secret = \"\"\n",
    "tenant_id = \"$TENANT_ID\"\n",
    "\n",
    "credential = ClientSecretCredential(client_id=client_id, client_secret=client_secret, tenant_id=tenant_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    FloatType,\n",
    "    BooleanType,\n",
    "    DateType,\n",
    "    TimestampType,\n",
    ")\n",
    "from delta.tables import *\n",
    "import json\n",
    "from typing import Tuple\n",
    "\n",
    "# Prepare Schema\n",
    "schema = {\n",
    "    \"patient_id\": [\"string\", False],\n",
    "    \"person_id\": [\"string\", False],\n",
    "    \"person_id_date_added\": [\"timestamp\", True],\n",
    "    \"iris_id\": [\"string\", True],\n",
    "    \"iris_id_date_added\": [\"timestamp\", True],\n",
    "    \"incident_id\": [\"string\", True],\n",
    "    \"incident_id_date_added\": [\"timestamp\", True],\n",
    "    \"last_name\": [\"string\", True],\n",
    "    \"first_name\": [\"string\", True],\n",
    "    \"birth_date\": [\"date\", True],\n",
    "    \"gender\": [\"string\", True],\n",
    "    \"race\": [\"string\", True],\n",
    "    \"ethnicity\": [\"string\", True],\n",
    "    \"rr_id\": [\"string\", True],\n",
    "    \"status\": [\"string\", True],\n",
    "    \"conditions\": [\"string\", True],\n",
    "    \"eicr_id\": [\"string\", False],\n",
    "    \"eicr_version_number\": [\"integer\", True],\n",
    "    \"authoring_datetime\": [\"timestamp\", True],\n",
    "    \"provider_id\": [\"string\", True],\n",
    "    \"facility_id_number\": [\"string\", True],\n",
    "    \"facility_name\": [\"string\", True],\n",
    "    \"facility_type\": [\"string\", True],\n",
    "    \"encounter_type\": [\"string\", True],\n",
    "    \"encounter_start_date\": [\"date\", True],\n",
    "    \"encounter_end_date\": [\"date\", True],\n",
    "    \"active_problem_1\": [\"string\", True],\n",
    "    \"active_problem_date_1\": [\"date\", True],\n",
    "    \"active_problem_2\": [\"string\", True],\n",
    "    \"active_problem_date_2\": [\"date\", True],\n",
    "    \"active_problem_3\": [\"string\", True],\n",
    "    \"active_problem_date_3\": [\"date\", True],\n",
    "    \"active_problem_4\": [\"string\", True],\n",
    "    \"active_problem_date_4\": [\"date\", True],\n",
    "    \"active_problem_5\": [\"string\", True],\n",
    "    \"active_problem_date_5\": [\"date\", True],\n",
    "    \"reason_for_visit\": [\"string\", True],\n",
    "    \"test_type_1\": [\"string\", True],\n",
    "    \"test_type_code_1\": [\"string\", True],\n",
    "    \"test_result_1\": [\"string\", True],\n",
    "    \"test_result_interp_1\": [\"string\", True],\n",
    "    \"specimen_type_1\": [\"string\", True],\n",
    "    \"performing_lab_1\": [\"string\", True],\n",
    "    \"specimen_collection_date_1\": [\"timestamp\", True],\n",
    "    \"result_date_1\": [\"timestamp\", True],\n",
    "    \"test_type_2\": [\"string\", True],\n",
    "    \"test_type_code_2\": [\"string\", True],\n",
    "    \"test_result_2\": [\"string\", True],\n",
    "    \"test_result_interp_2\": [\"string\", True],\n",
    "    \"specimen_type_2\": [\"string\", True],\n",
    "    \"performing_lab_2\": [\"string\", True],\n",
    "    \"specimen_collection_date_2\": [\"timestamp\", True],\n",
    "    \"result_date_2\": [\"timestamp\", True],\n",
    "    \"test_type_3\": [\"string\", True],\n",
    "    \"test_type_code_3\": [\"string\", True],\n",
    "    \"test_result_3\": [\"string\", True],\n",
    "    \"test_result_interp_3\": [\"string\", True],\n",
    "    \"specimen_type_3\": [\"string\", True],\n",
    "    \"performing_lab_3\": [\"string\", True],\n",
    "    \"specimen_collection_date_3\": [\"timestamp\", True],\n",
    "    \"result_date_3\": [\"timestamp\", True],\n",
    "    \"test_type_4\": [\"string\", True],\n",
    "    \"test_type_code_4\": [\"string\", True],\n",
    "    \"test_result_4\": [\"string\", True],\n",
    "    \"test_result_interp_4\": [\"string\", True],\n",
    "    \"specimen_type_4\": [\"string\", True],\n",
    "    \"performing_lab_4\": [\"string\", True],\n",
    "    \"specimen_collection_date_4\": [\"timestamp\", True],\n",
    "    \"result_date_4\": [\"timestamp\", True],\n",
    "    \"test_type_5\": [\"string\", True],\n",
    "    \"test_type_code_5\": [\"string\", True],\n",
    "    \"test_result_5\": [\"string\", True],\n",
    "    \"test_result_interp_5\": [\"string\", True],\n",
    "    \"specimen_type_5\": [\"string\", True],\n",
    "    \"performing_lab_5\": [\"string\", True],\n",
    "    \"specimen_collection_date_5\": [\"timestamp\", True],\n",
    "    \"result_date_5\": [\"timestamp\", True],\n",
    "    \"test_type_6\": [\"string\", True],\n",
    "    \"test_type_code_6\": [\"string\", True],\n",
    "    \"test_result_6\": [\"string\", True],\n",
    "    \"test_result_interp_6\": [\"string\", True],\n",
    "    \"specimen_type_6\": [\"string\", True],\n",
    "    \"performing_lab_6\": [\"string\", True],\n",
    "    \"specimen_collection_date_6\": [\"timestamp\", True],\n",
    "    \"result_date_6\": [\"timestamp\", True],\n",
    "    \"test_type_7\": [\"string\", True],\n",
    "    \"test_type_code_7\": [\"string\", True],\n",
    "    \"test_result_7\": [\"string\", True],\n",
    "    \"test_result_interp_7\": [\"string\", True],\n",
    "    \"specimen_type_7\": [\"string\", True],\n",
    "    \"performing_lab_7\": [\"string\", True],\n",
    "    \"specimen_collection_date_7\": [\"timestamp\", True],\n",
    "    \"result_date_7\": [\"timestamp\", True],\n",
    "    \"test_type_8\": [\"string\", True],\n",
    "    \"test_type_code_8\": [\"string\", True],\n",
    "    \"test_result_8\": [\"string\", True],\n",
    "    \"test_result_interp_8\": [\"string\", True],\n",
    "    \"specimen_type_8\": [\"string\", True],\n",
    "    \"performing_lab_8\": [\"string\", True],\n",
    "    \"specimen_collection_date_8\": [\"timestamp\", True],\n",
    "    \"result_date_8\": [\"timestamp\", True],\n",
    "    \"test_type_9\": [\"string\", True],\n",
    "    \"test_type_code_9\": [\"string\", True],\n",
    "    \"test_result_9\": [\"string\", True],\n",
    "    \"test_result_interp_9\": [\"string\", True],\n",
    "    \"specimen_type_9\": [\"string\", True],\n",
    "    \"performing_lab_9\": [\"string\", True],\n",
    "    \"specimen_collection_date_9\": [\"timestamp\", True],\n",
    "    \"result_date_9\": [\"timestamp\", True],\n",
    "    \"test_type_10\": [\"string\", True],\n",
    "    \"test_type_code_10\": [\"string\", True],\n",
    "    \"test_result_10\": [\"string\", True],\n",
    "    \"test_result_interp_10\": [\"string\", True],\n",
    "    \"specimen_type_10\": [\"string\", True],\n",
    "    \"performing_lab_10\": [\"string\", True],\n",
    "    \"specimen_collection_date_10\": [\"timestamp\", True],\n",
    "    \"result_date_10\": [\"timestamp\", True],\n",
    "    \"test_type_11\": [\"string\", True],\n",
    "    \"test_type_code_11\": [\"string\", True],\n",
    "    \"test_result_11\": [\"string\", True],\n",
    "    \"test_result_interp_11\": [\"string\", True],\n",
    "    \"specimen_type_11\": [\"string\", True],\n",
    "    \"performing_lab_11\": [\"string\", True],\n",
    "    \"specimen_collection_date_11\": [\"timestamp\", True],\n",
    "    \"result_date_11\": [\"timestamp\", True],\n",
    "    \"test_type_12\": [\"string\", True],\n",
    "    \"test_type_code_12\": [\"string\", True],\n",
    "    \"test_result_12\": [\"string\", True],\n",
    "    \"test_result_interp_12\": [\"string\", True],\n",
    "    \"specimen_type_12\": [\"string\", True],\n",
    "    \"performing_lab_12\": [\"string\", True],\n",
    "    \"specimen_collection_date_12\": [\"timestamp\", True],\n",
    "    \"result_date_12\": [\"timestamp\", True],\n",
    "    \"test_type_13\": [\"string\", True],\n",
    "    \"test_type_code_13\": [\"string\", True],\n",
    "    \"test_result_13\": [\"string\", True],\n",
    "    \"test_result_interp_13\": [\"string\", True],\n",
    "    \"specimen_type_13\": [\"string\", True],\n",
    "    \"performing_lab_13\": [\"string\", True],\n",
    "    \"specimen_collection_date_13\": [\"timestamp\", True],\n",
    "    \"result_date_13\": [\"timestamp\", True],\n",
    "    \"test_type_14\": [\"string\", True],\n",
    "    \"test_type_code_14\": [\"string\", True],\n",
    "    \"test_result_14\": [\"string\", True],\n",
    "    \"test_result_interp_14\": [\"string\", True],\n",
    "    \"specimen_type_14\": [\"string\", True],\n",
    "    \"performing_lab_14\": [\"string\", True],\n",
    "    \"specimen_collection_date_14\": [\"timestamp\", True],\n",
    "    \"result_date_14\": [\"timestamp\", True],\n",
    "    \"test_type_15\": [\"string\", True],\n",
    "    \"test_type_code_15\": [\"string\", True],\n",
    "    \"test_result_15\": [\"string\", True],\n",
    "    \"test_result_interp_15\": [\"string\", True],\n",
    "    \"specimen_type_15\": [\"string\", True],\n",
    "    \"performing_lab_15\": [\"string\", True],\n",
    "    \"specimen_collection_date_15\": [\"timestamp\", True],\n",
    "    \"result_date_15\": [\"timestamp\", True],\n",
    "    \"test_type_16\": [\"string\", True],\n",
    "    \"test_type_code_16\": [\"string\", True],\n",
    "    \"test_result_16\": [\"string\", True],\n",
    "    \"test_result_interp_16\": [\"string\", True],\n",
    "    \"specimen_type_16\": [\"string\", True],\n",
    "    \"performing_lab_16\": [\"string\", True],\n",
    "    \"specimen_collection_date_16\": [\"timestamp\", True],\n",
    "    \"result_date_16\": [\"timestamp\", True],\n",
    "    \"test_type_17\": [\"string\", True],\n",
    "    \"test_type_code_17\": [\"string\", True],\n",
    "    \"test_result_17\": [\"string\", True],\n",
    "    \"test_result_interp_17\": [\"string\", True],\n",
    "    \"specimen_type_17\": [\"string\", True],\n",
    "    \"performing_lab_17\": [\"string\", True],\n",
    "    \"specimen_collection_date_17\": [\"timestamp\", True],\n",
    "    \"result_date_17\": [\"timestamp\", True],\n",
    "    \"test_type_18\": [\"string\", True],\n",
    "    \"test_type_code_18\": [\"string\", True],\n",
    "    \"test_result_18\": [\"string\", True],\n",
    "    \"test_result_interp_18\": [\"string\", True],\n",
    "    \"specimen_type_18\": [\"string\", True],\n",
    "    \"performing_lab_18\": [\"string\", True],\n",
    "    \"specimen_collection_date_18\": [\"timestamp\", True],\n",
    "    \"result_date_18\": [\"timestamp\", True],\n",
    "    \"test_type_19\": [\"string\", True],\n",
    "    \"test_type_code_19\": [\"string\", True],\n",
    "    \"test_result_19\": [\"string\", True],\n",
    "    \"test_result_interp_19\": [\"string\", True],\n",
    "    \"specimen_type_19\": [\"string\", True],\n",
    "    \"performing_lab_19\": [\"string\", True],\n",
    "    \"specimen_collection_date_19\": [\"timestamp\", True],\n",
    "    \"result_date_19\": [\"timestamp\", True],\n",
    "    \"test_type_20\": [\"string\", True],\n",
    "    \"test_type_code_20\": [\"string\", True],\n",
    "    \"test_result_20\": [\"string\", True],\n",
    "    \"test_result_interp_20\": [\"string\", True],\n",
    "    \"specimen_type_20\": [\"string\", True],\n",
    "    \"performing_lab_20\": [\"string\", True],\n",
    "    \"specimen_collection_date_20\": [\"timestamp\", True],\n",
    "    \"result_date_20\": [\"timestamp\", True]\n",
    "  }\n",
    "\n",
    "def get_schemas(schema: dict) -> Tuple[StructType, dict]:\n",
    "    \"\"\"\n",
    "    Get a Spark StructType object from a JSON schema string.\n",
    "\n",
    "    :param schema: A dictionary defining the schema of the ECR datastore including \n",
    "        the data type of each field and whether null values are allowed. Should be of the form:\n",
    "        '{\"fieldname\": [<data type>, <nullable?(True/False)>]}'.\n",
    "    :return: A tuple containing a Spark StructType object representing the schema \n",
    "    and a dictionary defining field mappings for merge operations. \n",
    "    \"\"\"\n",
    "\n",
    "    schema_type_map = {\n",
    "        \"string\": StringType(),\n",
    "        \"integer\": IntegerType(),\n",
    "        \"float\": FloatType(),\n",
    "        \"boolean\": BooleanType(),\n",
    "        \"date\": DateType(),\n",
    "        \"timestamp\": TimestampType(),\n",
    "    }\n",
    "    spark_schema = StructType()\n",
    "    merge_schema = {}\n",
    "    for field in schema:\n",
    "        spark_schema.add(StructField(field, schema_type_map[schema[field][0]], schema[field][1]))\n",
    "        merge_schema[field] = \"new.\" + field\n",
    "    return spark_schema, merge_schema\n",
    "\n",
    "\n",
    "spark_schema, merge_schema = get_schemas(schema)\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"Update eCR Datastore\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "from datetime import datetime\n",
    "from azure.identity import ManagedIdentityCredential\n",
    "from azure.core.credentials import AccessToken\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "import time\n",
    "\n",
    "# Class to allow us to use the spark pool's managed identity to access blobs \n",
    "# and containers (to move them around and rename them)\n",
    "class spoof_token:\n",
    "    def get_token(*args, **kwargs):\n",
    "        return AccessToken(\n",
    "            token=mssparkutils.credentials.getToken(audience=\"storage\"),\n",
    "            expires_on=int(time.time())+60*10 # some random time in future... synapse doesn't document how to get the actual time\n",
    "        )\n",
    "\n",
    "# Read JSON files into a DataFrame with the specified schema\n",
    "new_ecr_records = spark.read.schema(spark_schema).json(PARSED_ECR_PATH,multiLine=True)\n",
    "\n",
    "# Check if Delta table exists\n",
    "if DeltaTable.isDeltaTable(spark, ECR_DATASTORE_PATH):\n",
    "    # If the table exists add new records.\n",
    "    ecr_datastore = DeltaTable.forPath(spark, ECR_DATASTORE_PATH)\n",
    "\n",
    "    ecr_datastore.alias(\"old\").merge(\n",
    "        new_ecr_records.alias(\"new\"), \"old.eicr_id = new.eicr_id\"\n",
    "    ).whenNotMatchedInsert(values=merge_schema).execute()\n",
    "else:\n",
    "    # If Delta table doesn't exist, create it.\n",
    "    new_ecr_records.write.format(\"delta\").mode(\"append\").save(ECR_DATASTORE_PATH)\n",
    "\n",
    "# Make a copy of the Delta table in CSV format for easy access.\n",
    "ecr_datastore = DeltaTable.forPath(spark, ECR_DATASTORE_PATH).toDF()\n",
    "\n",
    "# Get a credential so we can manipulate the created blobs\n",
    "credential = ManagedIdentityCredential()\n",
    "credential._credential = spoof_token() # monkey-patch the contents of the private `_credential`\n",
    "\n",
    "# Set up storage client\n",
    "container_url = f\"https://{STORAGE_ACCOUNT}.blob.core.windows.net/\"\n",
    "\n",
    "# Instantiate a new BlobServiceClient using connection string\n",
    "blob_service_client = BlobServiceClient(account_url=container_url,credential=credential)\n",
    "\n",
    "# Get client for the delta-tables container\n",
    "container_client = blob_service_client.get_container_client(container=\"delta-tables\")\n",
    "\n",
    "# We'll need the filesystem properties to manage the ADLS gen 2 storage, since it's not just blobs\n",
    "datalake_client = DataLakeServiceClient(container_url, credential)\n",
    "delta_tables_file_system_client = datalake_client.get_file_system_client(\"delta-tables\")\n",
    "file_system_props = delta_tables_file_system_client.get_file_system_properties()\n",
    "\n",
    "for format in DAILY_EXTRACT_FORMATS:\n",
    "\n",
    "    # Write standard pyspark directories for each file format\n",
    "    # Force pyspark to coalesce the results into a single file\n",
    "    format_path = ECR_DATASTORE_DAILY_EXTRACT_PATH + \".\" + format\n",
    "    modified_datastore_directory = BASE_DATASTORE_DIRECTORY + \".\" + format + \"/\"\n",
    "    ecr_datastore.coalesce(1).write.format(format).mode('overwrite').save(format_path)\n",
    "\n",
    "    # Locate the file which actually has the data amidst the pyspark kruft\n",
    "    blob_generator = container_client.list_blob_names(name_starts_with=modified_datastore_directory)\n",
    "    partial_file = \"\"\n",
    "    for blob_name in blob_generator:\n",
    "        filepath = str(blob_name).split(\"/\")[-1]\n",
    "        if filepath.startswith(\"part-\") and filepath.endswith(\".\" + format):\n",
    "            partial_file = blob_name\n",
    "\n",
    "    # Create a copy of just the data at the root level, formatted appropriately\n",
    "    new_blob = blob_service_client.get_blob_client(container=\"delta-tables\", blob=\"updated_ecr_datastore.\" + format)\n",
    "    blob_to_copy = container_client.get_blob_client(blob=partial_file)\n",
    "    new_blob.start_copy_from_url(blob_to_copy.url)\n",
    "\n",
    "    # Now delete the pyspark junk folder by deleting all virtual filepaths\n",
    "    # Have to delete the blob files first, then we can delete the directory\n",
    "    blob_generator = container_client.list_blob_names(name_starts_with=modified_datastore_directory)\n",
    "    for blob_name in blob_generator:\n",
    "        container_client.delete_blob(blob=blob_name)\n",
    "    try:\n",
    "        directory_client = datalake_client.get_directory_client(file_system=file_system_props, directory=modified_datastore_directory)\n",
    "        directory_client.delete_directory()\n",
    "    # Azure will complain that the operation results in a status of \"Request Accepted\"\n",
    "    # It will literally throw an error that the request is completed successfully...\n",
    "    # AND THEN it says the URI doesn't exist (after it deletes it...)\n",
    "    # So preserve your sanity and just ignore those warnings\n",
    "    except:\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "save_output": true,
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
