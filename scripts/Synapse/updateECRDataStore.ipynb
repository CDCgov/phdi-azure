{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# updateECRdatastore\n",
    "\n",
    "This notebook updates the ECR datastore delta table with new ECR records (`PARSED_ECR_PATH`); a new ECR datastore delta table is created if one does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "STORAGE_ACCOUNT = \"$STORAGE_ACCOUNT\"\n",
    "BASE_DATASTORE_DIRECTORY = \"ecr-datastore\"\n",
    "DELTA_TABLES_FILESYSTEM = f\"abfss://delta-tables@{STORAGE_ACCOUNT}.dfs.core.windows.net/\"\n",
    "ECR_DATASTORE_PATH = DELTA_TABLES_FILESYSTEM + \"ecr-datastore\"\n",
    "ECR_DATASTORE_DAILY_EXTRACT_PATH = DELTA_TABLES_FILESYSTEM + \"ecr-datastore\"\n",
    "PARSED_ECR_PATH = DELTA_TABLES_FILESYSTEM + \"raw_data\"\n",
    "DAILY_EXTRACT_FORMATS = [\"parquet\",\"csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils\n",
    "\n",
    "# Set up for writing to blob storage\n",
    "delta_bucket_name = \"delta-tables\"\n",
    "linked_service_name = \"$BLOB_STORAGE_LINKED_SERVICE\" \n",
    "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\n",
    "wasb_path = 'wasbs://%s@%s.blob.core.windows.net/' % (delta_bucket_name, STORAGE_ACCOUNT)\n",
    "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (delta_bucket_name, STORAGE_ACCOUNT), blob_sas_token)\n",
    "# Try mounting the remote storage directory at the mount point\n",
    "try:\n",
    "    mssparkutils.fs.mount(\n",
    "        wasb_path,\n",
    "        \"/\",\n",
    "        {\"LinkedService\": linked_service_name}\n",
    "    )\n",
    "except:\n",
    "    print(\"Already mounted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    FloatType,\n",
    "    BooleanType,\n",
    "    DateType,\n",
    "    TimestampType,\n",
    "    ArrayType\n",
    ")\n",
    "from pyspark.sql.functions import explode_outer\n",
    "\n",
    "# Prepare Schemas\n",
    "ecr_schema_path = f\"abfss://delta-tables@{STORAGE_ACCOUNT}.dfs.core.windows.net/ecr_datastore_config.json\"\n",
    "ecr_schema = spark.read.json(ecr_schema_path,multiLine=True)\n",
    "\n",
    "def prepare_schemas(ecr_schema):\n",
    "    table_schemas = {\n",
    "    \"core\":{\n",
    "        \"patient_id\": [\"string\", False],\n",
    "        \"person_id\": [\"string\", False],\n",
    "        \"person_id_date_added\": [\"timestamp\", True],\n",
    "        \"iris_id\": [\"string\", True],\n",
    "        \"iris_id_date_added\": [\"timestamp\", True],\n",
    "        \"incident_id\": [\"string\", True],\n",
    "        \"incident_id_date_added\": [\"timestamp\", True]}\n",
    "    }\n",
    "\n",
    "    row = ecr_schema.collect()[0].asDict()\n",
    "    for column_name, column_data in row.items():\n",
    "        if column_data['data_type'] != \"array\":\n",
    "            table_schemas[\"core\"][column_name] = [column_data['data_type'], column_data['nullable']]\n",
    "        else:\n",
    "            table_schemas[column_name] = {}\n",
    "            table_schemas[column_name]['eicr_id'] = ['string', False]\n",
    "            table_schemas[column_name][column_name] = {}\n",
    "            \n",
    "            for secondary_column_name, secondary_column_data in row[column_name]['secondary_schema'].asDict().items():\n",
    "                table_schemas[column_name][column_name][secondary_column_name] = [secondary_column_data['data_type'], secondary_column_data['nullable']]\n",
    "\n",
    "    return table_schemas\n",
    "\n",
    "def get_schemas(table_schemas: dict) -> dict[StructType]:\n",
    "    \"\"\"\n",
    "    Get a Spark StructType object from a JSON schema string.\n",
    "\n",
    "    :param schema: A dictionary defining the schema(s) of the ECR datastore including \n",
    "        the data type of each field and whether null values are allowed. Should be of the form:\n",
    "        '{\"fieldname\": [<data type>, <nullable?(True/False)>]}'.\n",
    "    :return: A dictionary containing a Spark StructType object representing the schema \n",
    "    and a dictionary defining field mappings for merge operations for each table. \n",
    "    \"\"\"\n",
    "\n",
    "    schema_type_map = {\n",
    "        \"string\": StringType(),\n",
    "        \"integer\": IntegerType(),\n",
    "        \"float\": FloatType(),\n",
    "        \"boolean\": BooleanType(),\n",
    "        \"date\": DateType(),\n",
    "        \"datetime\": TimestampType(),\n",
    "        \"number\": IntegerType()\n",
    "    }\n",
    "    spark_schemas = {}\n",
    "\n",
    "    for table_name, schema in table_schemas.items():\n",
    "        spark_schema = StructType()\n",
    "        flattened_df_schema = StructType()\n",
    "        merge_schema = {}\n",
    "        array_fields = []\n",
    "\n",
    "        for field in schema:\n",
    "            if isinstance(schema[field], dict):\n",
    "                for array_field, data in schema[field].items():\n",
    "                    array_fields.append(StructField(array_field, schema_type_map[data[0]],data[1]))\n",
    "                    merge_schema[array_field] = \"new.\" + array_field\n",
    "                    flattened_df_schema.add(StructField(array_field, schema_type_map[data[0]],data[1]))\n",
    "                spark_schema.add(StructField(field, ArrayType(\n",
    "                    StructType(array_fields)\n",
    "                )))\n",
    "                        \n",
    "            else:\n",
    "                spark_schema.add(StructField(field, schema_type_map[schema[field][0]], schema[field][1]))\n",
    "                flattened_df_schema.add(StructField(field, schema_type_map[schema[field][0]], schema[field][1]))\n",
    "                merge_schema[field] = \"new.\" + field\n",
    "\n",
    "        spark_schemas[table_name] = {\n",
    "            \"spark_schema\": spark_schema,\n",
    "            \"merge_schema\": merge_schema,\n",
    "            \"flattened_df_schema\": flattened_df_schema\n",
    "        }\n",
    "\n",
    "    return spark_schemas\n",
    "\n",
    "# Prepare Schemas \n",
    "table_schemas = prepare_schemas(ecr_schema)\n",
    "\n",
    "# Format table schemas for spark\n",
    "spark_schemas = get_schemas(table_schemas)\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"Update eCR Datastore\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import DeltaTable\n",
    "\n",
    "def update_ecr_datastore(schemas,table_name,table_schemas,ECR_DATASTORE_PATH, DELTA_TABLES_FILESYSTEM):\n",
    "    schema = schemas['spark_schema']\n",
    "    merge_schema = schemas['merge_schema']\n",
    "    flattened_df_schema = schemas['flattened_df_schema']\n",
    "\n",
    "    # Read JSON files into a DataFrame with the specified schema\n",
    "    print(\"reading raw JSON data...\")\n",
    "    new_ecr_records = spark.read.schema(schema).json(PARSED_ECR_PATH,multiLine=True)\n",
    "\n",
    "    print(\"Preparing data to be written to table...\")\n",
    "    # Transform non-core tables\n",
    "    if table_name != \"core\":\n",
    "        # Prep for exploding nested data\n",
    "        new_ecr_records = new_ecr_records.filter(new_ecr_records[table_name].isNotNull())\n",
    "        table_schema = table_schemas[table_name]\n",
    "        explode_cols = []\n",
    "        for col,col_data in table_schema.items():\n",
    "            if type(col_data) == dict:\n",
    "                explode_col = col\n",
    "                for c in col_data.keys():\n",
    "                    explode_cols.append(c)\n",
    "\n",
    "        new_ecr_records = new_ecr_records.selectExpr(\"eicr_id\", f\"explode({explode_col}) as alias\").select([\"eicr_id\"] + [f\"alias.{column}\" for column in explode_cols])\n",
    "    \n",
    "    # Temporary correction for quantitaitve values that are currently being parsed incorrectly as strings\n",
    "    if table_name == \"labs\":\n",
    "        string_to_float_columns = ['test_result_quantitative', 'test_result_ref_range_high', 'test_result_ref_range_low']\n",
    "        for column in string_to_float_columns:\n",
    "            new_ecr_records=new_ecr_records.withColumn(column, new_ecr_records[column].cast(\"float\").alias(column))\n",
    "    \n",
    "    # Check if the core table exists merge records otherwise create/overwrite the table\n",
    "    # TODO implement a primary key other tables similar to ecir_id in core. \n",
    "    #   Then we can stop overwriting in favor of a merge and ultimately only read in new or changed records as opposed to all of them as we are currently doing. \n",
    "    print(\"writing new records...\")\n",
    "    ECR_DATASTORE_PATH = ECR_DATASTORE_PATH + f\"-{table_name}\"\n",
    "    if DeltaTable.isDeltaTable(spark, ECR_DATASTORE_PATH) and table_name == \"core\":\n",
    "        ecr_datastore = DeltaTable.forPath(spark, ECR_DATASTORE_PATH)\n",
    "\n",
    "        ecr_datastore.alias(\"old\").merge(\n",
    "            new_ecr_records.alias(\"new\"), \"old.eicr_id = new.eicr_id\"\n",
    "        ).whenNotMatchedInsert(values=merge_schema).execute()\n",
    "    else:\n",
    "        # If Delta table doesn't exist, create it.\n",
    "        new_ecr_records.write.format(\"delta\").mode(\"overwrite\").save(ECR_DATASTORE_PATH)    \n",
    "\n",
    "    # Optimize query performance by compacting the deltatable\n",
    "    print(\"Optimizing...\")\n",
    "    ecr_datastore = DeltaTable.forPath(spark, ECR_DATASTORE_PATH)\n",
    "    ecr_datastore.optimize().executeCompaction()\n",
    "\n",
    "    print(\"Writing daily extracts...\")\n",
    "    # Make a copy of the Delta table in CSV format for easy access.\n",
    "    ecr_datastore = ecr_datastore.toDF()\n",
    "\n",
    "    for format in DAILY_EXTRACT_FORMATS:\n",
    "\n",
    "        # Write standard pyspark directories for each file format\n",
    "        # Force pyspark to coalesce the results into a single file\n",
    "        format_path = ECR_DATASTORE_PATH + \".\" + format\n",
    "        # modified_datastore_directory = ECR_DATASTORE_PATH + \".\" + format + \"/\"\n",
    "        ecr_datastore.coalesce(1).write.format(format).option(\"header\",True).mode('overwrite').save(format_path)\n",
    "\n",
    "        # Locate the file which actually has the data amidst the pyspark kruft\n",
    "        partial_file = \"\"\n",
    "        for f in mssparkutils.fs.ls(format_path):\n",
    "            file_in_namespace = f.path.split(\"/\")[-1]\n",
    "            if file_in_namespace.startswith(\"part-\") and file_in_namespace.endswith(\".\" + format):\n",
    "                partial_file = f.path\n",
    "\n",
    "        # Create a copy of just the data at the root level, formatted appropriately\n",
    "        mssparkutils.fs.cp(partial_file, DELTA_TABLES_FILESYSTEM + f\"/updated_ecr_datastore/updated_ecr_datastore-{table_name}.\" + format)\n",
    "\n",
    "        # Now delete the pyspark junk folder by deleting all virtual filepaths\n",
    "        mssparkutils.fs.rm(format_path, recurse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name, schemas in spark_schemas.items():\n",
    "    update_ecr_datastore(schemas,table_name,table_schemas, ECR_DATASTORE_PATH, DELTA_TABLES_FILESYSTEM)\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "save_output": true,
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
