{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### tabularizePatientTable\n",
        "This notebook extracts all data from the Patient table (`patient`) in the Master Patient Index and tabularizes the data into Lists of Lists of Lists (LoLoL)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "pip install psycopg2-binary azure-identity phdi recordlinkage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Database-related imports\n",
        "import psycopg2\n",
        "from psycopg2.sql import Identifier, SQL\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from phdi.linkage.postgres import DIBBsConnectorClient\n",
        "\n",
        "# Ground-truth labeling imports\n",
        "import time\n",
        "import pandas as pd\n",
        "import recordlinkage as rl\n",
        "from phdi.linkage import score_linkage_vs_truth\n",
        "from recordlinkage.base import BaseCompareFeature\n",
        "import numpy as np\n",
        "from phdi.harmonization import compare_strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# GLOBAL VARIABLES FOR DATABASE ACCESS\n",
        "\n",
        "# Set your Key Vault information\n",
        "vault_name = \"$KEY_VAULT\"\n",
        "KEY_VAULT_URL = f\"https://{vault_name}.vault.azure.net\"\n",
        "vault_linked_service = \"$KEY_VAULT_LINKED_SERVICE\"\n",
        "\n",
        "# Set up db_client\n",
        "DB_NAME = \"DibbsMpiDB\"\n",
        "DB_USER = \"postgres\"\n",
        "DB_HOST = \"$MPI_DB_HOST\"\n",
        "DB_PORT = \"5432\"\n",
        "DB_TABLE_PATIENT = \"patient\"\n",
        "DB_TABLE_PERSON= \"person\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GLOBAL VARIABLES FOR GROUND-TRUTH LABELING\n",
        "DATA_SIZE = None    # Optional variable; if none, use whole table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MPI ACCESS AND TABULATION FUNCTIONS\n",
        "\n",
        "# Generate a query to extract all data from the patient table of the MPI.\n",
        "def generate_query(db_client):\n",
        "    select_query_stubs = []\n",
        "    query_data = []\n",
        "    for key in db_client.fields_to_jsonpaths:\n",
        "        query = f\"jsonb_path_query_array(patient_resource,%s) as {key}\"\n",
        "        select_query_stubs.append(query)\n",
        "        query_data.append(db_client.fields_to_jsonpaths[key])\n",
        "\n",
        "    select_query = \"SELECT patient_id, \" + \", \".join(stub for stub in select_query_stubs)\n",
        "\n",
        "    query = select_query + \" FROM {patient_table};\"\n",
        "    query = SQL(query).format(patient_table=Identifier(db_client.patient_table))\n",
        "    return query, query_data\n",
        "\n",
        "\n",
        "# Format returned data as Lists of Lists of Lists (LoLoL)\n",
        "def format_data(data, db_client):\n",
        "    db_client._close_connections(db_conn=conn, db_cursor=cur)\n",
        "    data_cols = []\n",
        "    for key in sorted(list(db_client.fields_to_jsonpaths.keys())):\n",
        "        data_cols.append(key)\n",
        "    data_cols.insert(0, \"patient_id\")\n",
        "    data.insert(0, data_cols)\n",
        "\n",
        "    # Bring all data elements up one list level to avoid overly deep nesting\n",
        "    for i in range(1, len(data)):\n",
        "        for j in range(1, len(data[i])):\n",
        "            if len(data[i][j]) > 0:\n",
        "                data[i][j] = data[i][j][0]\n",
        "            else:\n",
        "                data[i][j] = \"\"\n",
        "    return data\n",
        "\n",
        "# Access the MPI Database\n",
        "credential = DefaultAzureCredential()\n",
        "db_password =  TokenLibrary.getSecret(vault_name,\"mpi-db-password\",vault_linked_service)\n",
        "db_client = DIBBsConnectorClient(database = DB_NAME, user = DB_USER, password = db_password, host= DB_HOST, port = DB_PORT, patient_table= DB_TABLE_PATIENT, person_table=DB_TABLE_PERSON)\n",
        "\n",
        "# Create a connection and a cursor\n",
        "conn = db_client.get_connection()\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Query for the data and format it\n",
        "query, query_data = generate_query(db_client)\n",
        "cur.execute(query, query_data)\n",
        "data = [list(row) for row in cur.fetchall()]\n",
        "formatted_data = format_data(data,db_client)\n",
        "\n",
        "# Apply any size caveats, if desired\n",
        "if DATA_SIZE is not None:\n",
        "    dataset = formatted_data[:min(DATA_SIZE+1, len(formatted_data))]\n",
        "else:\n",
        "    dataset = formatted_data\n",
        "labeling_set = pd.DataFrame(dataset[1:], columns=data[0])\n",
        "\n",
        "# Now, we need a copy of the data in a FHIR format for the linkage algorithms\n",
        "conn = db_client.get_connection()\n",
        "cur = conn.cursor()\n",
        "query = \"SELECT patient_resource from patient;\"\n",
        "cur.execute(query)\n",
        "fhir_data = [list(row)[0] for row in cur.fetchall()]\n",
        "db_client._close_connections(db_conn=conn, db_cursor=cur)\n",
        "\n",
        "if DATA_SIZE is not None:\n",
        "    evaluation_set = fhir_data[:min(DATA_SIZE+1, len(formatted_data))]\n",
        "else:\n",
        "    evaluation_set = fhir_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GROUND TRUTH LABELING: VIRGINIA FUNCTIONS\n",
        "\n",
        "# Transform a recordlinkage toolkit multi-index into a set of candidate tuples\n",
        "def get_pred_match_dict_from_multi_idx(mltidx, n_rows):\n",
        "    candidate_tuples = mltidx.to_list()\n",
        "    pred_matches = {k: set() for k in range(n_rows)}\n",
        "    for pair in candidate_tuples:\n",
        "        reference_record = min(pair)\n",
        "        linked_record = max(pair)\n",
        "        pred_matches[reference_record].add(linked_record)\n",
        "    return pred_matches\n",
        "\n",
        "\n",
        "# Special class for comparing LoL first name elements\n",
        "# Use the full concatenation of all names to account for multiple given names\n",
        "class CompareNestedString(BaseCompareFeature):\n",
        "    def _compute_vectorized(self, s1, s2):\n",
        "        return (s1.str[0] == s2.str[0]).astype(float)\n",
        "\n",
        "def get_va_labels(data):\n",
        "    start = time.time()\n",
        "\n",
        "    # Create a full index on patient table so we don't miss any pairs\n",
        "    indexer = rl.Index()\n",
        "    indexer.full()\n",
        "    candidate_links = indexer.index(data)\n",
        "    # Note: using a multi-indexer treats the row number as the index, so\n",
        "    # results will automatically be in acceptable eval format\n",
        "\n",
        "    print(len(candidate_links), \"candidate pairs identified\")\n",
        "\n",
        "    # Apply feature comparisons on each supported field from the MPI\n",
        "    comp = rl.Compare()\n",
        "    comp.add(CompareNestedString(\"first_name\", \"first_name\",label=\"first_name\"))\n",
        "    comp.exact(\"last_name\", \"last_name\", label=\"last_name\")\n",
        "    comp.exact(\"birthdate\", \"birthdate\", label=\"birthdate\")\n",
        "    comp.add(CompareNestedString(\"address\", \"address\", label=\"address\"))\n",
        "    features = comp.compute(candidate_links, data)\n",
        "    matches = features[features.sum(axis=1) == 4]\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Computation took\", str(round(end - start, 2)), \"seconds\")\n",
        "\n",
        "    n_rows = DATA_SIZE if DATA_SIZE is not None else len(data)\n",
        "    matches = get_pred_match_dict_from_multi_idx(matches.index, n_rows)\n",
        "    return matches\n",
        "\n",
        "va_labels = get_va_labels(labeling_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GROUND-TRUTH LABELING: RECORD LINKAGE TOOLKIT FUNCTIONS\n",
        "\n",
        "# Special class for comparing LoL first name elements\n",
        "# Use the full concatenation of all names to account for multiple given names\n",
        "class CompareFirstName(BaseCompareFeature):\n",
        "    def _compute_vectorized(self, s1, s2):\n",
        "        jarowinklers = np.vectorize(compare_strings)(s1.str.join(\" \"), s2.str.join(\" \"))\n",
        "        return jarowinklers >= 0.85\n",
        "\n",
        "\n",
        "# Special class for comparing LoL address line elements\n",
        "# Check each address line against each other address line to account for moving\n",
        "class CompareAddress(BaseCompareFeature):\n",
        "    def _compute_vectorized(self, s1, s2):\n",
        "\n",
        "        def comp_address_fields(a1_list, a2_list):\n",
        "            best_score = 0.0\n",
        "            for a1 in a1_list:\n",
        "                for a2 in a2_list:\n",
        "                    score = compare_strings(a1, a2)\n",
        "                    if score >= best_score:\n",
        "                        best_score = score\n",
        "            return best_score\n",
        "\n",
        "        jarowinklers = np.vectorize(comp_address_fields)(s1, s2)\n",
        "        return jarowinklers >= 0.85\n",
        "    \n",
        "\n",
        "def predict_third_party_labels(data):\n",
        "    start = time.time()\n",
        "\n",
        "    # Create a full index on patient table so we don't miss any pairs\n",
        "    indexer = rl.Index()\n",
        "    indexer.full()\n",
        "    candidate_links = indexer.index(data)\n",
        "    # Note: using a multi-indexer treats the row number as the index, so\n",
        "    # results will automatically be in acceptable eval format\n",
        "\n",
        "    print(len(candidate_links), \"candidate pairs identified\")\n",
        "\n",
        "    # Apply feature comparisons on each supported field from the MPI\n",
        "    comp = rl.Compare()\n",
        "    comp.add(CompareFirstName(\"first_name\", \"first_name\",label=\"first_name\"))\n",
        "    comp.string(\n",
        "        \"last_name\", \"last_name\", method=\"jarowinkler\", threshold=0.85, label=\"last_name\"\n",
        "    )\n",
        "    comp.string(\"mrn\", \"mrn\", method=\"jarowinkler\", threshold=0.85, label=\"mrn\")\n",
        "    comp.string(\n",
        "        \"birthdate\", \"birthdate\", method=\"jarowinkler\", threshold=0.85, label=\"birthdate\"\n",
        "    )\n",
        "    comp.add(CompareAddress(\"address\", \"address\", label=\"address\"))\n",
        "    comp.string(\"city\", \"city\", method=\"jarowinkler\", threshold=0.85, label=\"city\")\n",
        "    comp.string(\"state\", \"state\", method=\"jarowinkler\", threshold=0.85, label=\"state\")\n",
        "    comp.string(\"zip\", \"zip\", method=\"jarowinkler\", threshold=0.85, label=\"zip\")\n",
        "    comp.string(\"sex\", \"sex\", method=\"jarowinkler\", threshold=0.85, label=\"sex\")\n",
        "    features = comp.compute(candidate_links, data)\n",
        "\n",
        "    # Create an EM Predictor and label the binary training vectors\n",
        "    clf = rl.ECMClassifier()\n",
        "    pred_links = clf.fit_predict(features)\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Computation took\", str(round(end - start, 2)), \"seconds\")\n",
        "\n",
        "    n_rows = DATA_SIZE if DATA_SIZE is not None else len(data)\n",
        "    matches = get_pred_match_dict_from_multi_idx(pred_links, n_rows)\n",
        "    return matches\n",
        "\n",
        "third_party_labels = predict_third_party_labels(labeling_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LINKAGE DRIVER FUNCTIONS\n",
        "\n",
        "from phdi.linkage.link import (\n",
        "    _flatten_patient_resource,\n",
        "    _bind_func_names_to_invocations,\n",
        "    extract_blocking_values_from_record,\n",
        "    _compare_records,\n",
        "    extract_blocking_values_from_record\n",
        ")\n",
        "from typing import List\n",
        "import copy\n",
        "\n",
        "def link_fhir_record_from_dataset(\n",
        "    record: dict,\n",
        "    algo_config: List[dict],\n",
        "    db_client\n",
        ") -> List:\n",
        "\n",
        "    flattened_record = _flatten_patient_resource(record)\n",
        "\n",
        "    # Need to bind function names back to their symbolic invocations\n",
        "    # in context of the module--i.e. turn the string of a function\n",
        "    # name back into the callable defined in link.py\n",
        "    algo_config = copy.deepcopy(algo_config)\n",
        "    algo_config = _bind_func_names_to_invocations(algo_config)\n",
        "\n",
        "    # Accumulate all matches across all passes to return\n",
        "    found_matches = []\n",
        "    for linkage_pass in algo_config:\n",
        "        blocking_fields = linkage_pass[\"blocks\"]\n",
        "\n",
        "        # MPI will be able to find patients if *any* of their names or addresses\n",
        "        # contains extracted values, so minimally block on the first line\n",
        "        # if applicable\n",
        "        field_blocks = extract_blocking_values_from_record(record, blocking_fields)\n",
        "        data_block = db_client.block_data(field_blocks)\n",
        "\n",
        "        # First row of returned block is column headers\n",
        "        # Map column name to idx, not including patient/person IDs\n",
        "        col_to_idx = {v: k for k, v in enumerate(data_block[0][2:])}\n",
        "        data_block = data_block[1:]\n",
        "\n",
        "        # Blocked fields are returned as LoLoL, but only name / address\n",
        "        # need to preserve multiple elements, so flatten other fields\n",
        "        for i in range(len(data_block)):\n",
        "            blocked_record = data_block[i]\n",
        "            for j in range(len(blocked_record)):\n",
        "                # patient_id, person_id not included in col->idx mapping\n",
        "                if j < 2:\n",
        "                    continue\n",
        "                if len(blocked_record[j]) > 0:\n",
        "                    blocked_record[j] = blocked_record[j][0]\n",
        "                else:\n",
        "                    blocked_record[j] = \"\"\n",
        "\n",
        "        # Check if incoming record matches each thing it blocked with\n",
        "        kwargs = linkage_pass.get(\"kwargs\", {})\n",
        "        for blocked_record in data_block:\n",
        "            is_match = _compare_records(\n",
        "                flattened_record,\n",
        "                blocked_record,\n",
        "                linkage_pass[\"funcs\"],\n",
        "                col_to_idx,\n",
        "                linkage_pass[\"matching_rule\"],\n",
        "                **kwargs\n",
        "            )\n",
        "            if is_match:\n",
        "                found_matches.append(blocked_record[0])\n",
        "    \n",
        "    return found_matches\n",
        "\n",
        "def map_patient_ids_to_idxs(pids: List, data: pd.DataFrame):\n",
        "    record_idxs = []\n",
        "    for pid in pids:\n",
        "        row_idx = data[data['patient_id'] == pid].index.values[0]\n",
        "        record_idxs.append(row_idx)\n",
        "    return record_idxs\n",
        "\n",
        "\n",
        "def link_all_fhir_records_block_dataset(records: List, algo_config: List[dict], db_client, label_set):\n",
        "    found_matches = {}\n",
        "    for record in records:\n",
        "        ridx = map_patient_ids_to_idxs([record.get(\"id\")], label_set)[0]\n",
        "        linked_records = link_fhir_record_from_dataset(record, algo_config, db_client)\n",
        "        linked_idxs = map_patient_ids_to_idxs(linked_records, label_set)\n",
        "        idx_set = set(linked_idxs)\n",
        "        if ridx in idx_set:\n",
        "            idx_set.remove(ridx)\n",
        "        found_matches[ridx] = idx_set\n",
        "    return found_matches\n",
        "\n",
        "\n",
        "def dedupe_match_double_counts(match_dict):\n",
        "    for k in match_dict:\n",
        "        if k > 0:\n",
        "            lower_set = set(list(range(k)))\n",
        "            match_dict[k] = match_dict[k].difference(lower_set)\n",
        "    return match_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ALGORITHM EVALUATION: LAC EXISTING\n",
        "\n",
        "LAC_ALGO = [\n",
        "    {\n",
        "        \"funcs\": {\n",
        "            \"first_name\": \"feature_match_fuzzy_string\",\n",
        "            \"last_name\": \"feature_match_fuzzy_string\",\n",
        "            \"address\": \"feature_match_fuzzy_string\",\n",
        "            \"mrn\": \"feature_match_fuzzy_string\",\n",
        "        },\n",
        "        \"blocks\": [\n",
        "            {\"value\": \"first_name\", \"transformation\": \"first4\"},\n",
        "            {\"value\": \"last_name\", \"transformation\": \"first4\"},\n",
        "            {\"value\": \"birthdate\"},\n",
        "        ],\n",
        "        \"matching_rule\": \"eval_perfect_match\",\n",
        "        \"cluster_ratio\": 0.9,\n",
        "    },\n",
        "    {\n",
        "        \"funcs\": {\n",
        "            \"first_name\": \"feature_match_fuzzy_string\",\n",
        "            \"last_name\": \"feature_match_fuzzy_string\",\n",
        "            \"address\": \"feature_match_fuzzy_string\",\n",
        "            \"mrn\": \"feature_match_fuzzy_string\",\n",
        "        },\n",
        "        \"blocks\": [\n",
        "            {\"value\": \"first_name\", \"transformation\": \"first4\"},\n",
        "            {\"value\": \"last_name\", \"transformation\": \"first4\"},\n",
        "            {\"value\": \"address\", \"transformation\": \"first4\"},\n",
        "        ],\n",
        "        \"matching_rule\": \"eval_perfect_match\",\n",
        "        \"cluster_ratio\": 0.9,\n",
        "    },\n",
        "    {\n",
        "        \"funcs\": {\n",
        "            \"first_name\": \"feature_match_fuzzy_string\",\n",
        "            \"last_name\": \"feature_match_fuzzy_string\",\n",
        "            \"address\": \"feature_match_fuzzy_string\",\n",
        "            \"mrn\": \"feature_match_fuzzy_string\",\n",
        "        },\n",
        "        \"blocks\": [\n",
        "            {\"value\": \"birthdate\"},\n",
        "        ],\n",
        "        \"matching_rule\": \"eval_perfect_match\",\n",
        "        \"cluster_ratio\": 0.9,\n",
        "    },\n",
        "]\n",
        "\n",
        "found_matches_lac = link_all_fhir_records_block_dataset(evaluation_set, DIBBS_BASIC, db_client, labeling_set)\n",
        "found_matches_lac = dedupe_match_double_counts(found_matches_lac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ALGORITHM EVALUATION: DIBBs BASIC\n",
        "from phdi.linkage import DIBBS_BASIC\n",
        "found_matches_dibbs_basic = link_all_fhir_records_block_dataset(evaluation_set, DIBBS_BASIC, db_client, labeling_set)\n",
        "found_matches_dibbs_basic = dedupe_match_double_counts(found_matches_dibbs_basic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ALGORITHM EVALUATION: DIBBs ENHANCED\n",
        "from phdi.linkage import DIBBS_ENHANCED\n",
        "found_matches_dibbs_enhanced = link_all_fhir_records_block_dataset(evaluation_set, DIBBS_ENHANCED, db_client, labeling_set)\n",
        "found_matches_dibbs_enhanced = dedupe_match_double_counts(found_matches_dibbs_enhanced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RUN THE NUMBERS AND GET THE STATS FUNCTIONS\n",
        "\n",
        "'''\n",
        "To ensure accurate statistics, the matches and the true matches dictionaries\n",
        "in the statistical evaluation function should have the following form:\n",
        "\n",
        "{\n",
        "    row_num_of_record_in_data: set(row_nums_of_linked_records)\n",
        "}\n",
        "\n",
        "Each row in the data should be represented as a key in both dictionaries.\n",
        "The value for each of these keys should be a set that contains all other\n",
        "row numbers for records in the data set that link to the key record.\n",
        "'''\n",
        "def display_statistical_evaluation(\n",
        "    matches: dict, true_matches: dict, num_recs: int, cluster_mode_used: bool = False\n",
        "):\n",
        "    sensitivitiy, specificity, ppv, f1 = score_linkage_vs_truth(\n",
        "        matches, true_matches, num_recs, cluster_mode_used\n",
        "    )\n",
        "    print(\"Sensitivity:\", sensitivitiy)\n",
        "    print(\"Specificity:\", specificity)\n",
        "    print(\"PPV:\", ppv)\n",
        "    print(\"F1:\", f1)\n",
        "\n",
        "if DATA_SIZE is not None:\n",
        "    n_records = DATA_SIZE\n",
        "else:\n",
        "    n_records = len(evaluation_set)\n",
        "\n",
        "print(\"DISPLAYING EVALUATION ON VA LABELS:\")\n",
        "print()\n",
        "print(\"LAC Existing Algorithm:\")\n",
        "display_statistical_evaluation(found_matches_lac, va_labels, n_records)\n",
        "print()\n",
        "print(\"DIBBs Basic Algorithm:\")\n",
        "display_statistical_evaluation(found_matches_dibbs_basic, va_labels, n_records)\n",
        "print()\n",
        "print(\"DIBBs Log-Odds Algorithm:\")\n",
        "display_statistical_evaluation(found_matches_dibbs_enhanced, va_labels, n_records)\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n",
        "print(\"DISPLAYING EVALUATION ON EMC LABELS:\")\n",
        "print()\n",
        "print(\"LAC Existing Algorithm:\")\n",
        "display_statistical_evaluation(found_matches_lac, third_party_labels, n_records)\n",
        "print()\n",
        "print(\"DIBBs Basic Algorithm:\")\n",
        "display_statistical_evaluation(found_matches_dibbs_basic, third_party_labels, n_records)\n",
        "print()\n",
        "print(\"DIBBs Log-Odds Algorithm:\")\n",
        "display_statistical_evaluation(found_matches_dibbs_enhanced, third_party_labels, n_records)\n"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
