{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### tabularizePatientTable\n",
        "This notebook extracts all data from the Patient table (`patient`) in the Master Patient Index and tabularizes the data into Lists of Lists of Lists (LoLoL)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "pip install psycopg2-binary azure-identity phdi recordlinkage azure-keyvault-secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Database-related imports\n",
        "import psycopg2\n",
        "from psycopg2.sql import Identifier, SQL\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from phdi.linkage.postgres import DIBBsConnectorClient\n",
        "\n",
        "# Ground-truth labeling imports\n",
        "import time\n",
        "import pandas as pd\n",
        "import recordlinkage as rl\n",
        "from recordlinkage.base import BaseCompareFeature\n",
        "import numpy as np\n",
        "from phdi.harmonization import compare_strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# GLOBAL VARIABLES FOR DATABASE ACCESS\n",
        "\n",
        "# Set your Key Vault information\n",
        "vault_name = \"$KEY_VAULT\"\n",
        "KEY_VAULT_URL = f\"https://{vault_name}.vault.azure.net\"\n",
        "vault_linked_service = \"$KEY_VAULT_LINKED_SERVICE\"\n",
        "\n",
        "# Set up db_client\n",
        "DB_NAME = \"DibbsMpiDB\"\n",
        "DB_USER = \"postgres\"\n",
        "DB_HOST = \"$MPI_DB_HOST\"\n",
        "DB_PORT = \"5432\"\n",
        "DB_TABLE_PATIENT = \"patient\"\n",
        "DB_TABLE_PERSON= \"person\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GLOBAL VARIABLES FOR GROUND-TRUTH LABELING\n",
        "DATA_SIZE = None    # Optional variable; if none, use whole table\n",
        "WINDOW_INDEX_SIZE = 13\n",
        "JARO_THRESHOLD = 0.85"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MPI ACCESS AND TABULATION FUNCTIONS\n",
        "\n",
        "# Generate a query to extract all data from the patient table of the MPI.\n",
        "def generate_query(db_client):\n",
        "    select_query_stubs = []\n",
        "    query_data = []\n",
        "    for key in db_client.fields_to_jsonpaths:\n",
        "        query = f\"jsonb_path_query_array(patient_resource,%s) as {key}\"\n",
        "        select_query_stubs.append(query)\n",
        "        query_data.append(db_client.fields_to_jsonpaths[key])\n",
        "\n",
        "    select_query = \"SELECT patient_id, \" + \", \".join(stub for stub in select_query_stubs)\n",
        "\n",
        "    query = select_query + \" FROM {patient_table};\"\n",
        "    query = SQL(query).format(patient_table=Identifier(db_client.patient_table))\n",
        "    return query, query_data\n",
        "\n",
        "\n",
        "# Format returned data as Lists of Lists of Lists (LoLoL)\n",
        "def format_data(data, db_client):\n",
        "    db_client._close_connections(db_conn=conn, db_cursor=cur)\n",
        "    data_cols = []\n",
        "    for key in sorted(list(db_client.fields_to_jsonpaths.keys())):\n",
        "        data_cols.append(key)\n",
        "    data_cols.insert(0, \"patient_id\")\n",
        "    data.insert(0, data_cols)\n",
        "\n",
        "    # Bring all data elements up one list level to avoid overly deep nesting\n",
        "    for i in range(1, len(data)):\n",
        "        for j in range(1, len(data[i])):\n",
        "            if len(data[i][j]) > 0:\n",
        "                data[i][j] = data[i][j][0]\n",
        "            else:\n",
        "                data[i][j] = \"\"\n",
        "    return data\n",
        "\n",
        "# Access the MPI Database\n",
        "credential = DefaultAzureCredential()\n",
        "db_password =  TokenLibrary.getSecret(vault_name,\"mpi-db-password\",vault_linked_service)\n",
        "db_client = DIBBsConnectorClient(database = DB_NAME, user = DB_USER, password = db_password, host= DB_HOST, port = DB_PORT, patient_table= DB_TABLE_PATIENT, person_table=DB_TABLE_PERSON)\n",
        "\n",
        "# Create a connection and a cursor\n",
        "conn = db_client.get_connection()\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Query for the data and format it\n",
        "query, query_data = generate_query(db_client)\n",
        "cur.execute(query, query_data)\n",
        "data = [list(row) for row in cur.fetchall()]\n",
        "formatted_data = format_data(data,db_client)\n",
        "\n",
        "# Apply any size caveats, if desired\n",
        "if DATA_SIZE is not None:\n",
        "    dataset = formatted_data[:min(DATA_SIZE+1, len(formatted_data))]\n",
        "else:\n",
        "    dataset = formatted_data\n",
        "labeling_set = pd.DataFrame(dataset[1:], columns=data[0])\n",
        "\n",
        "# Now, we need a copy of the data in a FHIR format for the linkage algorithms\n",
        "conn = db_client.get_connection()\n",
        "cur = conn.cursor()\n",
        "query = \"SELECT patient_resource from patient;\"\n",
        "cur.execute(query)\n",
        "fhir_data = [list(row)[0] for row in cur.fetchall()]\n",
        "db_client._close_connections(db_conn=conn, db_cursor=cur)\n",
        "\n",
        "if DATA_SIZE is not None:\n",
        "    evaluation_set = fhir_data[:min(DATA_SIZE+1, len(formatted_data))]\n",
        "else:\n",
        "    evaluation_set = fhir_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GROUND TRUTH LABELING: VIRGINIA FUNCTIONS\n",
        "\n",
        "from recordlinkage.index import SortedNeighbourhood, BaseIndexAlgorithm\n",
        "from recordlinkage.utils import listify\n",
        "\n",
        "'''\n",
        "A custom Indexing function built to operate compatibly on the first_name column\n",
        "returned from the MPI. Since that's a list of strings (because someone could have\n",
        "multiple given names), we need a way to cross-conjoin these entries and apply\n",
        "the same fuzzy blocking filter window that a regular column of strings would get.\n",
        "This performs joint name concatenation on copies of that column in the data, and\n",
        "then uses an edit distance neighborhood to find fuzzy blocking candidates.\n",
        "'''\n",
        "class FirstNameSortedNeighborhood(BaseIndexAlgorithm):\n",
        "    def __init__(\n",
        "        self,\n",
        "        left_on=None,\n",
        "        right_on=None,\n",
        "        window=3,\n",
        "        sorting_key_values=None,\n",
        "        block_on=[],\n",
        "        block_left_on=[],\n",
        "        block_right_on=[],\n",
        "        **kwargs\n",
        "    ):\n",
        "        super(FirstNameSortedNeighborhood, self).__init__(**kwargs)\n",
        "\n",
        "        # variables to block on\n",
        "        self.left_on = left_on\n",
        "        self.right_on = right_on\n",
        "        self.window = window\n",
        "        self.sorting_key_values = sorting_key_values\n",
        "        self.block_on = block_on\n",
        "        self.block_left_on = block_left_on\n",
        "        self.block_right_on = block_right_on\n",
        "\n",
        "    def _get_left_and_right_on(self):\n",
        "        \"\"\"\n",
        "        We only care about the de-dupe case which involves no self.right, but this\n",
        "        still needs to be implemented for super compatibility.\n",
        "        \"\"\"\n",
        "        if self.right_on is None:\n",
        "            return (self.left_on, self.left_on)\n",
        "        else:\n",
        "            return (self.left_on, self.right_on)\n",
        "\n",
        "    def _get_sorting_key_values(self, array1, array2):\n",
        "        \"\"\"\n",
        "        Return the sorting key values as a series. This function is required by the\"\n",
        "        package for multi-index neighborhood filtering according to some papers it's\"\n",
        "        built on.\n",
        "        \"\"\"\n",
        "\n",
        "        concat_arrays = np.concatenate([array1, array2])\n",
        "        return np.unique(concat_arrays)\n",
        "\n",
        "    def _link_index(self, df_a, df_b):\n",
        "        df_a = df_a.copy()\n",
        "        df_b = df_b.copy()\n",
        "        df_a[\"first_name\"] = df_a[\"first_name\"].str.join(\" \")\n",
        "        df_b[\"first_name\"] = df_a[\"first_name\"].str.join(\" \")\n",
        "        left_on, right_on = self._get_left_and_right_on()\n",
        "        left_on = listify(left_on)\n",
        "        right_on = listify(right_on)\n",
        "\n",
        "        window = self.window\n",
        "\n",
        "        # Correctly generate blocking keys\n",
        "        block_left_on = listify(self.block_left_on)\n",
        "        block_right_on = listify(self.block_right_on)\n",
        "\n",
        "        if self.block_on:\n",
        "            block_left_on = listify(self.block_on)\n",
        "            block_right_on = listify(self.block_on)\n",
        "\n",
        "        blocking_keys = [\"sorting_key\"] + [\n",
        "            \"blocking_key_%d\" % i for i, v in enumerate(block_left_on)\n",
        "        ]\n",
        "\n",
        "        # Format the data to thread with index pairs\n",
        "        data_left = pd.DataFrame(df_a[listify(left_on) + block_left_on], copy=False)\n",
        "        data_left.columns = blocking_keys\n",
        "        data_left[\"index_x\"] = np.arange(len(df_a))\n",
        "        data_left.dropna(axis=0, how=\"any\", subset=blocking_keys, inplace=True)\n",
        "\n",
        "        data_right = pd.DataFrame(df_b[listify(right_on) + block_right_on], copy=False)\n",
        "        data_right.columns = blocking_keys\n",
        "        data_right[\"index_y\"] = np.arange(len(df_b))\n",
        "        data_right.dropna(axis=0, how=\"any\", subset=blocking_keys, inplace=True)\n",
        "\n",
        "        # sorting_key_values is the terminology in Data Matching [Christen,\n",
        "        # 2012]\n",
        "        if self.sorting_key_values is None:\n",
        "            self.sorting_key_values = self._get_sorting_key_values(\n",
        "                data_left[\"sorting_key\"].values, data_right[\"sorting_key\"].values\n",
        "            )\n",
        "\n",
        "        sorting_key_factors = pd.Series(\n",
        "            np.arange(len(self.sorting_key_values)), index=self.sorting_key_values\n",
        "        )\n",
        "\n",
        "        data_left[\"sorting_key\"] = data_left[\"sorting_key\"].map(sorting_key_factors)\n",
        "        data_right[\"sorting_key\"] = data_right[\"sorting_key\"].map(sorting_key_factors)\n",
        "\n",
        "        # Internal window size\n",
        "        _window = int((window - 1) / 2)\n",
        "\n",
        "        def merge_lagged(x, y, w):\n",
        "            \"\"\"Merge two dataframes with a lag on in the sorting key.\"\"\"\n",
        "\n",
        "            y = y.copy()\n",
        "            y[\"sorting_key\"] = y[\"sorting_key\"] + w\n",
        "\n",
        "            return x.merge(y, how=\"inner\")\n",
        "\n",
        "        pairs_concat = [\n",
        "            merge_lagged(data_left, data_right, w) for w in range(-_window, _window + 1)\n",
        "        ]\n",
        "\n",
        "        pairs_df = pd.concat(pairs_concat, axis=0)\n",
        "\n",
        "        return pd.MultiIndex(\n",
        "            levels=[df_a.index.values, df_b.index.values],\n",
        "            codes=[pairs_df[\"index_x\"].values, pairs_df[\"index_y\"].values],\n",
        "            verify_integrity=False,\n",
        "        )\n",
        "\n",
        "\n",
        "# Transform a recordlinkage toolkit multi-index into a set of candidate tuples\n",
        "def get_pred_match_dict_from_multi_idx(mltidx, n_rows):\n",
        "    candidate_tuples = mltidx.to_list()\n",
        "    pred_matches = {k: set() for k in range(n_rows)}\n",
        "    for pair in candidate_tuples:\n",
        "        reference_record = min(pair)\n",
        "        linked_record = max(pair)\n",
        "        pred_matches[reference_record].add(linked_record)\n",
        "    return pred_matches\n",
        "\n",
        "\n",
        "# Special class for comparing LoL concatenated elements\n",
        "# Use the full concatenation of all values to account for multiple entries like given names\n",
        "class CompareNestedString(BaseCompareFeature):\n",
        "    def _compute_vectorized(self, s1, s2):\n",
        "        return (s1.str[0] == s2.str[0]).astype(float)\n",
        "\n",
        "\n",
        "def get_va_labels(data):\n",
        "    start = time.time()\n",
        "\n",
        "    # Create a windowed neighborhood index on patient table because full is \n",
        "    # too expensive\n",
        "    indexer = rl.Index()\n",
        "    # Adding multiple different neighborhoods takes their union so we don't over-block\n",
        "    indexer.add(SortedNeighbourhood('last_name', window=WINDOW_INDEX_SIZE))\n",
        "    indexer.add(SortedNeighbourhood('birthdate', window=WINDOW_INDEX_SIZE))\n",
        "    indexer.add(SortedNeighbourhood('mrn', window=WINDOW_INDEX_SIZE))\n",
        "    indexer.add(FirstNameSortedNeighborhood('first_name', window=WINDOW_INDEX_SIZE))\n",
        "    candidate_links = indexer.index(data)\n",
        "\n",
        "    \n",
        "    # Note: using a multi-indexer treats the row number as the index, so\n",
        "    # results will automatically be in acceptable eval format\n",
        "\n",
        "    print(len(candidate_links), \"candidate pairs identified\")\n",
        "\n",
        "    # Apply feature comparisons on each supported field from the MPI\n",
        "    comp = rl.Compare()\n",
        "    comp.add(CompareNestedString(\"first_name\", \"first_name\",label=\"first_name\"))\n",
        "    comp.exact(\"last_name\", \"last_name\", label=\"last_name\")\n",
        "    comp.exact(\"birthdate\", \"birthdate\", label=\"birthdate\")\n",
        "    comp.add(CompareNestedString(\"address\", \"address\", label=\"address\"))\n",
        "    features = comp.compute(candidate_links, data)\n",
        "    matches = features[features.sum(axis=1) == 4]\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Computation took\", str(round(end - start, 2)), \"seconds\")\n",
        "\n",
        "    n_rows = DATA_SIZE if DATA_SIZE is not None else len(data)\n",
        "    matches = get_pred_match_dict_from_multi_idx(matches.index, n_rows)\n",
        "    return matches\n",
        "\n",
        "\n",
        "va_labels = get_va_labels(labeling_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GROUND-TRUTH LABELING: RECORD LINKAGE TOOLKIT FUNCTIONS\n",
        "\n",
        "# Special class for comparing LoL first name elements\n",
        "# Use the full concatenation of all names to account for multiple given names\n",
        "class CompareFirstName(BaseCompareFeature):\n",
        "    def _compute_vectorized(self, s1, s2):\n",
        "        jarowinklers = np.vectorize(compare_strings)(s1.str.join(\" \"), s2.str.join(\" \"))\n",
        "        return jarowinklers >= JARO_THRESHOLD\n",
        "\n",
        "\n",
        "# Special class for comparing LoL address line elements\n",
        "# Check each address line against each other address line to account for moving\n",
        "class CompareAddress(BaseCompareFeature):\n",
        "    def _compute_vectorized(self, s1, s2):\n",
        "\n",
        "        def comp_address_fields(a1_list, a2_list):\n",
        "            best_score = 0.0\n",
        "            for a1 in a1_list:\n",
        "                for a2 in a2_list:\n",
        "                    score = compare_strings(a1, a2)\n",
        "                    if score >= best_score:\n",
        "                        best_score = score\n",
        "            return best_score\n",
        "\n",
        "        jarowinklers = np.vectorize(comp_address_fields)(s1, s2)\n",
        "        return jarowinklers >= JARO_THRESHOLD\n",
        "    \n",
        "\n",
        "def predict_third_party_labels(data):\n",
        "    start = time.time()\n",
        "\n",
        "    # Create a windowed neighborhood index on patient table because full is \n",
        "    # too expensive with EM\n",
        "    indexer = rl.Index()\n",
        "    # Adding multiple different neighborhoods takes their union so we don't over-block\n",
        "    indexer.add(SortedNeighbourhood('last_name', window=WINDOW_INDEX_SIZE))\n",
        "    indexer.add(SortedNeighbourhood('birthdate', window=WINDOW_INDEX_SIZE))\n",
        "    indexer.add(SortedNeighbourhood('mrn', window=WINDOW_INDEX_SIZE))\n",
        "    indexer.add(FirstNameSortedNeighborhood('first_name', window=WINDOW_INDEX_SIZE))\n",
        "    candidate_links = indexer.index(data)\n",
        "    # Note: using a multi-indexer treats the row number as the index, so\n",
        "    # results will automatically be in acceptable eval format\n",
        "\n",
        "    print(len(candidate_links), \"candidate pairs identified\")\n",
        "\n",
        "    # Apply feature comparisons on each supported field from the MPI\n",
        "    comp = rl.Compare()\n",
        "    comp.add(CompareFirstName(\"first_name\", \"first_name\",label=\"first_name\"))\n",
        "    comp.string(\n",
        "        \"last_name\", \"last_name\", method=\"jarowinkler\", threshold=JARO_THRESHOLD, label=\"last_name\"\n",
        "    )\n",
        "    comp.string(\"mrn\", \"mrn\", method=\"jarowinkler\", threshold=JARO_THRESHOLD, label=\"mrn\")\n",
        "    comp.string(\n",
        "        \"birthdate\", \"birthdate\", method=\"jarowinkler\", threshold=JARO_THRESHOLD, label=\"birthdate\"\n",
        "    )\n",
        "    comp.add(CompareAddress(\"address\", \"address\", label=\"address\"))\n",
        "    comp.string(\"city\", \"city\", method=\"jarowinkler\", threshold=JARO_THRESHOLD, label=\"city\")\n",
        "    comp.string(\"state\", \"state\", method=\"jarowinkler\", threshold=JARO_THRESHOLD, label=\"state\")\n",
        "    comp.string(\"zip\", \"zip\", method=\"jarowinkler\", threshold=JARO_THRESHOLD, label=\"zip\")\n",
        "    comp.string(\"sex\", \"sex\", method=\"jarowinkler\", threshold=JARO_THRESHOLD, label=\"sex\")\n",
        "    features = comp.compute(candidate_links, data)\n",
        "\n",
        "    # Create an EM Predictor and label the binary training vectors\n",
        "    clf = rl.ECMClassifier()\n",
        "    pred_links = clf.fit_predict(features)\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Computation took\", str(round(end - start, 2)), \"seconds\")\n",
        "\n",
        "    n_rows = DATA_SIZE if DATA_SIZE is not None else len(data)\n",
        "    matches = get_pred_match_dict_from_multi_idx(pred_links, n_rows)\n",
        "    return matches\n",
        "\n",
        "\n",
        "third_party_labels = predict_third_party_labels(labeling_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LINKAGE DRIVER FUNCTIONS\n",
        "\n",
        "from phdi.linkage.link import (\n",
        "    _flatten_patient_resource,\n",
        "    _bind_func_names_to_invocations,\n",
        "    extract_blocking_values_from_record,\n",
        "    _compare_records,\n",
        "    extract_blocking_values_from_record\n",
        ")\n",
        "from typing import List\n",
        "import copy\n",
        "\n",
        "'''\n",
        "Need a modified version of the link_record_against_mpi function from the PHDI SDK,\n",
        "since we don't want to assign either a person_id or membership test each cluster for\n",
        "the best strength. Instead, we just want to find all other_records within a data block\n",
        "that are matches to a single incoming record.\n",
        "'''\n",
        "def link_fhir_record_from_dataset(\n",
        "    record: dict,\n",
        "    algo_config: List[dict],\n",
        "    db_client\n",
        ") -> List:\n",
        "\n",
        "    flattened_record = _flatten_patient_resource(record)\n",
        "\n",
        "    # Need to bind function names back to their symbolic invocations\n",
        "    # in context of the module--i.e. turn the string of a function\n",
        "    # name back into the callable defined in link.py\n",
        "    algo_config = copy.deepcopy(algo_config)\n",
        "    algo_config = _bind_func_names_to_invocations(algo_config)\n",
        "\n",
        "    # Accumulate all matches across all passes to return\n",
        "    found_matches = []\n",
        "    for linkage_pass in algo_config:\n",
        "        blocking_fields = linkage_pass[\"blocks\"]\n",
        "\n",
        "        # MPI will be able to find patients if *any* of their names or addresses\n",
        "        # contains extracted values, so minimally block on the first line\n",
        "        # if applicable\n",
        "        field_blocks = extract_blocking_values_from_record(record, blocking_fields)\n",
        "        if len(field_blocks) == 0:\n",
        "            continue\n",
        "        data_block = db_client.block_data(field_blocks)\n",
        "\n",
        "        # First row of returned block is column headers\n",
        "        # Map column name to idx, not including patient/person IDs\n",
        "        col_to_idx = {v: k for k, v in enumerate(data_block[0][2:])}\n",
        "        data_block = data_block[1:]\n",
        "\n",
        "        # Blocked fields are returned as LoLoL, but only name / address\n",
        "        # need to preserve multiple elements, so flatten other fields\n",
        "        for i in range(len(data_block)):\n",
        "            blocked_record = data_block[i]\n",
        "            for j in range(len(blocked_record)):\n",
        "                # patient_id, person_id not included in col->idx mapping\n",
        "                if j < 2:\n",
        "                    continue\n",
        "                if len(blocked_record[j]) > 0:\n",
        "                    blocked_record[j] = blocked_record[j][0]\n",
        "                else:\n",
        "                    blocked_record[j] = \"\"\n",
        "\n",
        "        # Correct any None types that might be lurking\n",
        "        if flattened_record[2] is None:\n",
        "            flattened_record[2] = [\"\"]\n",
        "        if flattened_record[5] is None:\n",
        "            flattened_record[5] = [\"\"]\n",
        "\n",
        "        # Check if incoming record matches each thing it blocked with\n",
        "        kwargs = linkage_pass.get(\"kwargs\", {})\n",
        "        for blocked_record in data_block:\n",
        "            is_match = _compare_records(\n",
        "                flattened_record,\n",
        "                blocked_record,\n",
        "                linkage_pass[\"funcs\"],\n",
        "                col_to_idx,\n",
        "                linkage_pass[\"matching_rule\"],\n",
        "                **kwargs\n",
        "            )\n",
        "            if is_match:\n",
        "                found_matches.append(blocked_record[0])\n",
        "    \n",
        "    return found_matches\n",
        "\n",
        "\n",
        "'''\n",
        "Turn the patient_ids of identified \"found matches\" into the threaded multi-row-indices\n",
        "that the ground truth labeler can understand. This way, all indices are expressed in\n",
        "the same scheme for statistical comparison.\n",
        "'''\n",
        "def map_patient_ids_to_idxs(pids: List, data: pd.DataFrame):\n",
        "    record_idxs = []\n",
        "    for pid in pids:\n",
        "        row_idx = data[data['patient_id'] == pid].index.values\n",
        "        if len(row_idx) > 0:\n",
        "            record_idxs.append(row_idx[0])\n",
        "    return record_idxs\n",
        "\n",
        "\n",
        "'''\n",
        "Find existing patient records in a dataset that map to each incoming record in a block \n",
        "of FHIR data. Since the FHIR data itself is pulled from the MPI, we can freely use it\n",
        "for querying for blocks without risk of finding unrecognized data.\n",
        "'''\n",
        "def link_all_fhir_records_block_dataset(records: List, algo_config: List[dict], db_client, label_set):\n",
        "    found_matches = {}\n",
        "    for record in records:\n",
        "        ridx = map_patient_ids_to_idxs([record.get(\"id\")], label_set)[0]\n",
        "        linked_records = link_fhir_record_from_dataset(record, algo_config, db_client)\n",
        "        linked_idxs = map_patient_ids_to_idxs(linked_records, label_set)\n",
        "        idx_set = set(linked_idxs)\n",
        "        if ridx in idx_set:\n",
        "            idx_set.remove(ridx)\n",
        "        found_matches[ridx] = idx_set\n",
        "    return found_matches\n",
        "\n",
        "\n",
        "'''\n",
        "Due to transforming patient_ids back into indices, multiple tuples get inserted for each\n",
        "match, i.e. we record the link (i,j) and the link (j,i), which would skew our stats.\n",
        "This function eliminates these redundancies and makes sure each link is counted once.\n",
        "'''\n",
        "def dedupe_match_double_counts(match_dict):\n",
        "    for k in match_dict:\n",
        "        if k > 0:\n",
        "            lower_set = set(list(range(k)))\n",
        "            match_dict[k] = match_dict[k].difference(lower_set)\n",
        "    return match_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ALGORITHM EVALUATION: LAC EXISTING\n",
        "\n",
        "LAC_ALGO = [\n",
        "    {\n",
        "        \"funcs\": {\n",
        "            \"first_name\": \"feature_match_fuzzy_string\",\n",
        "            \"last_name\": \"feature_match_fuzzy_string\",\n",
        "            \"address\": \"feature_match_fuzzy_string\",\n",
        "            \"mrn\": \"feature_match_fuzzy_string\",\n",
        "        },\n",
        "        \"blocks\": [\n",
        "            {\"value\": \"first_name\", \"transformation\": \"first4\"},\n",
        "            {\"value\": \"last_name\", \"transformation\": \"first4\"},\n",
        "            {\"value\": \"birthdate\"},\n",
        "        ],\n",
        "        \"matching_rule\": \"eval_perfect_match\",\n",
        "        \"cluster_ratio\": 0.9,\n",
        "    },\n",
        "    {\n",
        "        \"funcs\": {\n",
        "            \"first_name\": \"feature_match_fuzzy_string\",\n",
        "            \"last_name\": \"feature_match_fuzzy_string\",\n",
        "            \"address\": \"feature_match_fuzzy_string\",\n",
        "            \"mrn\": \"feature_match_fuzzy_string\",\n",
        "        },\n",
        "        \"blocks\": [\n",
        "            {\"value\": \"first_name\", \"transformation\": \"first4\"},\n",
        "            {\"value\": \"last_name\", \"transformation\": \"first4\"},\n",
        "            {\"value\": \"address\", \"transformation\": \"first4\"},\n",
        "        ],\n",
        "        \"matching_rule\": \"eval_perfect_match\",\n",
        "        \"cluster_ratio\": 0.9,\n",
        "    },\n",
        "    {\n",
        "        \"funcs\": {\n",
        "            \"first_name\": \"feature_match_fuzzy_string\",\n",
        "            \"last_name\": \"feature_match_fuzzy_string\",\n",
        "            \"address\": \"feature_match_fuzzy_string\",\n",
        "            \"mrn\": \"feature_match_fuzzy_string\",\n",
        "        },\n",
        "        \"blocks\": [\n",
        "            {\"value\": \"birthdate\"},\n",
        "        ],\n",
        "        \"matching_rule\": \"eval_perfect_match\",\n",
        "        \"cluster_ratio\": 0.9,\n",
        "    },\n",
        "]\n",
        "\n",
        "found_matches_lac = link_all_fhir_records_block_dataset(evaluation_set, LAC_ALGO, db_client, labeling_set)\n",
        "found_matches_lac = dedupe_match_double_counts(found_matches_lac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ALGORITHM EVALUATION: DIBBs BASIC\n",
        "from phdi.linkage import DIBBS_BASIC\n",
        "found_matches_dibbs_basic = link_all_fhir_records_block_dataset(evaluation_set, DIBBS_BASIC, db_client, labeling_set)\n",
        "found_matches_dibbs_basic = dedupe_match_double_counts(found_matches_dibbs_basic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ALGORITHM EVALUATION: DIBBs ENHANCED\n",
        "from phdi.linkage import DIBBS_ENHANCED\n",
        "found_matches_dibbs_enhanced = link_all_fhir_records_block_dataset(evaluation_set, DIBBS_ENHANCED, db_client, labeling_set)\n",
        "found_matches_dibbs_enhanced = dedupe_match_double_counts(found_matches_dibbs_enhanced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MOUNT THE FILE SYSTEM SO WE CAN WRITE THE OUTPUTS TO FILES\n",
        "\n",
        "# Set paths\n",
        "STORAGE_ACCOUNT = \"$STORAGE_ACCOUNT\"\n",
        "LINKAGE_OUTPUTS_FILESYSTEM = f\"abfss://linkage-notebook-outputs@{STORAGE_ACCOUNT}.dfs.core.windows.net/\"\n",
        "BLOB_STORAGE_LINKED_SERVICE = \"$BLOB_STORAGE_LINKED_SERVICE\"\n",
        "\n",
        "from notebookutils import mssparkutils\n",
        "\n",
        "# Set up for writing to blob storage\n",
        "linkage_bucket_name = \"linkage-notebook-outputs\"\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(BLOB_STORAGE_LINKED_SERVICE)\n",
        "wasb_path = 'wasbs://%s@%s.blob.core.windows.net/' % (linkage_bucket_name, STORAGE_ACCOUNT)\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (linkage_bucket_name, STORAGE_ACCOUNT), blob_sas_token)\n",
        "# Try mounting the remote storage directory at the mount point\n",
        "try:\n",
        "    mssparkutils.fs.mount(\n",
        "        wasb_path,\n",
        "        \"/\",\n",
        "        {\"LinkedService\": f\"${BLOB_STORAGE_LINKED_SERVICE}\"}\n",
        "    )\n",
        "except:\n",
        "    print(\"Already mounted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RECOMPUTE AND EXPORT LOG-ODDS\n",
        "\n",
        "from phdi.linkage import calculate_m_probs, calculate_u_probs, calculate_log_odds\n",
        "import json\n",
        "\n",
        "m_probs = calculate_m_probs(labeling_set, third_party_labels)\n",
        "u_probs = calculate_u_probs(labeling_set, third_party_labels, n_samples=25000)\n",
        "log_odds = calculate_log_odds(m_probs, u_probs)\n",
        "log_odds.pop(\"patient_id\")\n",
        "mssparkutils.fs.put(LINKAGE_OUTPUTS_FILESYSTEM + \"updated_log_odds.json\", json.dumps(log_odds), True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RUN THE NUMBERS AND GET THE STATS FUNCTIONS\n",
        "\n",
        "'''\n",
        "To ensure accurate statistics, the matches and the true matches dictionaries\n",
        "in the statistical evaluation function should have the following form:\n",
        "\n",
        "{\n",
        "    row_num_of_record_in_data: set(row_nums_of_linked_records)\n",
        "}\n",
        "\n",
        "Each row in the data should be represented as a key in both dictionaries.\n",
        "The value for each of these keys should be a set that contains all other\n",
        "row numbers for records in the data set that link to the key record.\n",
        "'''\n",
        "\n",
        "def score_linkage_vs_truth(found_matches, true_matches, records_in_dataset):\n",
        "\n",
        "    # Need division by 2 because ordering is irrelevant, matches are symmetric\n",
        "    total_possible_matches = (records_in_dataset * (records_in_dataset - 1)) / 2.0\n",
        "    true_positives = 0.0\n",
        "    false_positives = 0.0\n",
        "    false_negatives = 0.0\n",
        "\n",
        "    for root_record in true_matches:\n",
        "        if root_record in found_matches:\n",
        "            true_positives += len(\n",
        "                true_matches[root_record].intersection(found_matches[root_record])\n",
        "            )\n",
        "            false_positives += len(\n",
        "                found_matches[root_record].difference(true_matches[root_record])\n",
        "            )\n",
        "            false_negatives += len(\n",
        "                true_matches[root_record].difference(found_matches[root_record])\n",
        "            )\n",
        "        else:\n",
        "            false_negatives += len(true_matches[root_record])\n",
        "    for record in set(set(found_matches.keys()).difference(true_matches.keys())):\n",
        "        false_positives += len(found_matches[record])\n",
        "\n",
        "    true_negatives = (\n",
        "        total_possible_matches - true_positives - false_positives - false_negatives\n",
        "    )\n",
        "    npv = round((true_negatives / (true_negatives + false_negatives)), 3)\n",
        "    sensitivity = round(true_positives / (true_positives + false_negatives), 3)\n",
        "    specificity = round(true_negatives / (true_negatives + false_positives), 3)\n",
        "    ppv = round(true_positives / (true_positives + false_positives), 3)\n",
        "    f1 = round(\n",
        "        (2 * true_positives) / (2 * true_positives + false_negatives + false_positives),\n",
        "        3,\n",
        "    )\n",
        "    return {\n",
        "        \"tp\": true_positives,\n",
        "        \"fp\": false_positives,\n",
        "        \"fn\": false_negatives,\n",
        "        \"sens\": sensitivity,\n",
        "        \"spec\": specificity,\n",
        "        \"ppv\": ppv,\n",
        "        \"npv\": npv,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "display_str = \"\"\n",
        "\n",
        "if DATA_SIZE is not None:\n",
        "    n_records = DATA_SIZE\n",
        "else:\n",
        "    n_records = len(evaluation_set)\n",
        "\n",
        "for lbl_type in [\"va\", \"emc\"]:\n",
        "    if lbl_type == \"va\":\n",
        "        labels = va_labels\n",
        "    else:\n",
        "        labels = third_party_labels\n",
        "    \n",
        "    stats_dict_lac = score_linkage_vs_truth(found_matches_lac, labels, n_records)\n",
        "    stats_dict_dibbs_b = score_linkage_vs_truth(found_matches_dibbs_basic, labels, n_records)\n",
        "    stats_dict_dibbs_e = score_linkage_vs_truth(found_matches_dibbs_enhanced, labels, n_records)\n",
        "\n",
        "    display_str += \"DISPLAYING EVALUATION ON \" + lbl_type.upper() + \" LABELS:\\n\"\n",
        "    display_str += \"\\n\"\n",
        "\n",
        "    for algo in [\"lac\", \"basic\", \"enhanced\"]:\n",
        "        if algo == \"lac\":\n",
        "            display_str += \"LAC Existing Algorithm:\\n\"\n",
        "            scores = stats_dict_lac\n",
        "        elif algo == \"basic\":\n",
        "            display_str += \"DIBBs Basic Algorithm:\\n\"\n",
        "            scores = stats_dict_dibbs_b\n",
        "        else:\n",
        "            display_str += \"DIBBs Log-Odds Algorithm:\\n\"\n",
        "            scores = stats_dict_dibbs_e\n",
        "\n",
        "        display_str += \"True Positives: \" + str(scores[\"tp\"]) + \"\\n\"\n",
        "        display_str += \"False Positives: \" + str(scores[\"fp\"]) + \"\\n\"\n",
        "        display_str += \"False Negatives: \" + str(scores[\"fn\"]) + \"\\n\"\n",
        "        display_str += \"Sensitivity: \" + str(scores[\"sens\"]) + \"\\n\"\n",
        "        display_str += \"Specificity: \" + str(scores[\"spec\"]) + \"\\n\"\n",
        "        display_str += \"PPV: \" + str(scores[\"ppv\"]) + \"\\n\"\n",
        "        display_str += \"NPV: \" + str(scores[\"npv\"]) + \"\\n\"\n",
        "        display_str += \"F1: \" + str(scores[\"f1\"]) + \"\\n\"\n",
        "        display_str += \"\\n\"\n",
        "    \n",
        "    display_str += \"\\n\"\n",
        "\n",
        "print(display_str)\n",
        "\n",
        "mssparkutils.fs.put(LINKAGE_OUTPUTS_FILESYSTEM + \"results.txt\", display_str, True)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
