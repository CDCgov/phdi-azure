{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### tabularizePatientTable\n",
        "This notebook extracts all data from the Patient table (`patient`) in the Master Patient Index and tabularizes the data into Lists of Lists of Lists (LoLoL)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "pip install azure-identity phdi recordlinkage azure-keyvault-secrets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# IMPORTS AND CONSTANTS\n",
        "\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# Ground-truth labeling imports\n",
        "import time\n",
        "import pandas as pd\n",
        "import recordlinkage as rl\n",
        "from recordlinkage.base import BaseCompareFeature\n",
        "import numpy as np\n",
        "from phdi.harmonization import compare_strings\n",
        "\n",
        "# Set your Key Vault information\n",
        "vault_name = \"$KEY_VAULT\"\n",
        "KEY_VAULT_URL = f\"https://{vault_name}.vault.azure.net\"\n",
        "vault_linked_service = \"$KEY_VAULT_LINKED_SERVICE\"\n",
        "\n",
        "# Set up db_client\n",
        "DB_NAME = \"DibbsMpiDB\"\n",
        "DB_USER = \"postgres\"\n",
        "DB_HOST = \"$MPI_DB_HOST\"\n",
        "DB_PORT = \"5432\"\n",
        "DB_TABLE_PATIENT = \"patient\"\n",
        "DB_TABLE_PERSON= \"person\"\n",
        "\n",
        "# Adjust data volume for scaling\n",
        "# Make sure evaluation size is less than labeling size!\n",
        "from random import sample\n",
        "LABELING_SIZE = 10000\n",
        "EVALUATION_SIZE = 1000\n",
        "\n",
        "# Ground-truth labeling parameters\n",
        "WINDOW_INDEX_SIZE = 5\n",
        "JARO_THRESHOLD = 0.85"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPEN PARALLEL CONNECTION TO MPI, READ DB INTO COMPRESSED MEMORY\n",
        "\n",
        "from pyspark.sql import SparkSession, Row\n",
        "import json\n",
        "\n",
        "# Access the MPI Database\n",
        "credential = DefaultAzureCredential()\n",
        "db_password =  TokenLibrary.getSecret(vault_name,\"mpi-db-password\",vault_linked_service)\n",
        "\n",
        "url = f\"jdbc:postgresql://{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
        "db_props = {\n",
        "    \"user\": DB_USER,\n",
        "    \"password\": db_password,\n",
        "    \"driver\": \"org.postgresql.Driver\"\n",
        "}\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder.master(\"local[*]\")\n",
        "    .appName(\"Build sub-sampled MPI\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "# Parallel pyspark read the MPI and create a SQL window\n",
        "view_name = \"patient_view\"\n",
        "full_mpi_data = spark.read.jdbc(url, DB_TABLE_PATIENT, properties=db_props)\n",
        "full_mpi_data.createOrReplaceTempView(view_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MPI ACCESS AND TABULATION FUNCTIONS\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "FIELDS_TO_JSONPATHS = {\n",
        "    \"address\": \"\"\"$.address[*].line\"\"\",\n",
        "    \"birthdate\": \"$.birthDate\",\n",
        "    \"city\": \"\"\"$.address[*].city\"\"\",\n",
        "    \"first_name\": \"\"\"$.name[*].given\"\"\",\n",
        "    \"last_name\": \"\"\"$.name[*].family\"\"\",\n",
        "    \"codes\": \"\"\"$.identifier[*].type.coding[0].code\"\"\",\n",
        "    \"vals\": \"\"\"$.identifier[*].value\"\"\",\n",
        "    \"sex\": \"$.gender\",\n",
        "    \"state\": \"\"\"$.address[*].state\"\"\",\n",
        "    \"zip\": \"\"\"$.address[*].postalCode\"\"\",\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "Generate a query to extract all data from a parallelized extract of the MPI. This function\n",
        "uses a modified set of FHIRpath keys to efficiently extract the de-serialized JSON objects\n",
        "holding patient data from the MPI, so that we can filter and process them in parallel to\n",
        "create a DataFrame representation of the MPI without needing any future network calls to\n",
        "the DB instance.\n",
        "\"\"\"\n",
        "def generate_query(view):\n",
        "    select_query_stubs = []\n",
        "    query_data = []\n",
        "    for key in FIELDS_TO_JSONPATHS:\n",
        "        query = f\"get_json_object(patient_resource,'{FIELDS_TO_JSONPATHS[key]}') as {key}\"\n",
        "        select_query_stubs.append(query)\n",
        "    select_query = \"SELECT patient_id, person_id, \" + \", \".join(stub for stub in select_query_stubs)\n",
        "    query = select_query + f\" FROM {view};\"\n",
        "    return query\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Parallelized mapping function that computes a patient's MRN from two other extracted values\n",
        "from the MPI database. Pyspark doesn't support JSONB path querying, so we can't directly find\n",
        "the struct containing MRN within the JSON array of a patient's identifiers. However, the\n",
        "field's 'codes' array and 'vals' array *can* be efficiently extracted in parallel. Since\n",
        "this operation preserves order, we can infer the MRN based on a zip of these lists.\n",
        "\"\"\"\n",
        "def extract_mrn(row):\n",
        "    mrn = None\n",
        "    codes = json.loads(row['codes'])\n",
        "    vals = json.loads(row['vals'])\n",
        "    mr_idx = [pos for (pos, v) in enumerate(codes) if v == \"MR\"]\n",
        "    if len(mr_idx) > 0:\n",
        "        mrn = vals[mr_idx[0]]\n",
        "    return Row(row['patient_id'], row['person_id'], row['address'], row['birthdate'], row['city'], row['first_name'], row['last_name'], mrn, row['sex'], row['state'], row['zip'])\n",
        "\n",
        "# Create the pyspark-supported query and grab non-formatted patient data\n",
        "query = generate_query(view_name)\n",
        "extracted_mpi_data = spark.sql(query)\n",
        "\n",
        "# Checkpoint the DF here so we don't have re-query the pyspark view every time\n",
        "# lineage evaluates\n",
        "extracted_mpi_data.cache()\n",
        "\n",
        "# Derive each patient's MRN and format the columns we want to map the data into\n",
        "formatted_mpi_data = extracted_mpi_data.rdd.map(extract_mrn)\n",
        "formatted_cols = [x for x in FIELDS_TO_JSONPATHS.keys() if x != \"codes\" and x != \"vals\"]\n",
        "formatted_cols.append(\"mrn\")\n",
        "formatted_cols = sorted(formatted_cols)\n",
        "formatted_cols.insert(0, 'person_id')\n",
        "formatted_cols.insert(0, 'patient_id')\n",
        "\n",
        "# Pyspark will read the JSON data as de-serialized strings, which means it will\n",
        "# actually save the quotes into the string-proper so that it can be re-serialized if\n",
        "# necessary; however, we don't ever need to re-serialize the MPI (that would be super\n",
        "# slow) so strip the quotes to make comparisons easier\n",
        "mpi_df = formatted_mpi_data.toDF(formatted_cols)\n",
        "for col in formatted_cols:\n",
        "    if col != \"first_name\" and col != \"address\":\n",
        "        mpi_df = mpi_df.withColumn(col, F.regexp_replace(col, \"\\\"\", \"\"))\n",
        "\n",
        "# Cache the updated version so we can filter on it later\n",
        "mpi_df.cache()\n",
        "\n",
        "# Make a random sampling generator for use on both the labeling and the eval set,\n",
        "# since the eval set *must* be included in the label set\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "# Apply sampling, if desired, and convert into pandas DF for labeling\n",
        "labeling_set = [list(x) for x in mpi_df.collect()]\n",
        "sampled_idx = None\n",
        "if LABELING_SIZE < len(labeling_set):\n",
        "    sampled_idx = rng.choice(len(labeling_set), LABELING_SIZE, replace=False)\n",
        "    sampled_idx = set(sampled_idx)\n",
        "    labeling_set = [v for (i, v) in enumerate(labeling_set) if i in sampled_idx]\n",
        "labeling_set = pd.DataFrame(labeling_set, columns=formatted_cols)\n",
        "\n",
        "# Now, we need a copy of the data in a FHIR format for the linkage algorithms\n",
        "fhir_query = \"select patient_resource from \" + view_name + \";\"\n",
        "fhir_pull = spark.sql(fhir_query)\n",
        "evaluation_set = [json.loads(x['patient_resource']) for x in fhir_pull.collect()]\n",
        "\n",
        "# As above, apply sampling, but use same generator to ensure that eval set\n",
        "# is a subset of the labeling set\n",
        "if EVALUATION_SIZE < len(evaluation_set):\n",
        "    #Re-use labeling sampler, if one was created\n",
        "    if sampled_idx is not None:\n",
        "        eval_sample = rng.choice(list(sampled_idx), EVALUATION_SIZE, replace=False, shuffle=False)\n",
        "    # We kept the whole MPI, so any record can be evaluated\n",
        "    else:\n",
        "        eval_sample = rng.choice(len(evaluation_set), EVALUATION_SIZE, replace=False)\n",
        "    eval_sample = set(eval_sample)\n",
        "    evaluation_set = [v for (i, v) in enumerate(evaluation_set) if i in eval_sample]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GROUND TRUTH LABELING: VIRGINIA FUNCTIONS\n",
        "\n",
        "from recordlinkage.index import SortedNeighbourhood, BaseIndexAlgorithm\n",
        "from recordlinkage.utils import listify\n",
        "\n",
        "'''\n",
        "A custom Indexing function built to operate compatibly on the first_name column\n",
        "returned from the MPI. Since that's a list of strings (because someone could have\n",
        "multiple given names), we need a way to cross-conjoin these entries and apply\n",
        "the same fuzzy blocking filter window that a regular column of strings would get.\n",
        "This performs joint name concatenation on copies of that column in the data, and\n",
        "then uses an edit distance neighborhood to find fuzzy blocking candidates.\n",
        "'''\n",
        "class FirstNameSortedNeighborhood(BaseIndexAlgorithm):\n",
        "    def __init__(\n",
        "        self,\n",
        "        left_on=None,\n",
        "        right_on=None,\n",
        "        window=3,\n",
        "        sorting_key_values=None,\n",
        "        block_on=[],\n",
        "        block_left_on=[],\n",
        "        block_right_on=[],\n",
        "        **kwargs\n",
        "    ):\n",
        "        super(FirstNameSortedNeighborhood, self).__init__(**kwargs)\n",
        "\n",
        "        # variables to block on\n",
        "        self.left_on = left_on\n",
        "        self.right_on = right_on\n",
        "        self.window = window\n",
        "        self.sorting_key_values = sorting_key_values\n",
        "        self.block_on = block_on\n",
        "        self.block_left_on = block_left_on\n",
        "        self.block_right_on = block_right_on\n",
        "\n",
        "    def _get_left_and_right_on(self):\n",
        "        \"\"\"\n",
        "        We only care about the de-dupe case which involves no self.right, but this\n",
        "        still needs to be implemented for super compatibility.\n",
        "        \"\"\"\n",
        "        if self.right_on is None:\n",
        "            return (self.left_on, self.left_on)\n",
        "        else:\n",
        "            return (self.left_on, self.right_on)\n",
        "\n",
        "    def _get_sorting_key_values(self, array1, array2):\n",
        "        \"\"\"\n",
        "        Return the sorting key values as a series. This function is required by the\"\n",
        "        package for multi-index neighborhood filtering according to some papers it's\"\n",
        "        built on.\n",
        "        \"\"\"\n",
        "\n",
        "        concat_arrays = np.concatenate([array1, array2])\n",
        "        return np.unique(concat_arrays)\n",
        "\n",
        "    def _link_index(self, df_a, df_b):\n",
        "        df_a = df_a.copy()\n",
        "        df_b = df_b.copy()\n",
        "        df_a[\"first_name\"] = df_a[\"first_name\"].str.join(\" \")\n",
        "        df_b[\"first_name\"] = df_a[\"first_name\"].str.join(\" \")\n",
        "        left_on, right_on = self._get_left_and_right_on()\n",
        "        left_on = listify(left_on)\n",
        "        right_on = listify(right_on)\n",
        "    \n",
        "        window = self.window\n",
        "\n",
        "        # Correctly generate blocking keys\n",
        "        block_left_on = listify(self.block_left_on)\n",
        "        block_right_on = listify(self.block_right_on)\n",
        "\n",
        "        if self.block_on:\n",
        "            block_left_on = listify(self.block_on)\n",
        "            block_right_on = listify(self.block_on)\n",
        "\n",
        "        blocking_keys = [\"sorting_key\"] + [\n",
        "            \"blocking_key_%d\" % i for i, v in enumerate(block_left_on)\n",
        "        ]\n",
        "\n",
        "        # Format the data to thread with index pairs\n",
        "        data_left = pd.DataFrame(df_a[listify(left_on) + block_left_on], copy=False)\n",
        "        data_left.columns = blocking_keys\n",
        "        data_left[\"index_x\"] = np.arange(len(df_a))\n",
        "        data_left.dropna(axis=0, how=\"any\", subset=blocking_keys, inplace=True)\n",
        "\n",
        "        data_right = pd.DataFrame(df_b[listify(right_on) + block_right_on], copy=False)\n",
        "        data_right.columns = blocking_keys\n",
        "        data_right[\"index_y\"] = np.arange(len(df_b))\n",
        "        data_right.dropna(axis=0, how=\"any\", subset=blocking_keys, inplace=True)\n",
        "\n",
        "        # sorting_key_values is the terminology in Data Matching [Christen,\n",
        "        # 2012]\n",
        "        if self.sorting_key_values is None:\n",
        "            self.sorting_key_values = self._get_sorting_key_values(\n",
        "                data_left[\"sorting_key\"].values, data_right[\"sorting_key\"].values\n",
        "            )\n",
        "\n",
        "        sorting_key_factors = pd.Series(\n",
        "            np.arange(len(self.sorting_key_values)), index=self.sorting_key_values\n",
        "        )\n",
        "\n",
        "        data_left[\"sorting_key\"] = data_left[\"sorting_key\"].map(sorting_key_factors)\n",
        "        data_right[\"sorting_key\"] = data_right[\"sorting_key\"].map(sorting_key_factors)\n",
        "\n",
        "        # Internal window size\n",
        "        _window = int((window - 1) / 2)\n",
        "\n",
        "        def merge_lagged(x, y, w):\n",
        "            \"\"\"Merge two dataframes with a lag on in the sorting key.\"\"\"\n",
        "\n",
        "            y = y.copy()\n",
        "            y[\"sorting_key\"] = y[\"sorting_key\"] + w\n",
        "\n",
        "            return x.merge(y, how=\"inner\")\n",
        "\n",
        "        pairs_concat = [\n",
        "            merge_lagged(data_left, data_right, w) for w in range(-_window, _window + 1)\n",
        "        ]\n",
        "\n",
        "        pairs_df = pd.concat(pairs_concat, axis=0)\n",
        "\n",
        "        return pd.MultiIndex(\n",
        "            levels=[df_a.index.values, df_b.index.values],\n",
        "            codes=[pairs_df[\"index_x\"].values, pairs_df[\"index_y\"].values],\n",
        "            verify_integrity=False,\n",
        "        )\n",
        "\n",
        "\n",
        "# Transform a recordlinkage toolkit multi-index into a set of candidate tuples\n",
        "def get_pred_match_dict_from_multi_idx(mltidx, n_rows):\n",
        "    candidate_tuples = mltidx.to_list()\n",
        "    pred_matches = {k: set() for k in range(n_rows)}\n",
        "    for pair in candidate_tuples:\n",
        "        reference_record = min(pair)\n",
        "        linked_record = max(pair)\n",
        "        pred_matches[reference_record].add(linked_record)\n",
        "    return pred_matches\n",
        "\n",
        "\n",
        "# Special class for comparing LoL concatenated elements\n",
        "# Use the full concatenation of all values to account for multiple entries like given names\n",
        "class CompareNestedString(BaseCompareFeature):\n",
        "    def _compute_vectorized(self, s1, s2):\n",
        "        return (s1.str[0] == s2.str[0]).astype(float)\n",
        "\n",
        "\n",
        "def get_va_labels(data):\n",
        "    start = time.time()\n",
        "\n",
        "    # Create a windowed neighborhood index on patient table because full is \n",
        "    # too expensive\n",
        "    indexer = rl.Index()\n",
        "    # Adding multiple different neighborhoods takes their union so we don't over-block\n",
        "    indexer.add(SortedNeighbourhood('last_name', window=WINDOW_INDEX_SIZE))\n",
        "    indexer.add(SortedNeighbourhood('birthdate', window=WINDOW_INDEX_SIZE))\n",
        "    indexer.add(SortedNeighbourhood('mrn', window=WINDOW_INDEX_SIZE))\n",
        "    indexer.add(FirstNameSortedNeighborhood('first_name', window=WINDOW_INDEX_SIZE))\n",
        "    candidate_links = indexer.index(data)\n",
        "\n",
        "    \n",
        "    # Note: using a multi-indexer treats the row number as the index, so\n",
        "    # results will automatically be in acceptable eval format\n",
        "\n",
        "    print(len(candidate_links), \"candidate pairs identified\")\n",
        "\n",
        "    # Apply feature comparisons on each supported field from the MPI\n",
        "    comp = rl.Compare()\n",
        "    comp.add(CompareNestedString(\"first_name\", \"first_name\",label=\"first_name\"))\n",
        "    comp.exact(\"last_name\", \"last_name\", label=\"last_name\")\n",
        "    comp.exact(\"birthdate\", \"birthdate\", label=\"birthdate\")\n",
        "    comp.add(CompareNestedString(\"address\", \"address\", label=\"address\"))\n",
        "    features = comp.compute(candidate_links, data)\n",
        "    matches = features[features.sum(axis=1) == 4]\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Computation took\", str(round(end - start, 2)), \"seconds\")\n",
        "\n",
        "    matches = get_pred_match_dict_from_multi_idx(matches.index, len(data))\n",
        "    return matches, candidate_links\n",
        "\n",
        "\n",
        "va_labels, candidate_links = get_va_labels(labeling_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GROUND-TRUTH LABELING: RECORD LINKAGE TOOLKIT FUNCTIONS\n",
        "\n",
        "# Special class for comparing LoL first name elements\n",
        "# Use the full concatenation of all names to account for multiple given names\n",
        "class CompareFirstName(BaseCompareFeature):\n",
        "    def _compute_vectorized(self, s1, s2):\n",
        "        jarowinklers = np.vectorize(compare_strings)(s1.str.join(\" \"), s2.str.join(\" \"))\n",
        "        return jarowinklers >= JARO_THRESHOLD\n",
        "\n",
        "\n",
        "# Special class for comparing LoL address line elements\n",
        "# Check each address line against each other address line to account for moving\n",
        "class CompareAddress(BaseCompareFeature):\n",
        "    def _compute_vectorized(self, s1, s2):\n",
        "\n",
        "        def comp_address_fields(a1_list, a2_list):\n",
        "            best_score = 0.0\n",
        "            for a1 in a1_list:\n",
        "                for a2 in a2_list:\n",
        "                    score = compare_strings(a1, a2)\n",
        "                    if score >= best_score:\n",
        "                        best_score = score\n",
        "            return best_score\n",
        "\n",
        "        jarowinklers = np.vectorize(comp_address_fields)(s1, s2)\n",
        "        return jarowinklers >= JARO_THRESHOLD\n",
        "    \n",
        "\n",
        "def predict_third_party_labels(data, candidate_links):\n",
        "    start = time.time()\n",
        "\n",
        "    # Apply feature comparisons on each supported field from the MPI\n",
        "    comp = rl.Compare()\n",
        "    comp.add(CompareFirstName(\"first_name\", \"first_name\",label=\"first_name\"))\n",
        "    comp.string(\n",
        "        \"last_name\", \"last_name\", method=\"jarowinkler\", threshold=JARO_THRESHOLD, label=\"last_name\"\n",
        "    )\n",
        "    comp.string(\"mrn\", \"mrn\", method=\"jarowinkler\", threshold=JARO_THRESHOLD, label=\"mrn\")\n",
        "    comp.string(\n",
        "        \"birthdate\", \"birthdate\", method=\"jarowinkler\", threshold=JARO_THRESHOLD, label=\"birthdate\"\n",
        "    )\n",
        "    comp.add(CompareAddress(\"address\", \"address\", label=\"address\"))\n",
        "    comp.string(\"city\", \"city\", method=\"jarowinkler\", threshold=JARO_THRESHOLD, label=\"city\")\n",
        "    comp.string(\"zip\", \"zip\", method=\"jarowinkler\", threshold=JARO_THRESHOLD, label=\"zip\")\n",
        "    features = comp.compute(candidate_links, data)\n",
        "\n",
        "    # Create an EM Predictor and label the binary training vectors\n",
        "    clf = rl.ECMClassifier()\n",
        "    pred_links = clf.fit_predict(features)\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Computation took\", str(round(end - start, 2)), \"seconds\")\n",
        "\n",
        "    matches = get_pred_match_dict_from_multi_idx(pred_links, len(data))\n",
        "    return matches\n",
        "\n",
        "\n",
        "third_party_labels = predict_third_party_labels(labeling_set, candidate_links)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LINKAGE DRIVER FUNCTIONS\n",
        "\n",
        "from phdi.linkage.link import _flatten_patient_resource, extract_blocking_values_from_record\n",
        "from typing import List\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Function to use a previously-computed parallel dataframe to quickly block possible candidates\n",
        "for an incoming record. Since the supplied `mpi_df` is a native spark DataFrame, we can use\n",
        "hyper-efficient `where` filters to block on the appropriate string columns of MPI candidates\n",
        "rather than use a database client to retrieve information over the network. We also cache\n",
        "the filtered dataframe so that when we later use an RDD to operate on it, the filters are\n",
        "preserved in memory without needing to re-apply the lineage.\n",
        "\"\"\"\n",
        "def spark_block(block_vals: dict, mpi_df):\n",
        "\n",
        "    # We'll sequentially apply each blocking filter, since that's equivalent to finding\n",
        "    # their intersection all at once\n",
        "    result = mpi_df\n",
        "    result.cache()\n",
        "    for blocking_criterion in block_vals:\n",
        "        props = block_vals[blocking_criterion]\n",
        "\n",
        "        # Special case if we're blocking on first_name or address: pyspark can serialize these\n",
        "        # as JSON strings, but that means they actually get stored as strings, so we need to \n",
        "        # account for the brackets '[' and ']'\n",
        "        if blocking_criterion == \"first_name\" or blocking_criterion == \"address\":\n",
        "            if \"transformation\" in props:\n",
        "                if props[\"transformation\"] == \"first4\":\n",
        "                    result = result.where(result[blocking_criterion].startswith(\"[\\\"\" + props[\"value\"]))\n",
        "                elif props[\"transformation\"] == \"last4\":\n",
        "                    result = result.where(result[blocking_criterion].endswith(props[\"value\"] + \"\\\"]\"))\n",
        "            else:\n",
        "                result = result.where(result[blocking_criterion] == \"[\\\"\" + props[\"value\"] + \"\\\"]\")\n",
        "\n",
        "        # Regular case is just a straight string comparison since we've already stripped the \n",
        "        # de-serialization quotes\n",
        "        else:\n",
        "            if \"transformation\" in props:\n",
        "                if props[\"transformation\"] == \"first4\":\n",
        "                    result = result.where(result[blocking_criterion].startswith(props[\"value\"]))\n",
        "                elif props[\"transformation\"] == \"last4\":\n",
        "                    result = result.where(result[blocking_criterion].endswith(props[\"value\"]))\n",
        "            else:\n",
        "                result = result.where(result[blocking_criterion] == props[\"value\"])\n",
        "    return result\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Quick helper to extract the threshold and metric used in fuzzy string comparisons.\n",
        "We have this to not clutter the main analytic function.\n",
        "\"\"\"\n",
        "def _get_fuzzy_comp_params(**kwargs):\n",
        "    similarity_measure = \"JaroWinkler\"\n",
        "    if \"similarity_measure\" in kwargs:\n",
        "        similarity_measure = kwargs[\"similarity_measure\"]\n",
        "    threshold = 0.7\n",
        "    if \"threshold\" in kwargs:\n",
        "        threshold = kwargs[\"threshold\"]\n",
        "    return similarity_measure, threshold\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Helper to apply the result of a feature-wise comparison between an incoming record and a \n",
        "candidate from the MPI to the accumulated 'match score' of the two. In a 'normal' case\n",
        "where we're not using log-odds, this is just a count of the number of feature comparisons\n",
        "that satisfy the fuzzy string threshold. In the log-odds case, this is an accumulation of\n",
        "the weighted probability score that the two records are a match.\n",
        "\"\"\"\n",
        "def _apply_score_contribution(feature_score, col, fuzzy_threshold, match_score, match_rule, **kwargs):\n",
        "    if \"log\" in match_rule:\n",
        "        col_odds = kwargs[\"log_odds\"][col]\n",
        "        match_score += (feature_score * col_odds)\n",
        "    else:\n",
        "        if feature_score >= fuzzy_threshold:\n",
        "            match_score += 1.0\n",
        "    return match_score\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Main parallel driver function that gets map-applied to each row of an RDD constructed\n",
        "from the candidate block filtered from the MPI. Each such candidate row (record) is\n",
        "iteratively processed by each feature comparison function, and the results are \n",
        "accumulated into a total score that is used by the matching rule to decide if the two\n",
        "records should be linked.\n",
        "\n",
        "This function will be distributed to an executor on a worker node once Spark actually\n",
        "invokes the parallel evaluation, so even though we're sequentially processing the comparison\n",
        "functions for each feature, those are (comparatively) hyper-efficient because they\n",
        "just aggregate equality and greather than comparisons. This allowas each worker node in\n",
        "the cluster to process multiple rows of the candidate block simultaneously.\n",
        "\"\"\"\n",
        "def spark_compare_map_helper(row, flattened_record, funcs, col_to_idx, matching_rule, **kwargs):\n",
        "\n",
        "    # Iteratively accumulate results of each feature-wise comparison\n",
        "    match_score = 0.0\n",
        "    for col in funcs:\n",
        "        func = funcs[col]\n",
        "        feature_idx_in_record = col_to_idx[col]\n",
        "        feature_in_record = flattened_record[feature_idx_in_record]\n",
        "\n",
        "        if \"fuzzy\" in func:\n",
        "            similarity_measure, fuzzy_threshold = _get_fuzzy_comp_params(**kwargs)\n",
        "\n",
        "            # Given name is a list (possibly including middle name), so our logic says\n",
        "            # concatenate all the values together and then fuzzy compare\n",
        "            if col == \"first_name\":\n",
        "                feature_in_record = \" \".join(feature_in_record)\n",
        "                feature_in_mpi = \" \".join(json.loads(row[col]))\n",
        "                feature_score = compare_strings(feature_in_mpi, feature_in_record, similarity_measure)\n",
        "                match_score = _apply_score_contribution(\n",
        "                    feature_score, col, fuzzy_threshold, match_score, matching_rule, **kwargs\n",
        "                )\n",
        "\n",
        "            # Address is also a list, but rather than concatenate them all, we check if each\n",
        "            # line of an incoming address matches any line of an MPI address; this accounts for\n",
        "            # a patient's change of residence history\n",
        "            elif col == \"address\":\n",
        "                feature_in_mpi = json.loads(row[col])\n",
        "                best_score = 0.0\n",
        "                for r in feature_in_record:\n",
        "                    for m in feature_in_mpi:\n",
        "                        feature_comp = compare_strings(r, m, similarity_measure)\n",
        "                        if feature_comp > best_score:\n",
        "                            best_score = feature_comp\n",
        "                match_score = _apply_score_contribution(\n",
        "                    best_score, col, fuzzy_threshold, match_score, matching_rule, **kwargs\n",
        "                )\n",
        "            \n",
        "            # Regular case: straight string comparison on the fields\n",
        "            else:\n",
        "                feature_in_mpi = row[col]\n",
        "                feature_score = compare_strings(feature_in_mpi, feature_in_record, similarity_measure)\n",
        "                match_score = _apply_score_contribution(\n",
        "                    feature_score, col, fuzzy_threshold, match_score, matching_rule, **kwargs\n",
        "                )\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    return (row['patient_id'], match_score)\n",
        "    \n",
        "\n",
        "\"\"\"\n",
        "Orchestrator function that provisions the RDD-mapping of the parallel candidate evaluation.\n",
        "Once we've parallel-processed the candidates, we apply RDD filtering to identify only those\n",
        "candidates who satisfy the provided matching rule. (be that \"all feature-wise comparisons are\n",
        "true\" or \"total probability score exceeds log-odds cutoff\").\n",
        "\"\"\"\n",
        "def spark_compare(data_block, flattened_record, funcs, col_to_idx, matching_rule, **kwargs):\n",
        "    res = data_block.rdd.map(lambda row: spark_compare_map_helper(\n",
        "        row, flattened_record, funcs, col_to_idx, matching_rule, **kwargs\n",
        "    ))\n",
        "    if \"log\" in matching_rule:\n",
        "        match_cutoff = kwargs[\"true_match_threshold\"]\n",
        "    else:\n",
        "        match_cutoff = len(funcs)\n",
        "    match_list = []\n",
        "    match_list = res.filter(lambda row: row[1] >= match_cutoff).map(lambda x: x[0])\n",
        "    return match_list\n",
        "\n",
        "\n",
        "'''\n",
        "A parallel-optimized linkage method that uses a native Spark dataframe extract\n",
        "of the MPI, coupled with RDD transformation and explosion operations, to rapidly\n",
        "identify all candidates in a provisioned block who are \"true\" matches to an \n",
        "incoming record.\n",
        "'''\n",
        "def link_fhir_record_from_dataset(\n",
        "    record: dict,\n",
        "    algo_config: List[dict],\n",
        "    formatted_cols,\n",
        "    mpi_df\n",
        ") -> List:\n",
        "\n",
        "    # Flatten incoming resource and remove any lurking None's\n",
        "    flattened_record = _flatten_patient_resource(record)\n",
        "    if flattened_record[2] is None:\n",
        "        flattened_record[2] = [\"\"]\n",
        "    if flattened_record[5] is None:\n",
        "        flattened_record[5] = [\"\"]\n",
        "\n",
        "\n",
        "    # Accumulate all matches across all passes to return\n",
        "    # We'll do that as a list of RDDs, since we can leverage schema-less\n",
        "    # unions later on to rapidly put the partitions adjacent to one another\n",
        "    compiled_rdds = []\n",
        "    for linkage_pass in algo_config:\n",
        "        blocking_fields = linkage_pass[\"blocks\"]\n",
        "        field_blocks = extract_blocking_values_from_record(record, blocking_fields)\n",
        "        if len(field_blocks) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Use the extract of the MPI to quickly filter down a block of candidates\n",
        "        data_block = spark_block(field_blocks, mpi_df)\n",
        "        col_to_idx = {v: k for k, v in enumerate(formatted_cols)}\n",
        "\n",
        "        # Parallel process the candidates to find any matches\n",
        "        # Make sure that we cache the results *prior* to explosion, since this will save\n",
        "        # us very expensive lineage evaluations down the road\n",
        "        kwargs = linkage_pass.get(\"kwargs\", {})\n",
        "        matching_records = spark_compare(\n",
        "            data_block, flattened_record, linkage_pass[\"funcs\"], col_to_idx, linkage_pass[\"matching_rule\"], **kwargs\n",
        "        )\n",
        "        matching_records.cache()\n",
        "        exploded_rdd = matching_records.map(lambda x: (flattened_record[0], x))\n",
        "        compiled_rdds.append(exploded_rdd)\n",
        "\n",
        "    return compiled_rdds\n",
        "\n",
        "\n",
        "'''\n",
        "Turn the patient_ids of identified \"found matches\" into the threaded multi-row-indices\n",
        "that the ground truth labeler can understand. This way, all indices are expressed in\n",
        "the same scheme for statistical comparison.\n",
        "'''\n",
        "def map_patient_ids_to_idxs(pids: List, data: pd.DataFrame):\n",
        "    record_idxs = []\n",
        "    for pid in pids:\n",
        "        row_idx = data[data['patient_id'] == pid].index.values\n",
        "        if len(row_idx) > 0:\n",
        "            record_idxs.append(row_idx[0])\n",
        "    return record_idxs\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Simple helper that converts a list of MPI-based UUID patient IDs into indices of those\n",
        "patients in the labeling dataset. This allows us to quickly evaluate stats later.\n",
        "\"\"\"\n",
        "def _map_ids(full_ids, label_set):\n",
        "    return map_patient_ids_to_idxs(full_ids, label_set)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Helper function to massage the result of a parallel groupBy-and-reduce into a structure\n",
        "where each incoming record in the dataset-to-evaluate is given a single row in the output,\n",
        "with all the IDs of the patients it linked to as the value to its key.\n",
        "\"\"\"\n",
        "def _matches_formatter(row, label_set):\n",
        "    new_row = (row[0], [x[1] for x in list(set(row[1]))])\n",
        "    new_row = (map_patient_ids_to_idxs([new_row[0]], label_set)[0], _map_ids(new_row[1], label_set))\n",
        "    new_row = (new_row[0], set([x for x in new_row[1] if x != new_row[0]]))\n",
        "    return new_row\n",
        "\n",
        "\n",
        "'''\n",
        "Find existing patient records in a dataset that map to each incoming record in a block \n",
        "of FHIR data. Since the FHIR data itself is pulled from the MPI, we can freely use it\n",
        "for querying for blocks without risk of finding unrecognized data.\n",
        "'''\n",
        "def link_all_fhir_records_block_dataset(records: List, algo_config: List[dict], label_set, formatted_cols, mpi_df, spark):\n",
        "    found_matches = {}\n",
        "    linked_rdds = []\n",
        "    start = time.time()\n",
        "    for record in records:\n",
        "        ridx = map_patient_ids_to_idxs([record.get(\"id\")], label_set)[0]\n",
        "        linked_records = link_fhir_record_from_dataset(record, algo_config, formatted_cols, mpi_df)\n",
        "        linked_rdds += linked_records\n",
        "\n",
        "    # Take giant union over all RDDs to truncate the lineage chain, caching again to avoid\n",
        "    # re-computing the feature evaluations\n",
        "    sc = spark.sparkContext\n",
        "    all_matches = sc.union(linked_rdds)\n",
        "    all_matches.cache()\n",
        "    match_groups = all_matches.groupBy(lambda x: x[0])\n",
        "    \n",
        "    # Now we have one giant rdd where ecach row is a tuple of (patient_id, IterableOverMatches)\n",
        "    # Re-format that, collect the results, and we're done!\n",
        "    formatted_matches = match_groups.map(lambda row: _matches_formatter(row, label_set))\n",
        "    print(\"finished linking \", str(time.time() - start))\n",
        "    found_matches = formatted_matches.collectAsMap()\n",
        "    return found_matches\n",
        "\n",
        "\n",
        "'''\n",
        "Due to transforming patient_ids back into indices, multiple tuples get inserted for each\n",
        "match, i.e. we record the link (i,j) and the link (j,i), which would skew our stats.\n",
        "This function eliminates these redundancies and makes sure each link is counted once.\n",
        "'''\n",
        "def dedupe_match_double_counts(match_dict):\n",
        "    for k in match_dict:\n",
        "        if k > 0:\n",
        "            lower_set = set(list(range(k)))\n",
        "            match_dict[k] = match_dict[k].difference(lower_set)\n",
        "    return match_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ALGORITHM EVALUATION: LAC EXISTING\n",
        "\n",
        "LAC_ALGO = [\n",
        "    {\n",
        "        \"funcs\": {\n",
        "            \"first_name\": \"feature_match_fuzzy_string\",\n",
        "            \"last_name\": \"feature_match_fuzzy_string\",\n",
        "            \"address\": \"feature_match_fuzzy_string\",\n",
        "            \"mrn\": \"feature_match_fuzzy_string\",\n",
        "        },\n",
        "        \"blocks\": [\n",
        "            {\"value\": \"first_name\", \"transformation\": \"first4\"},\n",
        "            {\"value\": \"last_name\", \"transformation\": \"first4\"},\n",
        "            {\"value\": \"birthdate\"},\n",
        "        ],\n",
        "        \"matching_rule\": \"eval_perfect_match\",\n",
        "        \"cluster_ratio\": 0.9,\n",
        "    },\n",
        "    {\n",
        "        \"funcs\": {\n",
        "            \"first_name\": \"feature_match_fuzzy_string\",\n",
        "            \"last_name\": \"feature_match_fuzzy_string\",\n",
        "            \"address\": \"feature_match_fuzzy_string\",\n",
        "            \"mrn\": \"feature_match_fuzzy_string\",\n",
        "        },\n",
        "        \"blocks\": [\n",
        "            {\"value\": \"first_name\", \"transformation\": \"first4\"},\n",
        "            {\"value\": \"last_name\", \"transformation\": \"first4\"},\n",
        "            {\"value\": \"address\", \"transformation\": \"first4\"},\n",
        "        ],\n",
        "        \"matching_rule\": \"eval_perfect_match\",\n",
        "        \"cluster_ratio\": 0.9,\n",
        "    },\n",
        "    {\n",
        "        \"funcs\": {\n",
        "            \"first_name\": \"feature_match_fuzzy_string\",\n",
        "            \"last_name\": \"feature_match_fuzzy_string\",\n",
        "            \"address\": \"feature_match_fuzzy_string\",\n",
        "            \"mrn\": \"feature_match_fuzzy_string\",\n",
        "        },\n",
        "        \"blocks\": [\n",
        "            {\"value\": \"birthdate\"},\n",
        "        ],\n",
        "        \"matching_rule\": \"eval_perfect_match\",\n",
        "        \"cluster_ratio\": 0.9,\n",
        "    },\n",
        "]\n",
        "\n",
        "found_matches_lac = link_all_fhir_records_block_dataset(evaluation_set, LAC_ALGO, labeling_set, formatted_cols, mpi_df, spark)\n",
        "found_matches_lac = dedupe_match_double_counts(found_matches_lac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ALGORITHM EVALUATION: DIBBs BASIC\n",
        "from phdi.linkage import DIBBS_BASIC\n",
        "found_matches_dibbs_basic = link_all_fhir_records_block_dataset(evaluation_set, DIBBS_BASIC, labeling_set, formatted_cols, mpi_df, spark)\n",
        "found_matches_dibbs_basic = dedupe_match_double_counts(found_matches_dibbs_basic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ALGORITHM EVALUATION: DIBBs ENHANCED\n",
        "from phdi.linkage import DIBBS_ENHANCED\n",
        "found_matches_dibbs_enhanced = link_all_fhir_records_block_dataset(evaluation_set, DIBBS_ENHANCED, labeling_set, formatted_cols, mpi_df, spark)\n",
        "found_matches_dibbs_enhanced = dedupe_match_double_counts(found_matches_dibbs_enhanced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MOUNT THE FILE SYSTEM SO WE CAN WRITE THE OUTPUTS TO FILES\n",
        "\n",
        "# Set paths\n",
        "STORAGE_ACCOUNT = \"$STORAGE_ACCOUNT\"\n",
        "LINKAGE_OUTPUTS_FILESYSTEM = f\"abfss://linkage-notebook-outputs@{STORAGE_ACCOUNT}.dfs.core.windows.net/\"\n",
        "BLOB_STORAGE_LINKED_SERVICE = \"$BLOB_STORAGE_LINKED_SERVICE\"\n",
        "\n",
        "from notebookutils import mssparkutils\n",
        "\n",
        "# Set up for writing to blob storage\n",
        "linkage_bucket_name = \"linkage-notebook-outputs\"\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(BLOB_STORAGE_LINKED_SERVICE)\n",
        "wasb_path = 'wasbs://%s@%s.blob.core.windows.net/' % (linkage_bucket_name, STORAGE_ACCOUNT)\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (linkage_bucket_name, STORAGE_ACCOUNT), blob_sas_token)\n",
        "# Try mounting the remote storage directory at the mount point\n",
        "try:\n",
        "    mssparkutils.fs.mount(\n",
        "        wasb_path,\n",
        "        \"/\",\n",
        "        {\"LinkedService\": f\"${BLOB_STORAGE_LINKED_SERVICE}\"}\n",
        "    )\n",
        "except:\n",
        "    print(\"Already mounted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RECOMPUTE AND EXPORT LOG-ODDS\n",
        "\n",
        "from phdi.linkage import calculate_m_probs, calculate_u_probs, calculate_log_odds\n",
        "import json\n",
        "\n",
        "m_probs = calculate_m_probs(labeling_set, third_party_labels)\n",
        "u_probs = calculate_u_probs(labeling_set, third_party_labels, n_samples=25000)\n",
        "log_odds = calculate_log_odds(m_probs, u_probs)\n",
        "log_odds.pop(\"patient_id\")\n",
        "mssparkutils.fs.put(LINKAGE_OUTPUTS_FILESYSTEM + \"updated_log_odds.json\", json.dumps(log_odds), True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RUN THE NUMBERS AND GET THE STATS FUNCTIONS\n",
        "\n",
        "'''\n",
        "To ensure accurate statistics, the matches and the true matches dictionaries\n",
        "in the statistical evaluation function should have the following form:\n",
        "\n",
        "{\n",
        "    row_num_of_record_in_data: set(row_nums_of_linked_records)\n",
        "}\n",
        "\n",
        "Each row in the data should be represented as a key in both dictionaries.\n",
        "The value for each of these keys should be a set that contains all other\n",
        "row numbers for records in the data set that link to the key record.\n",
        "'''\n",
        "def score_linkage_vs_truth(found_matches, true_matches, num_eval_records, num_label_records):\n",
        "\n",
        "    # Formula is: for m=num_recs in eval_set, n=num_recs in label_set,\n",
        "    # m * (n-1) - [1/2 (m * (m-1))]--each record could match with every other record\n",
        "    # in the MPI besides itself; this double counts matches in the eval_set though,\n",
        "    # since they're also in the MPI, so subtract those away to get accurate results\n",
        "    total_possible_matches = (num_eval_records * (num_label_records - 1.0)) - ((num_eval_records * (num_eval_records - 1.0)) / 2.0)\n",
        "    true_positives = 0.0\n",
        "    false_positives = 0.0\n",
        "    false_negatives = 0.0\n",
        "\n",
        "    for root_record in true_matches:\n",
        "        if root_record in found_matches:\n",
        "            true_positives += len(\n",
        "                true_matches[root_record].intersection(found_matches[root_record])\n",
        "            )\n",
        "            false_positives += len(\n",
        "                found_matches[root_record].difference(true_matches[root_record])\n",
        "            )\n",
        "            false_negatives += len(\n",
        "                true_matches[root_record].difference(found_matches[root_record])\n",
        "            )\n",
        "        else:\n",
        "            false_negatives += len(true_matches[root_record])\n",
        "    for record in set(set(found_matches.keys()).difference(true_matches.keys())):\n",
        "        false_positives += len(found_matches[record])\n",
        "\n",
        "    true_negatives = (\n",
        "        total_possible_matches - true_positives - false_positives - false_negatives\n",
        "    )\n",
        "\n",
        "    # Calculate some stats, but watch out for division by 0 if we happened to pick\n",
        "    # a bad subsample\n",
        "    if true_negatives + false_negatives > 0:\n",
        "        npv = round((true_negatives / (true_negatives + false_negatives)), 3)\n",
        "    else:\n",
        "        npv = \"N/A\"\n",
        "    if true_positives + false_negatives > 0:\n",
        "        sensitivity = round(true_positives / (true_positives + false_negatives), 3)\n",
        "    else:\n",
        "        sensitivity = \"N/A\"\n",
        "    if true_negatives + false_positives > 0:\n",
        "        specificity = round(true_negatives / (true_negatives + false_positives), 3)\n",
        "    else:\n",
        "        specificity = \"N/A\"\n",
        "    if true_positives + false_positives > 0:\n",
        "        ppv = round(true_positives / (true_positives + false_positives), 3)\n",
        "    else:\n",
        "        ppv = \"N/A\"\n",
        "    if (2 * true_positives + false_negatives + false_positives) > 0:\n",
        "        f1 = round(\n",
        "            (2 * true_positives) / (2 * true_positives + false_negatives + false_positives), 3\n",
        "        )\n",
        "    else:\n",
        "        f1 = \"N/A\"\n",
        "\n",
        "    return {\n",
        "        \"tp\": true_positives,\n",
        "        \"fp\": false_positives,\n",
        "        \"fn\": false_negatives,\n",
        "        \"sens\": sensitivity,\n",
        "        \"spec\": specificity,\n",
        "        \"ppv\": ppv,\n",
        "        \"npv\": npv,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "display_str = \"\"\n",
        "\n",
        "if RECORDS_TO_SAMPLE is not None:\n",
        "    n_records = RECORDS_TO_SAMPLE\n",
        "else:\n",
        "    n_records = len(evaluation_set)\n",
        "\n",
        "for lbl_type in [\"va\", \"emc\"]:\n",
        "    if lbl_type == \"va\":\n",
        "        labels = va_labels\n",
        "    else:\n",
        "        labels = third_party_labels\n",
        "    \n",
        "    stats_dict_lac = score_linkage_vs_truth(found_matches_lac, labels, n_records)\n",
        "    stats_dict_dibbs_b = score_linkage_vs_truth(found_matches_dibbs_basic, labels, n_records)\n",
        "    stats_dict_dibbs_e = score_linkage_vs_truth(found_matches_dibbs_enhanced, labels, n_records)\n",
        "\n",
        "    display_str += \"DISPLAYING EVALUATION ON \" + lbl_type.upper() + \" LABELS:\\n\"\n",
        "    display_str += \"\\n\"\n",
        "\n",
        "    for algo in [\"lac\", \"basic\", \"enhanced\"]:\n",
        "        if algo == \"lac\":\n",
        "            display_str += \"LAC Existing Algorithm:\\n\"\n",
        "            scores = stats_dict_lac\n",
        "        elif algo == \"basic\":\n",
        "            display_str += \"DIBBs Basic Algorithm:\\n\"\n",
        "            scores = stats_dict_dibbs_b\n",
        "        else:\n",
        "            display_str += \"DIBBs Log-Odds Algorithm:\\n\"\n",
        "            scores = stats_dict_dibbs_e\n",
        "\n",
        "        display_str += \"True Positives: \" + str(scores[\"tp\"]) + \"\\n\"\n",
        "        display_str += \"False Positives: \" + str(scores[\"fp\"]) + \"\\n\"\n",
        "        display_str += \"False Negatives: \" + str(scores[\"fn\"]) + \"\\n\"\n",
        "        display_str += \"Sensitivity: \" + str(scores[\"sens\"]) + \"\\n\"\n",
        "        display_str += \"Specificity: \" + str(scores[\"spec\"]) + \"\\n\"\n",
        "        display_str += \"PPV: \" + str(scores[\"ppv\"]) + \"\\n\"\n",
        "        display_str += \"NPV: \" + str(scores[\"npv\"]) + \"\\n\"\n",
        "        display_str += \"F1: \" + str(scores[\"f1\"]) + \"\\n\"\n",
        "        display_str += \"\\n\"\n",
        "    \n",
        "    display_str += \"\\n\"\n",
        "\n",
        "print(display_str)\n",
        "\n",
        "mssparkutils.fs.put(LINKAGE_OUTPUTS_FILESYSTEM + \"results.txt\", display_str, True)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
