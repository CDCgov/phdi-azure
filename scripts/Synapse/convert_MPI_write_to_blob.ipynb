{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# convert_MPI_write_to_blob\n",
        "\n",
        "This notebook reads in patient data from an uploaded parquet file (`mpi_incoming_file_path`), converts to FHIR, and writes the data to blob storage."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install azure-identity phdi"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Set up parameters for connecting to the storage account, vault client, and record linkage container app."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from notebookutils import mssparkutils\n",
        "from phdi.linkage.seed import convert_to_patient_fhir_resources\n",
        "from phdi.linkage.link import generate_hash_str\n",
        "from datetime import date\n",
        "import json\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "from random import randint\n",
        "\n",
        "# Set up file client\n",
        "storage_account = \"$STORAGE_ACCOUNT\"\n",
        "source_data_bucket = \"source-data\"\n",
        "patient_data_bucket = \"patient-data\"\n",
        "file_path = \"MPI.parquet\"\n",
        "storage_account_url = f\"https://{storage_account}.blob.core.windows.net/\"\n",
        "mpi_incoming_file_path = f\"abfss://{patient_data_bucket}@{storage_account}.dfs.core.windows.net/{file_path}\"\n",
        "\n",
        "# Set up for writing to blob storage\n",
        "blob_relative_path = \"\"\n",
        "blob_storage_linked_service = \"$BLOB_STORAGE_LINKED_SERVICE\" \n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(blob_storage_linked_service)\n",
        "wasb_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (source_data_bucket, storage_account, blob_relative_path)\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (source_data_bucket, storage_account), blob_sas_token)\n",
        "# Try mounting the remote storage directory at the mount point\n",
        "try:\n",
        "    mssparkutils.fs.mount(\n",
        "        wasb_path,\n",
        "        \"/\",\n",
        "        {\"LinkedService\": blob_storage_linked_service}\n",
        "    )\n",
        "except:\n",
        "    print(\"Already mounted\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def convert_write_data(row,bucket_name,storage_account,curr_date):\r\n",
        "    iris_id, fhir_bundle = convert_to_patient_fhir_resources(row.asDict())\r\n",
        "    patient = [\r\n",
        "        r for r in fhir_bundle.get(\"entry\", []) if r.get(\"resource\", {}).get(\"resourceType\", \"\") == \"Patient\"\r\n",
        "    ]\r\n",
        "    patient[0][\"meta\"] = {\"source\": \"uri:iris\"}\r\n",
        "\r\n",
        "    # generate unique hash for writing files\r\n",
        "    salt_str = \"salt\"\r\n",
        "    hash = generate_hash_str(json.dumps(fhir_bundle),salt_str)\r\n",
        "\r\n",
        "    # Write file to storage pre-harmonization\r\n",
        "    pre_filename = f\"abfss://{bucket_name}@{storage_account}.dfs.core.windows.net/fhir/lac_extract_{str(curr_date)}_{str(hash)}.json\"\r\n",
        "    mssparkutils.fs.put(pre_filename, json.dumps(fhir_bundle), True)\r\n",
        "\r\n",
        "async def batch_seed(rows,bucket_name,storage_account,curr_date):\r\n",
        "    resps = await asyncio.gather(*[convert_write_data(row,bucket_name,storage_account,curr_date) for row in rows])\r\n",
        "\r\n",
        "def harmonize(data):\r\n",
        "    data = standardize_names(data = data)\r\n",
        "    data = standardize_phones(data = data)\r\n",
        "    try:\r\n",
        "        data = standardize_dob(data = data)\r\n",
        "    except Exception as e:\r\n",
        "        pass\r\n",
        "    return data"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparkpool",
              "session_id": "82",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-08-02T17:39:16.2623826Z",
              "session_start_time": null,
              "execution_start_time": "2023-08-02T17:39:17.5133005Z",
              "execution_finish_time": "2023-08-02T17:39:17.6886572Z",
              "spark_jobs": null,
              "parent_msg_id": "84ad0f85-a015-4036-bd37-c7662c55a6dc"
            },
            "text/plain": "StatementMeta(sparkpool, 82, 8, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Harmonize data and write to blob storage\r\n",
        "def pre_process(mpi_incoming_file_path):\r\n",
        "    curr_date = date.today()\r\n",
        "    df = spark.read.parquet(mpi_incoming_file_path)\r\n",
        "    all_rows = df.collect()\r\n",
        "    batch_size = 10000\r\n",
        "  \r\n",
        "    for lower_bound in range(0,len(all_rows),batch_size):\r\n",
        "        upper_bound = lower_bound+batch_size\r\n",
        "        print(\"lower_bound:\",lower_bound)\r\n",
        "        print(\"upper_bound:\",upper_bound)\r\n",
        "        batch = all_rows[lower_bound:upper_bound]\r\n",
        "        asyncio.run(batch_seed(batch,source_data_bucket,storage_account,curr_date))\r\n",
        "\r\n",
        "pre_process(mpi_incoming_file_path)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}