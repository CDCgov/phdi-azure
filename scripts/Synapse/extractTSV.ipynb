{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import lit, col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "STORAGE_ACCOUNT = \"$STORAGE_ACCOUNT\"\n",
    "ECR_DELTA_TABLE_FILE_PATH = \"ecr-datastore\"\n",
    "COVID_IDENTIFICATION_CONFIG_FILE_PATH = f\"abfss://delta-tables@{STORAGE_ACCOUNT}.dfs.core.windows.net/covid_identification_config.json\"\n",
    "ECR_WITH_IDS_FILE_PATH = f\"abfss://delta-tables@{STORAGE_ACCOUNT}.dfs.core.windows.net/ecr_tsvs/matched_ecrs/{current_datetime}.tsv\"\n",
    "ECR_WITHOUT_IDS_FILE_PATH = f\"abfss://delta-tables@{STORAGE_ACCOUNT}.dfs.core.windows.net/ecr_tsvs/unmatched_ecrs/{current_datetime}.tsv\"\n",
    "\n",
    "# ECR\n",
    "ecr = spark.read.format(\"delta\").load(ECR_DELTA_TABLE_FILE_PATH)\n",
    "\n",
    "# Covid identification data\n",
    "df = spark.read.json(COVID_IDENTIFICATION_CONFIG_FILE_PATH, multiLine=True)\n",
    "covid_test_type_codes = df.select('covid_test_type_codes').rdd.flatMap(lambda x: x).collect()[0]\n",
    "\n",
    "\n",
    "TEST_TEMPLATE = [\n",
    "    \"test_type_\",\n",
    "    \"test_result_\",\n",
    "    \"test_result_interp_\",\n",
    "    \"specimen_type_\",\n",
    "    \"performing_lab_\",\n",
    "    \"specimen_collection_date_\",\n",
    "    \"result_date_\"\n",
    "]\n",
    "\n",
    "iris_tsv_column_aliases = {\n",
    "    \"incident_id\":\"Incident ID\",\n",
    "    \"last_name\":\" Last name\",\n",
    "    \"first_name\": \"First name\",\n",
    "    # Section action\n",
    "    # Section instance\n",
    "    \"rr_id\": \"RR ID\",\n",
    "    \"status\": \"Status\",\n",
    "    \"conditions\": \"Conditions\",\n",
    "    \"eicr_id\": \"eICR ID\",\n",
    "    \"eicr_version_number\":\"eICR Version Number\",\n",
    "    \"authoring_datetime\": \"Authoring date/time\",\n",
    "    \"provider_id\": \"Provider ID\",\n",
    "    \"facility_id_number\": \"Facility ID Number\",\n",
    "    \"facility_name\": \"Facility Name\",\n",
    "    \"facility_type\": \"Facility Type/Hospital unit\",\n",
    "    \"encounter_type\": \"Encounter Details: type\",\n",
    "    \"encounter_start_date\": \"Encounter Details: date (from)\",\n",
    "    \"encounter_end_date\": \"Encounter Details: date (to)\",\n",
    "    \"active_problem_1\": \"Active Problem 1\",\n",
    "    \"active_problem_date_1\": \"Active Problem Noted Date 1\",\n",
    "    \"active_problem_2\": \"Active Problem 2\",\n",
    "    \"active_problem_date_2\": \"Active Problem Noted Date 2\",\n",
    "    \"active_problem_3\": \"Active Problem 3\",\n",
    "    \"active_problem_date_3\": \"Active Problem Noted Date 3\",\n",
    "    \"active_problem_4\": \"Active Problem 4\",\n",
    "    \"active_problem_date_4\": \"Active Problem Noted Date 4\",\n",
    "    \"active_problem_5\": \"Active Problem 5\",\n",
    "    \"active_problem_date_5\": \"Active Problem Noted Date 5\",\n",
    "    \"reason_for_visit\": \"Reason for visit\",\n",
    "    # Comments\n",
    "    \"test_type_1\": \"Test Type 1\",\n",
    "    \"test_result_1\": \"Test Result 1\",\n",
    "    # \"test_result_interp_1\"\n",
    "    \"specimen_type_1\": \"Specimen Type 1\",\n",
    "    \"performing_lab_1\": \"Performing Lab 1\",\n",
    "    \"specimen_collection_date_1\": \"Specimen Collection Date 1\",\n",
    "    \"result_date_1\": \"Result Date 1\",\n",
    "    \"test_type_2\": \"Test Type 2\",\n",
    "    \"test_result_2\": \"Test Result 2\",\n",
    "    # \"test_result_interp_2\"\n",
    "    \"specimen_type_2\": \"Specimen Type 2\",\n",
    "    \"performing_lab_2\": \"Performing Lab 2\",\n",
    "    \"specimen_collection_date_2\": \"Specimen Collection Date 2\",\n",
    "    \"result_date_2\": \"Result Date 2\"\n",
    "    # \"Note\"\n",
    "    }\n",
    "\n",
    "\n",
    "def filter_tests(df_row, test_cols):\n",
    "    \"\"\"\n",
    "    Filters a given row from the ECR data store by COVID-relatedness of tests.\n",
    "    For each row, a list is constructed holding the test numbers of only the\n",
    "    tests that map to a predefined list of covid test types.\n",
    "    \"\"\"\n",
    "    row_dict = df_row.asDict()\n",
    "    filtered_tests = [c for c in test_cols if df_row[c] in covid_test_type_codes]\n",
    "    filtered_tests = [f.split(\"_\")[-1] for f in filtered_tests]\n",
    "    row_dict[\"covid_tests\"] = filtered_tests\n",
    "    return row_dict\n",
    "\n",
    "def identify_recent_tests(row_dict):\n",
    "    \"\"\"\n",
    "    Given a dictionary-formatted RDD row that's had covid-related tests identified,\n",
    "    determines which of those tests are the two most recent. If a row has no \n",
    "    associated covid tests, the created recency columns are left blank, and if\n",
    "    there's only one, the `recent_test_1` column is populated while the other is\n",
    "    left blank. Otherwise, both recency columns are populated with the test number\n",
    "    of either the first or second most recent covid related test.\n",
    "    \"\"\"\n",
    "    covid_tests = row_dict[\"covid_tests\"]\n",
    "    row_dict[\"recent_covid_test_1\"] = None\n",
    "    row_dict[\"recent_covid_test_2\"] = None\n",
    "    if len(covid_tests) == 0:\n",
    "        return row_dict\n",
    "    if len(covid_tests) == 1:\n",
    "        row_dict[\"recent_covid_test_1\"] == covid_tests[0]\n",
    "        return row_dict\n",
    "    tests_to_dates = dict(zip(covid_tests, [row_dict[\"result_date_\" + str(v)] for v in covid_tests]))\n",
    "    sorted_tests = sorted(tests_to_dates.items(), key=lambda x: x[1], reverse=True)\n",
    "    row_dict[\"recent_covid_test_1\"] = sorted_tests[0][0]\n",
    "    row_dict[\"recent_covid_test_2\"] = sorted_tests[1][0]\n",
    "    return row_dict\n",
    "\n",
    "def rewrite_test_x_with_test_y(row_dict, test_to_rewrite):\n",
    "    \"\"\"\n",
    "    Given a dictionary-formatted spark RDD row that's had recent covid tests identified,\n",
    "    replaces the values of test 1 and test 2 with the information pertaining to the\n",
    "    most recent and second most recent covid tests, respectively.\n",
    "    \"\"\"\n",
    "    replace_with_test = row_dict[\"recent_covid_test_\" + str(test_to_rewrite)]\n",
    "    if replace_with_test is None:\n",
    "        return row_dict\n",
    "    for test_field in TEST_TEMPLATE:\n",
    "        row_dict[test_field + str(test_to_rewrite)] = row_dict[test_field + replace_with_test]\n",
    "    return row_dict\n",
    "    \n",
    "\n",
    "# Explicitly define new schema to prevent interpretation breakage during RDD map\n",
    "new_schema = ecr.schema.add(StructField(\"covid_tests\", StringType(), True))\n",
    "new_schema = new_schema.add(StructField(\"recent_covid_test_1\", StringType(), True))\n",
    "new_schema = new_schema.add(StructField(\"recent_covid_test_2\", StringType(), True))\n",
    "\n",
    "# Apply map functions to parallelize row processing and identify covid tests\n",
    "test_cols = [c for c in ecr.columns if \"test_type\" in c]\n",
    "rdd2 = ecr.rdd.map(lambda x: filter_tests(x, test_cols))\n",
    "rdd3 = rdd2.map(lambda x: identify_recent_tests(x))\n",
    "rdd4 = rdd3.map(lambda x: rewrite_test_x_with_test_y(x, 1))\n",
    "rdd5 = rdd4.map(lambda x: rewrite_test_x_with_test_y(x, 2))\n",
    "\n",
    "# Convert back to DF and drop the temp cols we created as well as superfluous tests\n",
    "ecr = spark.createDataFrame(rdd5, new_schema)\n",
    "cols_to_drop = [\"covid_tests\", \"recent_covid_test_1\", \"recent_covid_test_2\"]\n",
    "for i in range(3,21):\n",
    "    for col_name in TEST_TEMPLATE:\n",
    "        cols_to_drop.append(col_name + str(i))\n",
    "ecr = ecr.drop(*cols_to_drop)\n",
    "\n",
    "# Now, rename all the processed columns to what LAC would like in their output\n",
    "ecr = ecr.select(\n",
    "    col(\"incident_id\").alias(iris_tsv_column_aliases[\"incident_id\"]),\n",
    "    # col(\"iris_id\"),\n",
    "    # col(\"patient_id\"),\n",
    "    # col(\"person_id\"),\n",
    "    col(\"first_name\").alias(iris_tsv_column_aliases[\"first_name\"]),\n",
    "    col(\"last_name\").alias(iris_tsv_column_aliases[\"last_name\"]),\n",
    "    lit(\"[ListSectionInsert]\").alias(\"Section action\"),\n",
    "    lit(None).cast('string').alias(\"Section instance\"),\n",
    "    col(\"rr_id\").alias(iris_tsv_column_aliases[\"rr_id\"]),\n",
    "    col(\"status\").alias(iris_tsv_column_aliases[\"status\"]),\n",
    "    col(\"conditions\").alias(iris_tsv_column_aliases[\"conditions\"]),\n",
    "    col(\"eicr_id\").alias(iris_tsv_column_aliases[\"eicr_id\"]),\n",
    "    col(\"eicr_version_number\").alias(iris_tsv_column_aliases[\"eicr_version_number\"]),\n",
    "    col(\"authoring_datetime\").alias(iris_tsv_column_aliases[\"authoring_datetime\"]),\n",
    "    col(\"provider_id\").alias(iris_tsv_column_aliases[\"provider_id\"]),\n",
    "    col(\"facility_id_number\").alias(iris_tsv_column_aliases[\"facility_id_number\"]),\n",
    "    col(\"facility_name\").alias(iris_tsv_column_aliases[\"facility_name\"]),\n",
    "    col(\"facility_type\").alias(iris_tsv_column_aliases[\"facility_type\"]),\n",
    "    col(\"encounter_type\").alias(iris_tsv_column_aliases[\"encounter_type\"]),\n",
    "    col(\"encounter_start_date\").alias(iris_tsv_column_aliases[\"encounter_start_date\"]),\n",
    "    col(\"encounter_end_date\").alias(iris_tsv_column_aliases[\"encounter_end_date\"]),\n",
    "    col(\"active_problem_1\").alias(iris_tsv_column_aliases[\"active_problem_1\"]),\n",
    "    col(\"active_problem_date_1\").alias(iris_tsv_column_aliases[\"active_problem_date_1\"]),\n",
    "    col(\"active_problem_2\").alias(iris_tsv_column_aliases[\"active_problem_2\"]),\n",
    "    col(\"active_problem_date_2\").alias(iris_tsv_column_aliases[\"active_problem_date_2\"]),\n",
    "    col(\"active_problem_3\").alias(iris_tsv_column_aliases[\"active_problem_3\"]),\n",
    "    col(\"active_problem_date_3\").alias(iris_tsv_column_aliases[\"active_problem_date_3\"]),\n",
    "    col(\"active_problem_4\").alias(iris_tsv_column_aliases[\"active_problem_4\"]),\n",
    "    col(\"active_problem_date_4\").alias(iris_tsv_column_aliases[\"active_problem_date_4\"]),\n",
    "    col(\"active_problem_5\").alias(iris_tsv_column_aliases[\"active_problem_5\"]),\n",
    "    col(\"active_problem_date_5\").alias(iris_tsv_column_aliases[\"active_problem_date_5\"]),\n",
    "    col(\"reason_for_visit\").alias(iris_tsv_column_aliases[\"reason_for_visit\"]),\n",
    "    lit(None).cast('string').alias(\"Comments\"),\n",
    "    col(\"test_type_1\").alias(iris_tsv_column_aliases[\"test_type_1\"]),\n",
    "    col(\"test_result_1\").alias(iris_tsv_column_aliases[\"test_result_1\"]),\n",
    "    col(\"specimen_type_1\").alias(iris_tsv_column_aliases[\"specimen_type_1\"]),\n",
    "    col(\"performing_lab_1\").alias(iris_tsv_column_aliases[\"performing_lab_1\"]),\n",
    "    col(\"specimen_collection_date_1\").alias(iris_tsv_column_aliases[\"specimen_collection_date_1\"]),\n",
    "    col(\"result_date_1\").alias(iris_tsv_column_aliases[\"result_date_1\"]),\n",
    "    col(\"test_type_2\").alias(iris_tsv_column_aliases[\"test_type_2\"]),\n",
    "    col(\"test_result_2\").alias(iris_tsv_column_aliases[\"test_result_2\"]),\n",
    "    col(\"specimen_type_2\").alias(iris_tsv_column_aliases[\"specimen_type_2\"]),\n",
    "    col(\"performing_lab_2\").alias(iris_tsv_column_aliases[\"performing_lab_2\"]),\n",
    "    col(\"specimen_collection_date_2\").alias(iris_tsv_column_aliases[\"specimen_collection_date_2\"]),\n",
    "    col(\"result_date_2\").alias(iris_tsv_column_aliases[\"result_date_2\"]),\n",
    "    lit(\"eCR data added to UDF through import utility\").alias(\"Note\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsv_flatten(row):\n",
    "    \"\"\"\n",
    "    Given a row from a spark RDD, extract some information to create a pair of TSV rows\n",
    "    for each row in the RDD. The first of these rows is a simple name, incident, and note\n",
    "    header, while the second contains the comprehensive value set built above.\n",
    "    \"\"\"\n",
    "    incident_id = row[iris_tsv_column_aliases[\"incident_id\"]]\n",
    "    last_name = row[iris_tsv_column_aliases[\"last_name\"]]\n",
    "    first_name = row[iris_tsv_column_aliases[\"first_name\"]]\n",
    "    section_action = row[\"Section action\"]\n",
    "    section_instance = row[\"Section instance\"]\n",
    "    comments = row[\"Comments\"]\n",
    "    note = row[\"Note\"]\n",
    "    none_tuple_1 = (None,) * (24)\n",
    "    none_tuple_2 = (None,) * (12)\n",
    "    return iter([(incident_id, first_name, last_name, section_action, section_instance) + none_tuple_1 + (comments,) + none_tuple_2 + (note,)] + [row])\n",
    "\n",
    "flattend_rdd=ecr.rdd.flatMap(tsv_flatten)\n",
    "\n",
    "# DF will break with this because properties aren't supposed to be nullable,\n",
    "# so change those fields because we're just writing output\n",
    "for sf in ecr.schema:\n",
    "    sf.nullable = True\n",
    "flattened_df = spark.createDataFrame(flattend_rdd, ecr.schema)\n",
    "\n",
    "\n",
    "# Now split this up into those that have incident IDs and those that don't\n",
    "df_with_ids = flattened_df.filter(flattened_df[\"Incident ID\"].isNotNull())\n",
    "df_no_ids = flattened_df.filter(flattened_df[\"Incident ID\"].isNull())\n",
    "df_with_ids.coalesce(1).write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").option(\"sep\", \"\\t\").csv(ECR_WITH_IDS_FILE_PATH)\n",
    "df_no_ids.coalesce(1).write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").option(\"sep\", \"\\t\").csv(ECR_WITHOUT_IDS_FILE_PATH)"
   ]
  }
 ],
 "metadata": {
  "description": null,
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "save_output": true,
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
