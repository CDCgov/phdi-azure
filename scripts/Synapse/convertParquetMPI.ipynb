{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# convertParquetMPI\n",
        "\n",
        "This notebook reads in patient data from an uploaded parquet file (`mpi_incoming_file_path`), converts the data to FHIR bundles, and posts the FHIR bundles to the Record Linkage endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install phdi==1.0.6 azure-keyvault-secrets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up parameters for connecting to the storage account, vault client, and record linkage container app."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from phdi.linkage.seed import convert_to_patient_fhir_resources\n",
        "from azure.identity import ManagedIdentityCredential\n",
        "from azure.core.credentials import AccessToken\n",
        "from azure.keyvault.secrets import SecretClient\n",
        "import requests\n",
        "import time\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Set up file client and endpoint credentials\n",
        "account_name = \"$STORAGE_ACCOUNT\"\n",
        "file_system_name = \"patient-data\"\n",
        "file_path = \"MPI.parquet\"\n",
        "mpi_incoming_file_path = f\"abfss://{file_system_name}@{account_name}.dfs.core.windows.net/{file_path}\"\n",
        "\n",
        "class spoof_token:\n",
        "    def get_token(*args, **kwargs):\n",
        "        return AccessToken(\n",
        "            token=mssparkutils.credentials.getToken(audience=\"vault\"),\n",
        "            expires_on=int(time.time())+60*10 # some random time in future... Synapse doesn't document how to get the actual time\n",
        "        )\n",
        "\n",
        "credential = ManagedIdentityCredential()\n",
        "credential._credential = spoof_token() # monkey-patch the contents of the private `_credential`\n",
        "\n",
        "# Set up key vault client\n",
        "vault_name = \"$KEY_VAULT\"\n",
        "vault_url = f\"https://{vault_name}.vault.azure.net/\"\n",
        "\n",
        "client = SecretClient(vault_url=vault_url, credential=credential)\n",
        "\n",
        "# Get client ID and secret for GitHub app registration\n",
        "client_id = client.get_secret(\"synapse-client-id\").value\n",
        "client_secret = client.get_secret(\"synapse-client-secret\").value\n",
        "\n",
        "# Get access token for record linkage container app\n",
        "tenant_id = \"$TENANT_ID\"\n",
        "url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\"\n",
        "\n",
        "data = {\n",
        "    'grant_type': 'client_credentials',\n",
        "    'client_id': client_id,\n",
        "    'client_secret': client_secret,\n",
        "    'scope': 'api://phdi-dev-record-linkage/.default'\n",
        "}\n",
        "\n",
        "response = requests.post(url, data=data)\n",
        "access_token = response.json()['access_token']\n",
        "\n",
        "# Make request to record linkage container app\n",
        "record_linkage_url = client.get_secret(\"record-linkage-url\").value + \"/link-record\" \n",
        "headers = {\n",
        "    'Authorization': f'Bearer {access_token}'\n",
        "}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read the MPI parquet data into a spark dataframe. Iterate over each row of patient data in the dataframe and convert to a FHIR bundle and associated iris_id. Create a POST request to the record linkage container with FHIR bundle and iris_id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert and post data from mpi_incoming_file_path\n",
        "def convert(mpi_incoming_file_path):\n",
        "    df = spark.read.parquet(mpi_incoming_file_path)\n",
        "    for row in df.collect():\n",
        "        iris_id, fhir_bundle = convert_to_patient_fhir_resources(row.asDict())\n",
        "        data = {\n",
        "        'bundle': fhir_bundle,\n",
        "        'external_person_id': iris_id\n",
        "    }\n",
        "        requests.post(record_linkage_url, headers=headers, json=data)\n",
        "\n",
        "convert(mpi_incoming_file_path)\n",
        "\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
