{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# convertParquetMPI\n",
        "\n",
        "This notebook reads in patient data from an uploaded parquet file (`mpi_incoming_file_path`), converts the data to FHIR bundles, and posts the FHIR bundles to the Record Linkage endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install azure-identity phdi"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up parameters for connecting to the storage account, vault client, and record linkage container app."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from notebookutils import mssparkutils\n",
        "from phdi.linkage.seed import convert_to_patient_fhir_resources\n",
        "from azure.identity import DefaultAzureCredential\n",
        "import requests\n",
        "import time\n",
        "from datetime import date\n",
        "from phdi.transport.http import http_request_with_retry\n",
        "from phdi.fhir.harmonization.standardization import (\n",
        "    standardize_names,\n",
        "    standardize_phones,\n",
        "    standardize_dob,\n",
        ")\n",
        "# from phdi.cloud.azure import AzureCloudContainerConnection, AzureCredentialManager\n",
        "from phdi.fhir.geospatial import SmartyFhirGeocodeClient\n",
        "import json\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Set up file client and endpoint credentials\n",
        "account_name = \"$STORAGE_ACCOUNT\"\n",
        "endpoint_scope = \"$SCOPE\"\n",
        "snapshot_bucket_name = \"bundle-snapshots\"\n",
        "file_system_name = \"patient-data\"\n",
        "file_path = \"MPI.parquet\"\n",
        "storage_account_url = f\"https://{account_name}.blob.core.windows.net/\"\n",
        "mpi_incoming_file_path = f\"abfss://{file_system_name}@{account_name}.dfs.core.windows.net/{file_path}\"\n",
        "\n",
        "# Set up for writing to blob storage\n",
        "blob_relative_path = \"\"\n",
        "linked_service_name = \"AzureBlobStorage1\" \n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\n",
        "wasb_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (snapshot_bucket_name, account_name, blob_relative_path)\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (snapshot_bucket_name, account_name), blob_sas_token)\n",
        "# Try mounting the remote storage directory at the mount point\n",
        "try:\n",
        "    mssparkutils.fs.mount(\n",
        "        wasb_path,\n",
        "        \"/\",\n",
        "        {\"LinkedService\": \"AzureBlobStorage1\"}\n",
        "    )\n",
        "except:\n",
        "    print(\"Already mounted\")\n",
        "\n",
        "# Set up key vault client\n",
        "vault_name = \"$KEY_VAULT\"\n",
        "vault_url = f\"https://{vault_name}.vault.azure.net/\"\n",
        "vault_linked_service = \"$KEY_VAULT_LINKED_SERVICE\"\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "# Get client ID and secret for GitHub app registration\n",
        "client_id = TokenLibrary.getSecret(vault_name,\"synapse-client-id\",vault_linked_service)\n",
        "client_secret = TokenLibrary.getSecret(vault_name,\"synapse-client-secret\",vault_linked_service)\n",
        "\n",
        "\n",
        "# Get smarty client information\n",
        "smarty_auth_id = TokenLibrary.getSecret(vault_name,\"smarty-auth-id\",vault_linked_service)\n",
        "smarty_auth_token = TokenLibrary.getSecret(vault_name,\"smarty-auth-token\",vault_linked_service)\n",
        "\n",
        "# Get access token for record linkage container app\n",
        "tenant_id = \"$TENANT_ID\"\n",
        "endpoint_login_url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\"\n",
        "\n",
        "# # # Set up Azure cloud managers\n",
        "# azure_credential_manager = AzureCredentialManager(resource_location=storage_account_url, \n",
        "#                                             credential_type=\"service-principle\", \n",
        "#                                             client_id=client_id, \n",
        "#                                             tenant_id=tenant_id, \n",
        "#                                             client_secret=client_secret)\n",
        "# azure_connection_manager = AzureCloudContainerConnection(cred_manager=azure_credential_manager,storage_account_url=storage_account_url)\n",
        "\n",
        "\n",
        "# Set up for record linkage and ingestion access\n",
        "def set_up_endpoint_access(endpoint, url):\n",
        "    data = {\n",
        "        'grant_type': 'client_credentials',\n",
        "        'client_id': client_id,\n",
        "        'client_secret': client_secret,\n",
        "        'scope': f'api://{endpoint_scope}-{endpoint}/.default'\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, data=data)\n",
        "    access_token = response.json()['access_token']\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {access_token}'\n",
        "    }\n",
        "    return data, headers\n",
        "\n",
        "record_linkage_data, record_linkage_headers = set_up_endpoint_access(endpoint=\"record-linkage\",url=endpoint_login_url)\n",
        "ingestion_data, ingestion_headers = set_up_endpoint_access(endpoint=\"ingestion\",url=endpoint_login_url)\n",
        "\n",
        "# Define function for refreshing Bearer access token\n",
        "def request_with_token_refresh(url,headers,data,endpoint_type,endpoint_login_url,try_again=True):\n",
        "    resp = http_request_with_retry(url=url,retry_count=2,request_type=\"POST\",headers=headers,allowed_methods=[\"POST\"],data=data)\n",
        "    if resp.status_code == 401 and try_again:\n",
        "        _, headers = set_up_endpoint_access(endpoint=endpoint_type,url=endpoint_login_url)\n",
        "        print(\"got new headers\")\n",
        "        return request_with_token_refresh(url,headers,data,endpoint_type,endpoint_login_url,try_again=False)\n",
        "    if resp.status_code != 200:\n",
        "        print(resp.text)\n",
        "    try:\n",
        "        resp.raise_for_status()\n",
        "    except Exception as e:\n",
        "        print(\"request failed\")\n",
        "        print(e)\n",
        "        print(url)\n",
        "        print(data)\n",
        "        print(headers)\n",
        "        print(endpoint_type)\n",
        "        print()\n",
        "\n",
        "    return resp\n",
        "\n",
        "# Get URLs for container apps\n",
        "record_linkage_url = TokenLibrary.getSecret(vault_name,\"record-linkage-url\",vault_linked_service)+ \"/link-record\" \n",
        "ingestion_url = TokenLibrary.getSecret(vault_name,\"ingestion-url\",vault_linked_service)\n",
        "write_to_storage_url = ingestion_url + \"/cloud/storage/write_blob_to_storage\"\n",
        "\n",
        "# Instantiate Smarty Client\n",
        "geocode_client = SmartyFhirGeocodeClient(\n",
        "                smarty_auth_id=smarty_auth_id,\n",
        "                smarty_auth_token=smarty_auth_token,\n",
        "                licenses=[\"us-rooftop-geocoding-enterprise-cloud\"]\n",
        "            )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read the MPI parquet data into a spark dataframe. Iterate over each row of patient data in the dataframe and convert to a FHIR bundle and associated iris_id. Create a POST request to the record linkage container with FHIR bundle and iris_id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert and post data from mpi_incoming_file_path\n",
        "def convert(mpi_incoming_file_path):\n",
        "    df = spark.read.parquet(mpi_incoming_file_path)\n",
        "    curr_date = date.today()\n",
        "    file_idx = 0\n",
        "    for row in df.collect()[0:4]:\n",
        "        file_idx += 1\n",
        "        print(file_idx)\n",
        "        iris_id, fhir_bundle = convert_to_patient_fhir_resources(row.asDict())\n",
        "\n",
        "        # Add metadata to LAC Extract patientdenoting it came from IRIS\n",
        "        # We know this URI is ~probably~ not right but Brady might know what to use...\n",
        "        patients = [\n",
        "            r for r in fhir_bundle.get(\"entry\", []) if r.get(\"resource\", {}).get(\"resourceType\", \"\") == \"Patient\"\n",
        "        ]\n",
        "        patients[0][\"meta\"] = {\"source\": \"uri:iris\"}\n",
        "        data = {\n",
        "            'bundle': fhir_bundle,\n",
        "            'external_person_id': iris_id\n",
        "        }\n",
        "\n",
        "\n",
        "        # Store a snapshot of the FHIR bundle before it hits standardization\n",
        "        pre_filename = f\"abfss://{snapshot_bucket_name}@{account_name}.dfs.core.windows.net/pre/lac_extract_{str(curr_date)}_{str(file_idx)}.json\"\n",
        "        pre_write_data = {\n",
        "            'blob': data[\"bundle\"],\n",
        "            'cloud_provider': 'azure',\n",
        "            'bucket_name': snapshot_bucket_name,\n",
        "            'file_name': pre_filename,\n",
        "            'storage_account_url':storage_account_url\n",
        "\n",
        "        }\n",
        "        mssparkutils.fs.put(pre_filename, json.dumps(data['bundle']), True)\n",
        "        # azure_connection_manager.upload_object(message=data[\"bundle\"],container_name=snapshot_bucket_name,filename=pre_filename)\n",
        "        # print(file_idx, \"pre_write_to_storage\")\n",
        "        \n",
        "\n",
        "        # Perform pipeline standardization on extracted data to mirror how non-extracted data gets handled by the pipeline\n",
        "        data['bundle'] = standardize_names(data = data[\"bundle\"])\n",
        "        print(file_idx, \"std_names\")\n",
        "\n",
        "        data['bundle'] = standardize_phones(data = data[\"bundle\"])\n",
        "        print(file_idx, \"std_phones\")\n",
        "\n",
        "        try:\n",
        "            data['bundle'] = standardize_dob(data = data['bundle'])\n",
        "            print(file_idx, \"std_dob\")\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "        data['bundle'] = geocode_client.geocode_bundle(bundle = data['bundle'])\n",
        "        print(file_idx, \"std_address\")\n",
        "\n",
        "        # Also store a copy of the bundle after it hits standardization\n",
        "        # post_file_name = f\"/post/lac_extract_{str(curr_date)}_{str(file_idx)}.json\"\n",
        "        post_filename = f\"abfss://{snapshot_bucket_name}@{account_name}.dfs.core.windows.net/post/lac_extract_{str(curr_date)}_{str(file_idx)}.json\"\n",
        "\n",
        "        post_write_data = {\n",
        "            'blob': data[\"bundle\"],\n",
        "            'cloud_provider': 'azure',\n",
        "            'bucket_name': snapshot_bucket_name,\n",
        "            'file_name': post_filename,\n",
        "            'storage_account_url':storage_account_url\n",
        "        } \n",
        "        mssparkutils.fs.put(post_filename, json.dumps(data['bundle']), True)\n",
        "        # azure_connection_manager.upload_object(message=data[\"bundle\"],container_name=snapshot_bucket_name,filename=post_file_name)\n",
        "        # print(file_idx, \"post_write_to_storage\")\n",
        "     \n",
        "        # Now we can send the extracted record off to the MPI\n",
        "        resp = request_with_token_refresh(url=record_linkage_url,headers=record_linkage_headers,data=data,endpoint_type=\"record-linkage\",endpoint_login_url=endpoint_login_url,try_again=True)\n",
        "        print(file_idx, \"record_linkage\", resp.status_code)\n",
        "        if resp.status_code >= 400:\n",
        "            continue\n",
        "        \n",
        "\n",
        "convert(mpi_incoming_file_path)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
