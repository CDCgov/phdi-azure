{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# convertParquetMPI\n",
        "\n",
        "This notebook reads in patient data from an uploaded parquet file (`mpi_incoming_filename`), converts to FHIR, and writes the data to blob storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install git+https://github.com/CDCgov/phdi@main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "filename=\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This script converts patient data from parquet to patient FHIR resources.\n",
        "from typing import Dict, Tuple\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def extract_given_name(data: Dict):\n",
        "    first_name = data.get(\"first_name\", None)\n",
        "    middle_name = data.get(\"middle_name\", None)\n",
        "\n",
        "    given_names = []\n",
        "\n",
        "    for name in [first_name, middle_name]:\n",
        "        if name is not None:\n",
        "            for n in name.split():\n",
        "                given_names.append(n)\n",
        "\n",
        "    if len(given_names) > 0:\n",
        "        return given_names\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def adjust_birthdate(data: Dict):\n",
        "    # TODO: remove this function and pass in the `format` parameter to dob\n",
        "    # standardization in ReadSourceData for LAC\n",
        "    format = \"%d%b%Y:00:00:00.000\"\n",
        "    dob = data.get(\"birthdate\", None)\n",
        "    if dob is not None and \":\" in dob:\n",
        "        datetime_str = datetime.strptime(dob, format)\n",
        "        dob = datetime_str.strftime(\"%Y-%m-%d\")\n",
        "    return dob\n",
        "\n",
        "def convert_to_patient_fhir_resources(data: Dict) -> Tuple:\n",
        "    \"\"\"\n",
        "    Converts and returns a row of patient data into patient resource in a FHIR-formatted\n",
        "    patient resouce with a newly generated patient id as well as the\n",
        "    `external_person_id`.\n",
        "\n",
        "    :param data: Dictionary of patient data that optionionally includes the following\n",
        "      fields: mrn, ssn, first_name, middle_name, last_name, home_phone, cell-phone, sex,\n",
        "      birthdate, address, city, state, zip.\n",
        "    :return: Tuple of the `external_person_id` and FHIR-formatted patient resource.\n",
        "    \"\"\"\n",
        "\n",
        "    patient_id = str(uuid.uuid4())\n",
        "\n",
        "    optional_data = {\n",
        "        \"mrn\": data.get(\"mrn\", None),\n",
        "        \"ssn\": data.get(\"ssn\", None),\n",
        "        \"home_phone\": data.get(\"home_phone\", None),\n",
        "        \"cell_phone\": data.get(\"cell_phone\", None),\n",
        "        \"email\": data.get(\"email\", None),\n",
        "    }\n",
        "    identifiers = []\n",
        "    telecom = []\n",
        "\n",
        "    # Iterate through each patient and convert patient data to FHIR resource\n",
        "    patient_resource = {\n",
        "        \"resourceType\": \"Patient\",\n",
        "        \"id\": f\"{patient_id}\",\n",
        "        \"name\": [\n",
        "            {\n",
        "                \"family\": f\"{data.get('last_name',None)}\",\n",
        "                \"given\": extract_given_name(data),\n",
        "            }\n",
        "        ],\n",
        "        \"gender\": f\"{data.get('sex',None)}\",\n",
        "        \"birthDate\": adjust_birthdate(data),\n",
        "        \"address\": [\n",
        "            {\n",
        "                \"use\": \"home\",\n",
        "                \"line\": [f\"{data.get('address',None)}\"],\n",
        "                \"city\": f\"{data.get('city',None)}\",\n",
        "                \"state\": f\"{data.get('state',None)}\",\n",
        "                \"postalCode\": f\"{data.get('zip',None)}\",\n",
        "            }\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    for col, value in optional_data.items():\n",
        "        if value is not None:\n",
        "            if col == \"mrn\":\n",
        "                mrn = {\n",
        "                    \"type\": {\n",
        "                        \"coding\": [\n",
        "                            {\n",
        "                                \"system\": \"http://terminology.hl7.org/CodeSystem/v2-0203\",\n",
        "                                \"code\": \"MR\",\n",
        "                            }\n",
        "                        ]\n",
        "                    },\n",
        "                    \"value\": value,\n",
        "                }\n",
        "                identifiers.append(mrn)\n",
        "            elif col == \"ssn\":\n",
        "                ssn = {\n",
        "                    \"type\": {\n",
        "                        \"coding\": [\n",
        "                            {\n",
        "                                \"system\": \"http://terminology.hl7.org/CodeSystem/v2-0203\",\n",
        "                                \"code\": \"SS\",\n",
        "                            }\n",
        "                        ]\n",
        "                    },\n",
        "                    \"value\": value,\n",
        "                }\n",
        "                identifiers.append(ssn)\n",
        "            elif col == \"home_phone\":\n",
        "                home_phone = {\n",
        "                        \"system\": \"phone\",\n",
        "                        \"value\": value,\n",
        "                        \"use\": \"home\",\n",
        "                    }\n",
        "                telecom.append(home_phone)\n",
        "            elif col == \"cell_phone\":\n",
        "                cell_phone = {\n",
        "                    \"system\": \"phone\",\n",
        "                    \"value\": value,\n",
        "                    \"use\": \"mobile\",\n",
        "                }\n",
        "                telecom.append(cell_phone)\n",
        "\n",
        "            elif col == \"email\":\n",
        "                email = {\"value\": value, \"system\": \"email\"}\n",
        "                telecom.append(email)\n",
        "\n",
        "        if len(identifiers) > 0:\n",
        "            patient_resource[\"identifier\"] = identifiers\n",
        "        if len(telecom) > 0:\n",
        "            patient_resource[\"telecom\"] = telecom\n",
        "\n",
        "    fhir_bundle = {\n",
        "        \"resourceType\": \"Bundle\",\n",
        "        \"type\": \"batch\",\n",
        "        \"id\": str(uuid.uuid4()),\n",
        "        \"entry\": [\n",
        "            {\n",
        "                \"fullUrl\": f\"urn:uuid:{patient_id}\",\n",
        "                \"resource\": patient_resource,\n",
        "                \"request\": {\"method\": \"PUT\", \"url\": f\"Patient/{patient_id}\"},\n",
        "            },\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    external_person_id = data.get(\"person_id\", None)\n",
        "    return (external_person_id, fhir_bundle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from notebookutils import mssparkutils\n",
        "# from phdi.linkage.seed import convert_to_patient_fhir_resources\n",
        "from datetime import date\n",
        "import json\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import pytz \n",
        "import time\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ProcessRowsInChunks\").getOrCreate()\n",
        "\n",
        "# Set up number of rows to be processed at a time\n",
        "n_rows = 1000\n",
        "\n",
        "# Set up file client\n",
        "storage_account = \"$STORAGE_ACCOUNT\"\n",
        "source_data_bucket = \"source-data\"\n",
        "patient_data_bucket = \"patient-data\"\n",
        "storage_account_url = f\"https://{storage_account}.blob.core.windows.net/\"\n",
        "mpi_incoming_filename = f\"abfss://{patient_data_bucket}@{storage_account}.dfs.core.windows.net/{filename}\"\n",
        "\n",
        "# Set up for writing to blob storage\n",
        "blob_relative_path = \"\"\n",
        "blob_storage_linked_service = \"$BLOB_STORAGE_LINKED_SERVICE\" \n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(blob_storage_linked_service)\n",
        "wasb_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (source_data_bucket, storage_account, blob_relative_path)\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (source_data_bucket, storage_account), blob_sas_token)\n",
        "\n",
        "# Try mounting the remote storage directory at the mount point\n",
        "try:\n",
        "    mssparkutils.fs.mount(\n",
        "        wasb_path,\n",
        "        \"/\",\n",
        "        {\"LinkedService\": blob_storage_linked_service}\n",
        "    )\n",
        "except:\n",
        "    print(\"Already mounted\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_row_start(filename,patient_data_bucket,storage_account):\n",
        "    \"\"\"\n",
        "    Checks where in the seed (or large) file to start processing.\n",
        "    \"\"\"\n",
        "    row_count_filename = f\"last_row_added_to_mpi_{filename.split('.')[0]}.json\"\n",
        "    incoming_file_dir = f\"abfss://{patient_data_bucket}@{storage_account}.dfs.core.windows.net/\"\n",
        "\n",
        "    incoming_files = mssparkutils.fs.ls(incoming_file_dir)\n",
        "    filenames = [file.name for file in incoming_files]\n",
        "\n",
        "    if row_count_filename in filenames:\n",
        "\n",
        "        l = mssparkutils.fs.head(incoming_file_dir + f\"{row_count_filename}\")\n",
        "        row_start = int(l.split(':')[-1][:-1])\n",
        "        \n",
        "    else:\n",
        "        row_start = 0\n",
        "    \n",
        "    return row_start, row_count_filename\n",
        "\n",
        "def is_valid_time_window():\n",
        "    \"\"\"\n",
        "    Checks that updating the MPI occurs outside the window in which eCR data is processed\n",
        "    \"\"\"\n",
        "    # Set the timezone to Pacific Time (PT)\n",
        "    pt_timezone = pytz.timezone(\"US/Pacific\")\n",
        "\n",
        "    # Get the current time in the Pacific Time zone\n",
        "    current_time = datetime.now().astimezone(pt_timezone)\n",
        "    \n",
        "    # Define the time window (9:30am to 11:30am PT)\n",
        "    start_time = current_time.replace(hour=9, minute=30, second=0, microsecond=0)\n",
        "    end_time = current_time.replace(hour=11, minute=30, second=0, microsecond=0)\n",
        "    \n",
        "    # Check if the current time is NOT within the specified window when eCR data is likely being processed\n",
        "    valid_time = start_time <= current_time <= end_time\n",
        "\n",
        "    return not valid_time\n",
        "\n",
        "\n",
        "def process_rows_in_chunks(dataframe, last_processed_row, patient_data_bucket, storage_account, row_count_filename, chunk_size):\n",
        "    \"\"\"\n",
        "    Processes rows to seed data in `n_rows` chunks outside the time window when eCR data is being processed.\n",
        "    \"\"\"\n",
        "    curr_date = date.today()\n",
        "    total_rows = dataframe.count()\n",
        "    start = last_processed_row\n",
        "    idx = start\n",
        "\n",
        "    while start < total_rows:\n",
        "\n",
        "        if is_valid_time_window():\n",
        "    \n",
        "            # Process the chunk of data\n",
        "            for row in dataframe.collect()[start:start+chunk_size]:\n",
        "                idx +=1 \n",
        "                iris_id, fhir_bundle = convert_to_patient_fhir_resources(row.asDict())\n",
        "                fhir_bundle[\"meta\"] = {\"source\": \"uri:iris\"}\n",
        "\n",
        "                data = {\n",
        "                    'bundle': fhir_bundle,\n",
        "                    'external_person_id': iris_id\n",
        "                }\n",
        "\n",
        "                pre_filename = f\"abfss://{source_data_bucket}@{storage_account}.dfs.core.windows.net/fhir/lac_extract_{str(curr_date)}_{str(idx)}.json\"\n",
        "                mssparkutils.fs.put(pre_filename, json.dumps(data), True)\n",
        "\n",
        "            start += chunk_size\n",
        "\n",
        "            # Update the last processed row in the checkpoint file\n",
        "            last_row_data = {\"last_row_added_to_mpi\":idx}\n",
        "            mssparkutils.fs.put(f\"abfss://{patient_data_bucket}@{storage_account}.dfs.core.windows.net/{row_count_filename}\", json.dumps(last_row_data), True)\n",
        "\n",
        "        else:\n",
        "            # Wait for a certain time before checking again\n",
        "            # Assuming a delay of 15 minutes\n",
        "            time.sleep(900)  # Sleep for 15 minutes before rechecking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read in MPI seed data\n",
        "df = spark.read.parquet(mpi_incoming_filename) \n",
        "\n",
        "# Process rows in chunks of n_rows\n",
        "last_processed_row, row_count_filename = get_row_start(filename,patient_data_bucket,storage_account)\n",
        "process_rows_in_chunks(df, last_processed_row, patient_data_bucket, storage_account, row_count_filename, chunk_size=n_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Move file that triggered the MPI update event and the row_count_filename file into the archive folder \n",
        "for f in filename, row_count_filename:\n",
        "    source = f\"abfss://{patient_data_bucket}@{storage_account}.dfs.core.windows.net/{f}\"\n",
        "    destination = f\"abfss://{patient_data_bucket}@{storage_account}.dfs.core.windows.net/archive/{f}\"\n",
        "    mssparkutils.fs.mv(src=source,dest=destination,create_path=True)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
