{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# convertParquetMPI\n",
        "\n",
        "This notebook reads in patient data from an uploaded parquet file (`mpi_incoming_filename`), converts to FHIR, and writes the data to blob storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install phdi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "filename=\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from notebookutils import mssparkutils\n",
        "from phdi.linkage.seed import convert_to_patient_fhir_resources\n",
        "from datetime import date\n",
        "import json\n",
        "\n",
        "# Set up file client\n",
        "storage_account = \"$STORAGE_ACCOUNT\"\n",
        "source_data_bucket = \"source-data\"\n",
        "patient_data_bucket = \"patient-data\"\n",
        "storage_account_url = f\"https://{storage_account}.blob.core.windows.net/\"\n",
        "mpi_incoming_filename = f\"abfss://{patient_data_bucket}@{storage_account}.dfs.core.windows.net/{filename}\"\n",
        "\n",
        "# Set up for writing to blob storage\n",
        "blob_relative_path = \"\"\n",
        "blob_storage_linked_service = \"$BLOB_STORAGE_LINKED_SERVICE\" \n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(blob_storage_linked_service)\n",
        "wasb_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (source_data_bucket, storage_account, blob_relative_path)\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (source_data_bucket, storage_account), blob_sas_token)\n",
        "\n",
        "# Try mounting the remote storage directory at the mount point\n",
        "try:\n",
        "    mssparkutils.fs.mount(\n",
        "        wasb_path,\n",
        "        \"/\",\n",
        "        {\"LinkedService\": blob_storage_linked_service}\n",
        "    )\n",
        "except:\n",
        "    print(\"Already mounted\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read the MPI parquet data into a spark dataframe. Iterate over each row of patient data in the dataframe and convert to a FHIR bundle and associated iris_id. Create a POST request to the record linkage container with FHIR bundle and iris_id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert data and write to blob storage\n",
        "def convert_write_data(mpi_incoming_filename):\n",
        "    curr_date = date.today()\n",
        "    df = spark.read.parquet(mpi_incoming_filename)\n",
        "    curr_date = date.today()\n",
        "    file_idx = 0\n",
        "    for row in df.collect():\n",
        "        file_idx += 1\n",
        "\n",
        "        iris_id, fhir_bundle = convert_to_patient_fhir_resources(row.asDict())\n",
        "        fhir_bundle[\"meta\"] = {\"source\": \"uri:iris\"}\n",
        "\n",
        "        data = {\n",
        "            'bundle': fhir_bundle,\n",
        "            'external_person_id': iris_id\n",
        "        }\n",
        "\n",
        "        pre_filename = f\"abfss://{source_data_bucket}@{storage_account}.dfs.core.windows.net/fhir/lac_extract_{str(curr_date)}_{str(file_idx)}.json\"\n",
        "        mssparkutils.fs.put(pre_filename, json.dumps(data), True)\n",
        "\n",
        "\n",
        "convert_write_data(mpi_incoming_filename)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
