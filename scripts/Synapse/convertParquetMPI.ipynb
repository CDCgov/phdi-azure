{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install azure-storage-file-datalake pyarrow git+https://github.com/CDCgov/phdi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.identity import ManagedIdentityCredential\n",
        "from azure.storage.filedatalake import DataLakeFileClient\n",
        "from azure.core.credentials import AccessToken\n",
        "import time\n",
        "import sys\n",
        "from phdi.linkage.seed import convert_to_patient_fhir_resources\n",
        "import pyarrow.parquet as pq\n",
        "import pyarrow as pa\n",
        "\n",
        "# Set up authentication\n",
        "class spoof_token:\n",
        "    def get_token(*args, **kwargs):\n",
        "        return AccessToken(\n",
        "            token=mssparkutils.credentials.getToken(audience=\"storage\"),\n",
        "            expires_on=int(time.time())+60*10\n",
        "        )\n",
        "\n",
        "credential = ManagedIdentityCredential()\n",
        "credential._credential = spoof_token()\n",
        "\n",
        "# Set up file client\n",
        "account_name = \"phdidevphi9d194c64\"\n",
        "file_system_name = \"source-data\"\n",
        "file_path = \"synthetic_patient_mpi_seed_data.parquet\"\n",
        "file_client = DataLakeFileClient(account_url=f\"https://{account_name}.dfs.core.windows.net\",\n",
        "                                 file_system_name=file_system_name,\n",
        "                                 file_path=file_path,\n",
        "                                 credential=credential)\n",
        "\n",
        "# This function converts parquet bytes into FHIR bundles grouped by IRIS ID\n",
        "def convert(bytes):\n",
        "    buffer = pa.BufferReader(bytes)\n",
        "    parquet_file = pq.ParquetFile(buffer)\n",
        "    converted_data = {}\n",
        "    for row in parquet_file.iter_batches(batch_size=1):\n",
        "        data = row.to_pylist()[0]\n",
        "        iris_id, fhir_bundle = convert_to_patient_fhir_resources(data)\n",
        "        converted_data[iris_id] = fhir_bundle\n",
        "    \n",
        "    return converted_data\n",
        "\n",
        "# Download parquet file and convert\n",
        "download = file_client.download_file()\n",
        "bytes = download.readall()\n",
        "\n",
        "converted_data = convert(bytes)\n",
        "\n",
        "print(converted_data)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
