{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17162204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/09 16:57:45 WARN Utils: Your hostname, MacBook-Pro-2.local resolves to a loopback address: 127.0.0.1; using 192.168.4.22 instead (on interface en0)\n",
      "23/05/09 16:57:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/Brandon/Documents/Skylight/phdi-azure/scripts/.venv/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/Brandon/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/Brandon/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fd76f60b-97d8-481d-818a-9dbc64d54619;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 136ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fd76f60b-97d8-481d-818a-9dbc64d54619\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/09 16:57:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"dibbs\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59236abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/09 16:57:55 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "schema_fields = [\"patient_id\",\n",
    "    \"person_id\",\n",
    "    \"last_name\",\n",
    "    \"first_name\",\n",
    "    \"rr_id\",\n",
    "    \"status\",\n",
    "    \"conditions\",\n",
    "    \"eicr_id\",\n",
    "    \"eicr_version_number\",\n",
    "    \"authoring_datetime\",\n",
    "    \"provider_id\",\n",
    "    \"facility_id_number\",\n",
    "    \"facility_name\",\n",
    "    \"facility_type\",\n",
    "    \"encounter_type\",\n",
    "    \"encounter_start_date\",\n",
    "    \"encounter_end_date\",\n",
    "    \"active_problem_1\",\n",
    "    \"active_problem_date_1\",\n",
    "    \"active_problem_2\",\n",
    "    \"active_problem_date_2\",\n",
    "    \"active_problem_3\",\n",
    "    \"active_problem_date_3\",\n",
    "    \"active_problem_4\",\n",
    "    \"active_problem_date_4\",\n",
    "    \"active_problem_5\",\n",
    "    \"active_problem_date_5\",\n",
    "    \"reason_for_visit\"\n",
    "]\n",
    "\n",
    "for i in range(1,21):\n",
    "    test_template = [\n",
    "        \"test_type_\" + str(i),\n",
    "        \"test_result_\" + str(i),\n",
    "        \"test_result_interp_\" + str(i),\n",
    "        \"specimen_type_\" + str(i),\n",
    "        \"performing_lab_\" + str(i),\n",
    "        \"specimen_collection_date_\" + str(i),\n",
    "        \"result_date_\" + str(i)\n",
    "    ]\n",
    "    schema_fields += test_template\n",
    "\n",
    "schema_cols = [StructField(f, StringType(), True) for f in schema_fields]\n",
    "\n",
    "# incident_id can't be null for subsequent joins\n",
    "schema_cols.append(StructField(\"incident_id\", StringType(), False))\n",
    "schema = StructType(schema_cols)\n",
    "\n",
    "df = spark.createDataFrame([], schema)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ecr_datastore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a76ad6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 18:===================================================>    (22 + 2) / 24]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Note the tuple around the inserted information--spark additions need to be\n",
    "# of the form [(,)] to infer struct types\n",
    "\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "covid_test_types = [\"covid-19 test\", \"another covid test descriptor\"]\n",
    "covid_conditions = [\"disease caused by severe acute respiratory syndrome coronavirus 2 (disorder)\",\"covid\",\"coronavirus\"]\n",
    "\n",
    "def generate_garbage_test():\n",
    "    test = []\n",
    "    for i in range(7):\n",
    "        test.append(''.join(random.choices(string.ascii_uppercase + string.digits, k=8)))\n",
    "    return tuple(test)\n",
    "\n",
    "def generate_empty_tuple_interval(n_tests):\n",
    "    interval = []\n",
    "    for i in range(n_tests):\n",
    "        interval += [None] * 7\n",
    "    return tuple(interval)\n",
    "\n",
    "def random_date(seed):\n",
    "    random.seed(seed)\n",
    "    d = random.randint(1, int(time.time()))\n",
    "    return datetime.date.fromtimestamp(d).strftime('%Y-%m-%d')\n",
    "\n",
    "def generate_covid_test():\n",
    "    result = random.choice([\"positive\", \"negative\"])\n",
    "    date_seed = random.randint(1, 2022)\n",
    "    specimen_date = random_date(date_seed)\n",
    "    date_1 = datetime.datetime.strptime(specimen_date, \"%Y-%m-%d\")\n",
    "    res_date = date_1 + datetime.timedelta(days=5)\n",
    "    res_date = res_date.strftime('%Y-%m-%d')\n",
    "    test = [\n",
    "        random.choice(covid_test_types),\n",
    "        result,\n",
    "        result,\n",
    "        random.choice([\"PCR\", \"antigen\"]),\n",
    "        ''.join(random.choices(string.ascii_uppercase + string.digits, k=8)),\n",
    "        specimen_date,\n",
    "        res_date\n",
    "    ]\n",
    "    return tuple(test)\n",
    "\n",
    "\n",
    "# Patient 1 will have covid tests in test 4, 11, and 18\n",
    "# 18 is most recent, followed by 4\n",
    "# Test 1 will be garbage, 2-3 empty, 5-8 empty, 9-10 garbage, 12-17 empty, 19-20 empty\n",
    "row_1 = (\"2c6d5fd1-4a70-11eb-99fd-ad786a821574\", \"a81bc81b-dead-4e5d-abff-90865d1e13b1\", \"Shepard\", \\\n",
    "         \"John\", \"12-34-56-78\", \"12\", [\"migraines\", \"allergies\", \"covid\"], \"11111111\", \"2\", \"\", \"999\", \"1\", \"Huerta Memorial Hospital\", \\\n",
    "         \"\", \"encounter\", \"2021-08-14\", \"2021-08-16\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"physical\")\n",
    "row_1 += generate_garbage_test() + generate_empty_tuple_interval(2) + generate_covid_test() + \\\n",
    "        generate_empty_tuple_interval(4) + generate_garbage_test() + generate_garbage_test() + \\\n",
    "        generate_covid_test() + generate_empty_tuple_interval(6) + generate_covid_test() + \\\n",
    "        generate_empty_tuple_interval(2)\n",
    "row_1 = [row_1 + (\"123456789\",)]\n",
    "\n",
    "# Patient 2 will have covid tests in test 1,2,5,9,16\n",
    "# 2 is most recent, followed by 9\n",
    "# 3-4 are garbage, 6-8 garbage, 10-15 empty, 17-20 garbage\n",
    "row_2 = (\"2fdd0b8b-4a70-11eb-99fd-ad786a821574\", \"a81bc81b-dead-4e5d-abff-90865d1e13b1\", \"Anderson\", \\\n",
    "         \"David\", \"97-56-4862\", \"24\", [\"arthritis\", \"coronavirus\", \"IBS\"], \"99999\", \"168\", \"2022-12-12\", \"6d8e9s-98w7szz\", \"84yfd3556d\", \\\n",
    "          \"Sunset Strip\", \"outpatient\", \"OKI\", \"2022-12-02\", \"2022-12-11\", \"arthritis\", \"2020-10-10\", \\\n",
    "          \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"concern\")\n",
    "row_2 += generate_covid_test() + generate_covid_test() + generate_garbage_test() + generate_garbage_test() + \\\n",
    "        generate_covid_test() + generate_garbage_test() + generate_garbage_test() + generate_garbage_test() + \\\n",
    "        generate_covid_test() + generate_empty_tuple_interval(6) + generate_covid_test() + \\\n",
    "        generate_garbage_test() + generate_garbage_test() + generate_garbage_test() + generate_garbage_test()\n",
    "row_2 = [row_2 + (\"987654321\",)]\n",
    "row_1.extend(row_2)\n",
    "\n",
    "# # Accepted practice for row appending is union\n",
    "row_1 = spark.createDataFrame(row_1, schema)\n",
    "df = df.union(row_1)\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"ecr_datastore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c0c51319",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_test_types = [\"covid-19 test\", \"another covid test descriptor\"]\n",
    "covid_conditions = [\"disease caused by severe acute respiratory syndrome coronavirus 2 (disorder)\",\"covid\",\"coronavirus\"]\n",
    "\n",
    "ECR_DELTA_TABLE_FILE_PATH = \"ecr_datastore/\"\n",
    "\n",
    "TEST_TEMPLATE = [\n",
    "    \"test_type_\",\n",
    "    \"test_result_\",\n",
    "    \"test_result_interp_\",\n",
    "    \"specimen_type_\",\n",
    "    \"performing_lab_\",\n",
    "    \"specimen_collection_date_\",\n",
    "    \"result_date_\"\n",
    "]\n",
    "\n",
    "iris_tsv_column_aliases = {\n",
    "    \"incident_id\":\"Incident ID\",\n",
    "    \"last_name\":\" Last name\",\n",
    "    \"first_name\": \"First name\",\n",
    "    # Section action\n",
    "    # Section instance\n",
    "    \"rr_id\": \"RR ID\",\n",
    "    \"status\": \"Status\",\n",
    "    \"conditions\": \"Conditions\",\n",
    "    \"eicr_id\": \"eICR ID\",\n",
    "    \"eicr_version_number\":\"eICR Version Number\",\n",
    "    \"authoring_datetime\": \"Authoring date/time\",\n",
    "    \"provider_id\": \"Provider ID\",\n",
    "    \"facility_id_number\": \"Facility ID Number\",\n",
    "    \"facility_name\": \"Facility Name\",\n",
    "    \"facility_type\": \"Facility Type/Hospital unit\",\n",
    "    \"encounter_type\": \"Encounter Details: type\",\n",
    "    \"encounter_start_date\": \"Encounter Details: date (from)\",\n",
    "    \"encounter_end_date\": \"Encounter Details: date (to)\",\n",
    "    \"active_problem_1\": \"Active Problem 1\",\n",
    "    \"active_problem_date_1\": \"Active Problem Noted Date 1\",\n",
    "    \"active_problem_2\": \"Active Problem 2\",\n",
    "    \"active_problem_date_2\": \"Active Problem Noted Date 2\",\n",
    "    \"active_problem_3\": \"Active Problem 3\",\n",
    "    \"active_problem_date_3\": \"Active Problem Noted Date 3\",\n",
    "    \"active_problem_4\": \"Active Problem 4\",\n",
    "    \"active_problem_date_4\": \"Active Problem Noted Date 4\",\n",
    "    \"active_problem_5\": \"Active Problem 5\",\n",
    "    \"active_problem_date_5\": \"Active Problem Noted Date 5\",\n",
    "    \"reason_for_visit\": \"Reason for visit\",\n",
    "    # Comments\n",
    "    \"test_type_1\": \"Test Type 1\",\n",
    "    \"test_result_1\": \"Test Result 1\",\n",
    "    # \"test_result_interp_1\"\n",
    "    \"specimen_type_1\": \"Specimen Type 1\",\n",
    "    \"performing_lab_1\": \"Performing Lab 1\",\n",
    "    \"specimen_collection_date_1\": \"Specimen Collection Date 1\",\n",
    "    \"result_date_1\": \"Result Date 1\",\n",
    "    \"test_type_2\": \"Test Type 2\",\n",
    "    \"test_result_2\": \"Test Result 2\",\n",
    "    # \"test_result_interp_2\"\n",
    "    \"specimen_type_2\": \"Specimen Type 2\",\n",
    "    \"performing_lab_2\": \"Performing Lab 2\",\n",
    "    \"specimen_collection_date_2\": \"Specimen Collection Date 2\",\n",
    "    \"result_date_2\": \"Result Date 2\"\n",
    "    # \"Note\"\n",
    "    }\n",
    "\n",
    "\n",
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"dibbs\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import lit, col\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "ecr = spark.read.format(\"delta\").load(ECR_DELTA_TABLE_FILE_PATH)\n",
    "\n",
    "\n",
    "def filter_tests(df_row, test_cols):\n",
    "    \"\"\"\n",
    "    Filters a given row from the ECR data store by COVID-relatedness of tests.\n",
    "    For each row, a list is constructed holding the test numbers of only the\n",
    "    tests that map to a predefined list of covid test types.\n",
    "    \"\"\"\n",
    "    row_dict = df_row.asDict()\n",
    "    filtered_tests = [c for c in test_cols if df_row[c] in covid_test_types]\n",
    "    filtered_tests = [f.split(\"_\")[-1] for f in filtered_tests]\n",
    "    row_dict[\"covid_tests\"] = filtered_tests\n",
    "    return row_dict\n",
    "\n",
    "def identify_recent_tests(row_dict):\n",
    "    \"\"\"\n",
    "    Given a dictionary-formatted RDD row that's had covid-related tests identified,\n",
    "    determines which of those tests are the two most recent. If a row has no \n",
    "    associated covid tests, the created recency columns are left blank, and if\n",
    "    there's only one, the `recent_test_1` column is populated while the other is\n",
    "    left blank. Otherwise, both recency columns are populated with the test number\n",
    "    of either the first or second most recent covid related test.\n",
    "    \"\"\"\n",
    "    covid_tests = row_dict[\"covid_tests\"]\n",
    "    row_dict[\"recent_covid_test_1\"] = None\n",
    "    row_dict[\"recent_covid_test_2\"] = None\n",
    "    if len(covid_tests) == 0:\n",
    "        return row_dict\n",
    "    if len(covid_tests) == 1:\n",
    "        row_dict[\"recent_covid_test_1\"] == covid_tests[0]\n",
    "        return row_dict\n",
    "    tests_to_dates = dict(zip(covid_tests, [row_dict[\"result_date_\" + str(v)] for v in covid_tests]))\n",
    "    sorted_tests = sorted(tests_to_dates.items(), key=lambda x: x[1], reverse=True)\n",
    "    row_dict[\"recent_covid_test_1\"] = sorted_tests[0][0]\n",
    "    row_dict[\"recent_covid_test_2\"] = sorted_tests[1][0]\n",
    "    return row_dict\n",
    "\n",
    "def rewrite_test_x_with_test_y(row_dict, test_to_rewrite):\n",
    "    \"\"\"\n",
    "    Given a dictionary-formatted spark RDD row that's had recent covid tests identified,\n",
    "    replaces the values of test 1 and test 2 with the information pertaining to the\n",
    "    most recent and second most recent covid tests, respectively.\n",
    "    \"\"\"\n",
    "    replace_with_test = row_dict[\"recent_covid_test_\" + str(test_to_rewrite)]\n",
    "    if replace_with_test is None:\n",
    "        return row_dict\n",
    "    for test_field in TEST_TEMPLATE:\n",
    "        row_dict[test_field + str(test_to_rewrite)] = row_dict[test_field + replace_with_test]\n",
    "    return row_dict\n",
    "    \n",
    "\n",
    "# Explicitly define new schema to prevent interpretation breakage during RDD map\n",
    "new_schema = ecr.schema.add(StructField(\"covid_tests\", StringType(), True))\n",
    "new_schema = new_schema.add(StructField(\"recent_covid_test_1\", StringType(), True))\n",
    "new_schema = new_schema.add(StructField(\"recent_covid_test_2\", StringType(), True))\n",
    "\n",
    "# Apply map functions to parallelize row processing and identify covid tests\n",
    "test_cols = [c for c in ecr.columns if \"test_type\" in c]\n",
    "rdd2 = ecr.rdd.map(lambda x: filter_tests(x, test_cols))\n",
    "rdd3 = rdd2.map(lambda x: identify_recent_tests(x))\n",
    "rdd4 = rdd3.map(lambda x: rewrite_test_x_with_test_y(x, 1))\n",
    "rdd5 = rdd4.map(lambda x: rewrite_test_x_with_test_y(x, 2))\n",
    "\n",
    "# Convert back to DF and drop the temp cols we created as well as superfluous tests\n",
    "ecr = spark.createDataFrame(rdd5, new_schema)\n",
    "cols_to_drop = [\"covid_tests\", \"recent_covid_test_1\", \"recent_covid_test_2\"]\n",
    "for i in range(3,21):\n",
    "    for col_name in TEST_TEMPLATE:\n",
    "        cols_to_drop.append(col_name + str(i))\n",
    "ecr = ecr.drop(*cols_to_drop)\n",
    "\n",
    "# Now, rename all the processed columns to what LAC would like in their output\n",
    "ecr = ecr.select(\n",
    "    col(\"incident_id\").alias(iris_tsv_column_aliases[\"incident_id\"]),\n",
    "#     col(\"iris_id\"),\n",
    "    col(\"patient_id\"),\n",
    "    col(\"person_id\"),\n",
    "    col(\"first_name\").alias(iris_tsv_column_aliases[\"first_name\"]),\n",
    "    col(\"last_name\").alias(iris_tsv_column_aliases[\"last_name\"]),\n",
    "    lit(\"[ListSectionInsert]\").alias(\"Section action\"),\n",
    "    lit(None).cast('string').alias(\"Section instance\"),\n",
    "    col(\"rr_id\").alias(iris_tsv_column_aliases[\"rr_id\"]),\n",
    "    col(\"status\").alias(iris_tsv_column_aliases[\"status\"]),\n",
    "    col(\"conditions\").alias(iris_tsv_column_aliases[\"conditions\"]),\n",
    "    col(\"eicr_id\").alias(iris_tsv_column_aliases[\"eicr_id\"]),\n",
    "    col(\"eicr_version_number\").alias(iris_tsv_column_aliases[\"eicr_version_number\"]),\n",
    "    col(\"authoring_datetime\").alias(iris_tsv_column_aliases[\"authoring_datetime\"]),\n",
    "    col(\"provider_id\").alias(iris_tsv_column_aliases[\"provider_id\"]),\n",
    "    col(\"facility_id_number\").alias(iris_tsv_column_aliases[\"facility_id_number\"]),\n",
    "    col(\"facility_name\").alias(iris_tsv_column_aliases[\"facility_name\"]),\n",
    "    col(\"facility_type\").alias(iris_tsv_column_aliases[\"facility_type\"]),\n",
    "    col(\"encounter_type\").alias(iris_tsv_column_aliases[\"encounter_type\"]),\n",
    "    col(\"encounter_start_date\").alias(iris_tsv_column_aliases[\"encounter_start_date\"]),\n",
    "    col(\"encounter_end_date\").alias(iris_tsv_column_aliases[\"encounter_end_date\"]),\n",
    "    col(\"active_problem_1\").alias(iris_tsv_column_aliases[\"active_problem_1\"]),\n",
    "    col(\"active_problem_date_1\").alias(iris_tsv_column_aliases[\"active_problem_date_1\"]),\n",
    "    col(\"active_problem_2\").alias(iris_tsv_column_aliases[\"active_problem_2\"]),\n",
    "    col(\"active_problem_date_2\").alias(iris_tsv_column_aliases[\"active_problem_date_2\"]),\n",
    "    col(\"active_problem_3\").alias(iris_tsv_column_aliases[\"active_problem_3\"]),\n",
    "    col(\"active_problem_date_3\").alias(iris_tsv_column_aliases[\"active_problem_date_3\"]),\n",
    "    col(\"active_problem_4\").alias(iris_tsv_column_aliases[\"active_problem_4\"]),\n",
    "    col(\"active_problem_date_4\").alias(iris_tsv_column_aliases[\"active_problem_date_4\"]),\n",
    "    col(\"active_problem_5\").alias(iris_tsv_column_aliases[\"active_problem_5\"]),\n",
    "    col(\"active_problem_date_5\").alias(iris_tsv_column_aliases[\"active_problem_date_5\"]),\n",
    "    col(\"reason_for_visit\").alias(iris_tsv_column_aliases[\"reason_for_visit\"]),\n",
    "    lit(None).cast('string').alias(\"Comments\"),\n",
    "    col(\"test_type_1\").alias(iris_tsv_column_aliases[\"test_type_1\"]),\n",
    "    col(\"test_result_1\").alias(iris_tsv_column_aliases[\"test_result_1\"]),\n",
    "    col(\"specimen_type_1\").alias(iris_tsv_column_aliases[\"specimen_type_1\"]),\n",
    "    col(\"performing_lab_1\").alias(iris_tsv_column_aliases[\"performing_lab_1\"]),\n",
    "    col(\"specimen_collection_date_1\").alias(iris_tsv_column_aliases[\"specimen_collection_date_1\"]),\n",
    "    col(\"result_date_1\").alias(iris_tsv_column_aliases[\"result_date_1\"]),\n",
    "    col(\"test_type_2\").alias(iris_tsv_column_aliases[\"test_type_2\"]),\n",
    "    col(\"test_result_2\").alias(iris_tsv_column_aliases[\"test_result_2\"]),\n",
    "    col(\"specimen_type_2\").alias(iris_tsv_column_aliases[\"specimen_type_2\"]),\n",
    "    col(\"performing_lab_2\").alias(iris_tsv_column_aliases[\"performing_lab_2\"]),\n",
    "    col(\"specimen_collection_date_2\").alias(iris_tsv_column_aliases[\"specimen_collection_date_2\"]),\n",
    "    col(\"result_date_2\").alias(iris_tsv_column_aliases[\"result_date_2\"]),\n",
    "    lit(\"eCR data added to UDF through import utility\").alias(\"Note\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "03bdb945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsv_flatten(row):\n",
    "    \"\"\"\n",
    "    Given a row from a spark RDD, extract some information to create a pair of TSV rows\n",
    "    for each row in the RDD. The first of these rows is a simple name, incident, and note\n",
    "    header, while the second contains the comprehensive value set built above.\n",
    "    \"\"\"\n",
    "    row_dict = row.asDict()\n",
    "    incident_id = row[iris_tsv_column_aliases[\"incident_id\"]]\n",
    "    last_name = row[iris_tsv_column_aliases[\"last_name\"]]\n",
    "    first_name = row[iris_tsv_column_aliases[\"first_name\"]]\n",
    "    note = row[\"Note\"]\n",
    "    num_cols = len(row_dict)\n",
    "    none_tuple = (None,) * (num_cols - 4)\n",
    "    return iter([(incident_id, last_name, first_name) + none_tuple + (note,)] + [row])\n",
    "\n",
    "flattend_rdd=ecr.rdd.flatMap(tsv_flatten)\n",
    "\n",
    "# DF will break with this because properties aren't supposed to be nullable,\n",
    "# so change those fields because we're just writing output\n",
    "for sf in ecr.schema:\n",
    "    sf.nullable = True\n",
    "df3 = spark.createDataFrame(flattend_rdd, ecr.schema)\n",
    "\n",
    "\n",
    "# Now split this up into those that have incident IDs and those that don't\n",
    "df_with_ids = df3.filter(df3[\"Incident ID\"].isNotNull())\n",
    "df_no_ids = df3.filter(df3[\"Incident ID\"].isNull())\n",
    "df_with_ids.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").option(\"sep\", \"\\t\").csv(\"with_ids.tsv\")\n",
    "df_no_ids.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").option(\"sep\", \"\\t\").csv(\"/test/no_ids.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375413fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
