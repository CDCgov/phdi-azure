{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c367a4",
   "metadata": {},
   "source": [
    "# Delta lakes: creating a sample store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ffebad",
   "metadata": {},
   "source": [
    "This notebook is intended to create a simple dummy delta lake store compliant with the schema outlined in the ECR validation directory. It leverages `pyspark` and `delta-spark` working within a virtual environment to create an empty table and insert a few dummy rows. It can also be run completely outside of a virtual environment, if desired, in which case only step 4 below need be followed. To configure an environment in order to run this script, perform the following steps:\n",
    "\n",
    "1. Create a python virtual environment in which to install dependencies and execute the notebook: `python -m venv .venv/`\n",
    "2. Activate the virtual environment: `source .venv/bin/activate`\n",
    "3. Install `jupyter` directly in the virtual environment (while it is possible to install kernelspec paths and manipulate environment variables, in practice, simply installing `jupyter` directly without `site-packages` makes importing modules the most seamless): `pip install jupyter`\n",
    "4. Install the `delta-spark` package in the virtual environment: `pip install delta-spark`. This will automatically install a compatible version of `pyspark` already integrated with the `delta` configuration (it will be version `3.3.2` for most installations).\n",
    "5. From within the virtual environment, run `jupyter notebook` to launch the navigation page, then select this notebook. The default `Python 3 (ipykernel)` kernel should be displayed in the top right corner of the window, under the `Logout` button.\n",
    "\n",
    "You can now successfully run this notebook from within your virtual environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921af746",
   "metadata": {},
   "source": [
    "## Creating the delta lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c5a3d8",
   "metadata": {},
   "source": [
    "First, we'll handle the imports and configuration of the spark builder and delta session (the packages typically assume an interactive running session, but we can use a pre-configured `builder` object to programmatically handle configuration for us)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb6c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"dibbs\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbab7a24",
   "metadata": {},
   "source": [
    "With the packages imported and configured, we can create the schema we'll use to instantiate our delta lake. Schemas in `pyspark` (and therefore `delta-spark`) are defined as `struct` type objects, in which one or more `struct fields` are specified with a name and data type. These can be used to create complex logic around acceptable input formats for the deta, but for now, we'll create some simple string columns designed to hold the data specified in the DIBBS eCR validation schema (found in the message parsing service). After defining the schema, we'll use it to create a blank but formatted spark data frame that we can add to and eventually save as a delta lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530531f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "schema_fields = [\"patient_id\",\n",
    "    \"person_id\",\n",
    "    \"last_name\",\n",
    "    \"first_name\",\n",
    "    \"rr_id\",\n",
    "    \"status\",\n",
    "    \"conditions\",\n",
    "    \"eicr_id\",\n",
    "    \"eicr_version_number\",\n",
    "    \"authoring_datetime\",\n",
    "    \"provider_id\",\n",
    "    \"facility_id_number\",\n",
    "    \"facility_name\",\n",
    "    \"facility_type\",\n",
    "    \"encounter_type\",\n",
    "    \"encounter_start_date\",\n",
    "    \"encounter_end_date\",\n",
    "    \"active_problem_1\",\n",
    "    \"active_problem_date_1\",\n",
    "    \"active_problem_2\",\n",
    "    \"active_problem_date_2\",\n",
    "    \"active_problem_3\",\n",
    "    \"active_problem_date_3\",\n",
    "    \"active_problem_4\",\n",
    "    \"active_problem_date_4\",\n",
    "    \"active_problem_5\",\n",
    "    \"active_problem_date_5\",\n",
    "    \"reason_for_visit\",\n",
    "    \"test_type_1\",\n",
    "    \"test_result_1\",\n",
    "    \"test_result_interp_1\",\n",
    "    \"specimen_type_1\",\n",
    "    \"performing_lab_1\",\n",
    "    \"specimen_collection_date_1\",\n",
    "    \"result_date_1\",\n",
    "    \"test_type_2\",\n",
    "    \"test_result_2\",\n",
    "    \"test_result_interp_2\",\n",
    "    \"specimen_type_2\",\n",
    "    \"performing_lab_2\",\n",
    "    \"specimen_collection_date_2\",\n",
    "    \"result_date_2\"\n",
    "]\n",
    "\n",
    "schema_cols = [StructField(f, StringType(), True) for f in schema_fields]\n",
    "\n",
    "# incident_id can't be null for subsequent joins\n",
    "schema_cols.append(StructField(\"incident_id\", StringType(), False))\n",
    "schema = StructType(schema_cols)\n",
    "\n",
    "df = spark.createDataFrame([], schema)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"test_delta_lake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340a15f7",
   "metadata": {},
   "source": [
    "With the table created and properly formatted with our desired columns, we can create two dummy rows and insert them into the table using spark's appends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad86768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the tuple around the inserted information--spark additions need to be\n",
    "# of the form [(,)] to infer struct types\n",
    "row_1 = [(\"2c6d5fd1-4a70-11eb-99fd-ad786a821574\", \"a81bc81b-dead-4e5d-abff-90865d1e13b1\", \"Shepard\", \\\n",
    "         \"John\", \"12-34-56-78\", \"12\", \"\", \"11111111\", \"2\", \"\", \"999\", \"1\", \"Huerta Memorial Hospital\", \\\n",
    "         \"\", \"encounter\", \"2021-08-14\", \"2021-08-16\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"physical\", \\\n",
    "         \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\" ,\"\", \"\", \"\", \"123456789\")]\n",
    "row_2 = [(\"2fdd0b8b-4a70-11eb-99fd-ad786a821574\", \"a81bc81b-dead-4e5d-abff-90865d1e13b1\", \"Anderson\", \\\n",
    "         \"David\", \"97-56-4862\", \"24\", \"\", \"99999\", \"168\", \"2022-12-12\", \"6d8e9s-98w7szz\", \"84yfd3556d\", \\\n",
    "          \"Sunset Strip\", \"outpatient\", \"OKI\", \"2022-12-02\", \"2022-12-11\", \"arthritis\", \"2020-10-10\", \\\n",
    "          \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"concern\", \"degenerative disk test\", \"positive\", \\\n",
    "          \"patient has a bad back\", \"vertebrae fluid\", \"Easy Pete's Discount Disk Checks\", \"2022-10-10\", \\\n",
    "          \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"987654321\")]\n",
    "row_1.extend(row_2)\n",
    "\n",
    "# Accepted practice for row appending is union\n",
    "row_1 = spark.createDataFrame(row_1, schema)\n",
    "df = df.union(row_1)\n",
    "df.show()\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"test_delta_lake\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
